{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebcf5999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Running Linear Regression for cpi_yoy\n",
      "--------------------------------------------------\n",
      "Running Linear Regression model for cpi_yoy...\n",
      "Training data: (269, 63), Test data: (68, 63)\n",
      "Training - RMSE: 0.0000, MAE: 0.0000, MAPE: 0.00%, R²: 1.0000\n",
      "Testing - RMSE: 0.0000, MAE: 0.0000, MAPE: 0.00%, R²: 1.0000\n",
      "Intercept: 106.3346\n",
      "Generating 24 period forecast...\n",
      "\n",
      "Future cpi_yoy forecasts:\n",
      "2025-01-01    102.96\n",
      "2025-02-01    102.96\n",
      "2025-03-01    102.96\n",
      "2025-04-01    102.96\n",
      "2025-05-01    102.96\n",
      "2025-06-01    102.96\n",
      "2025-07-01    102.96\n",
      "2025-08-01    102.96\n",
      "2025-09-01    102.96\n",
      "2025-10-01    102.96\n",
      "2025-11-01    102.96\n",
      "2025-12-01    102.96\n",
      "2026-01-01    102.96\n",
      "2026-02-01    102.96\n",
      "2026-03-01    102.96\n",
      "2026-04-01    102.96\n",
      "2026-05-01    102.96\n",
      "2026-06-01    102.96\n",
      "2026-07-01    102.96\n",
      "2026-08-01    102.96\n",
      "2026-09-01    102.96\n",
      "2026-10-01    102.96\n",
      "2026-11-01    102.96\n",
      "2026-12-01    102.96\n",
      "Freq: MS, dtype: float64\n",
      "\n",
      "--------------------------------------------------\n",
      "Running Ridge Regression for cpi_yoy\n",
      "--------------------------------------------------\n",
      "Running Ridge Regression model for cpi_yoy...\n",
      "Training data: (269, 63), Test data: (68, 63)\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best parameters: {'model__alpha': 1.438449888287663}\n",
      "Training - RMSE: 0.0819, MAE: 0.0683, MAPE: 0.06%, R²: 0.9998\n",
      "Testing - RMSE: 0.2090, MAE: 0.1816, MAPE: 0.18%, R²: 0.9706\n",
      "Intercept: 106.3346\n",
      "Generating 24 period forecast...\n",
      "\n",
      "Future cpi_yoy forecasts:\n",
      "2025-01-01    103.094473\n",
      "2025-02-01    103.094473\n",
      "2025-03-01    103.094473\n",
      "2025-04-01    103.094473\n",
      "2025-05-01    103.094473\n",
      "2025-06-01    103.094473\n",
      "2025-07-01    103.094473\n",
      "2025-08-01    103.094473\n",
      "2025-09-01    103.094473\n",
      "2025-10-01    103.094473\n",
      "2025-11-01    103.094473\n",
      "2025-12-01    103.094473\n",
      "2026-01-01    103.094473\n",
      "2026-02-01    103.094473\n",
      "2026-03-01    103.094473\n",
      "2026-04-01    103.094473\n",
      "2026-05-01    103.094473\n",
      "2026-06-01    103.094473\n",
      "2026-07-01    103.094473\n",
      "2026-08-01    103.094473\n",
      "2026-09-01    103.094473\n",
      "2026-10-01    103.094473\n",
      "2026-11-01    103.094473\n",
      "2026-12-01    103.094473\n",
      "Freq: MS, dtype: float64\n",
      "\n",
      "--------------------------------------------------\n",
      "Running Lasso Regression for cpi_yoy\n",
      "--------------------------------------------------\n",
      "Running Lasso Regression model for cpi_yoy...\n",
      "Training data: (269, 63), Test data: (68, 63)\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best parameters: {'model__alpha': 0.00206913808111479}\n",
      "Training - RMSE: 0.0259, MAE: 0.0179, MAPE: 0.02%, R²: 1.0000\n",
      "Testing - RMSE: 0.0154, MAE: 0.0116, MAPE: 0.01%, R²: 0.9998\n",
      "Intercept: 106.3346\n",
      "Generating 24 period forecast...\n",
      "\n",
      "Future cpi_yoy forecasts:\n",
      "2025-01-01    102.969113\n",
      "2025-02-01    102.969113\n",
      "2025-03-01    102.969113\n",
      "2025-04-01    102.969113\n",
      "2025-05-01    102.969113\n",
      "2025-06-01    102.969113\n",
      "2025-07-01    102.969113\n",
      "2025-08-01    102.969113\n",
      "2025-09-01    102.969113\n",
      "2025-10-01    102.969113\n",
      "2025-11-01    102.969113\n",
      "2025-12-01    102.969113\n",
      "2026-01-01    102.969113\n",
      "2026-02-01    102.969113\n",
      "2026-03-01    102.969113\n",
      "2026-04-01    102.969113\n",
      "2026-05-01    102.969113\n",
      "2026-06-01    102.969113\n",
      "2026-07-01    102.969113\n",
      "2026-08-01    102.969113\n",
      "2026-09-01    102.969113\n",
      "2026-10-01    102.969113\n",
      "2026-11-01    102.969113\n",
      "2026-12-01    102.969113\n",
      "Freq: MS, dtype: float64\n",
      "\n",
      "--------------------------------------------------\n",
      "Running Elastic_net Regression for cpi_yoy\n",
      "--------------------------------------------------\n",
      "Running Elastic_net Regression model for cpi_yoy...\n",
      "Training data: (269, 63), Test data: (68, 63)\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "Best parameters: {'model__alpha': 0.001, 'model__l1_ratio': 0.9}\n",
      "Training - RMSE: 0.0243, MAE: 0.0185, MAPE: 0.02%, R²: 1.0000\n",
      "Testing - RMSE: 0.0321, MAE: 0.0277, MAPE: 0.03%, R²: 0.9993\n",
      "Intercept: 106.3346\n",
      "Generating 24 period forecast...\n",
      "\n",
      "Future cpi_yoy forecasts:\n",
      "2025-01-01    102.982001\n",
      "2025-02-01    102.982001\n",
      "2025-03-01    102.982001\n",
      "2025-04-01    102.982001\n",
      "2025-05-01    102.982001\n",
      "2025-06-01    102.982001\n",
      "2025-07-01    102.982001\n",
      "2025-08-01    102.982001\n",
      "2025-09-01    102.982001\n",
      "2025-10-01    102.982001\n",
      "2025-11-01    102.982001\n",
      "2025-12-01    102.982001\n",
      "2026-01-01    102.982001\n",
      "2026-02-01    102.982001\n",
      "2026-03-01    102.982001\n",
      "2026-04-01    102.982001\n",
      "2026-05-01    102.982001\n",
      "2026-06-01    102.982001\n",
      "2026-07-01    102.982001\n",
      "2026-08-01    102.982001\n",
      "2026-09-01    102.982001\n",
      "2026-10-01    102.982001\n",
      "2026-11-01    102.982001\n",
      "2026-12-01    102.982001\n",
      "Freq: MS, dtype: float64\n",
      "\n",
      "--------------------------------------------------\n",
      "Running Linear Regression for cpi_mom\n",
      "--------------------------------------------------\n",
      "Running Linear Regression model for cpi_mom...\n",
      "Training data: (269, 63), Test data: (68, 63)\n",
      "Training - RMSE: 0.0000, MAE: 0.0000, MAPE: 0.00%, R²: 1.0000\n",
      "Testing - RMSE: 0.0000, MAE: 0.0000, MAPE: 0.00%, R²: 1.0000\n",
      "Intercept: 100.5051\n",
      "Generating 24 period forecast...\n",
      "\n",
      "Future cpi_mom forecasts:\n",
      "2025-01-01    100.29\n",
      "2025-02-01    100.29\n",
      "2025-03-01    100.29\n",
      "2025-04-01    100.29\n",
      "2025-05-01    100.29\n",
      "2025-06-01    100.29\n",
      "2025-07-01    100.29\n",
      "2025-08-01    100.29\n",
      "2025-09-01    100.29\n",
      "2025-10-01    100.29\n",
      "2025-11-01    100.29\n",
      "2025-12-01    100.29\n",
      "2026-01-01    100.29\n",
      "2026-02-01    100.29\n",
      "2026-03-01    100.29\n",
      "2026-04-01    100.29\n",
      "2026-05-01    100.29\n",
      "2026-06-01    100.29\n",
      "2026-07-01    100.29\n",
      "2026-08-01    100.29\n",
      "2026-09-01    100.29\n",
      "2026-10-01    100.29\n",
      "2026-11-01    100.29\n",
      "2026-12-01    100.29\n",
      "Freq: MS, dtype: float64\n",
      "\n",
      "--------------------------------------------------\n",
      "Running Ridge Regression for cpi_mom\n",
      "--------------------------------------------------\n",
      "Running Ridge Regression model for cpi_mom...\n",
      "Training data: (269, 63), Test data: (68, 63)\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best parameters: {'model__alpha': 0.001}\n",
      "Training - RMSE: 0.0005, MAE: 0.0004, MAPE: 0.00%, R²: 1.0000\n",
      "Testing - RMSE: 0.0007, MAE: 0.0006, MAPE: 0.00%, R²: 1.0000\n",
      "Intercept: 100.5051\n",
      "Generating 24 period forecast...\n",
      "\n",
      "Future cpi_mom forecasts:\n",
      "2025-01-01    100.289554\n",
      "2025-02-01    100.289554\n",
      "2025-03-01    100.289554\n",
      "2025-04-01    100.289554\n",
      "2025-05-01    100.289554\n",
      "2025-06-01    100.289554\n",
      "2025-07-01    100.289554\n",
      "2025-08-01    100.289554\n",
      "2025-09-01    100.289554\n",
      "2025-10-01    100.289554\n",
      "2025-11-01    100.289554\n",
      "2025-12-01    100.289554\n",
      "2026-01-01    100.289554\n",
      "2026-02-01    100.289554\n",
      "2026-03-01    100.289554\n",
      "2026-04-01    100.289554\n",
      "2026-05-01    100.289554\n",
      "2026-06-01    100.289554\n",
      "2026-07-01    100.289554\n",
      "2026-08-01    100.289554\n",
      "2026-09-01    100.289554\n",
      "2026-10-01    100.289554\n",
      "2026-11-01    100.289554\n",
      "2026-12-01    100.289554\n",
      "Freq: MS, dtype: float64\n",
      "\n",
      "--------------------------------------------------\n",
      "Running Lasso Regression for cpi_mom\n",
      "--------------------------------------------------\n",
      "Running Lasso Regression model for cpi_mom...\n",
      "Training data: (269, 63), Test data: (68, 63)\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best parameters: {'model__alpha': 0.001}\n",
      "Training - RMSE: 0.0050, MAE: 0.0032, MAPE: 0.00%, R²: 1.0000\n",
      "Testing - RMSE: 0.0036, MAE: 0.0020, MAPE: 0.00%, R²: 0.9999\n",
      "Intercept: 100.5051\n",
      "Generating 24 period forecast...\n",
      "\n",
      "Future cpi_mom forecasts:\n",
      "2025-01-01    100.291046\n",
      "2025-02-01    100.291046\n",
      "2025-03-01    100.291046\n",
      "2025-04-01    100.291046\n",
      "2025-05-01    100.291046\n",
      "2025-06-01    100.291046\n",
      "2025-07-01    100.291046\n",
      "2025-08-01    100.291046\n",
      "2025-09-01    100.291046\n",
      "2025-10-01    100.291046\n",
      "2025-11-01    100.291046\n",
      "2025-12-01    100.291046\n",
      "2026-01-01    100.291046\n",
      "2026-02-01    100.291046\n",
      "2026-03-01    100.291046\n",
      "2026-04-01    100.291046\n",
      "2026-05-01    100.291046\n",
      "2026-06-01    100.291046\n",
      "2026-07-01    100.291046\n",
      "2026-08-01    100.291046\n",
      "2026-09-01    100.291046\n",
      "2026-10-01    100.291046\n",
      "2026-11-01    100.291046\n",
      "2026-12-01    100.291046\n",
      "Freq: MS, dtype: float64\n",
      "\n",
      "--------------------------------------------------\n",
      "Running Elastic_net Regression for cpi_mom\n",
      "--------------------------------------------------\n",
      "Running Elastic_net Regression model for cpi_mom...\n",
      "Training data: (269, 63), Test data: (68, 63)\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "Best parameters: {'model__alpha': 0.001, 'model__l1_ratio': 0.5}\n",
      "Training - RMSE: 0.0047, MAE: 0.0030, MAPE: 0.00%, R²: 1.0000\n",
      "Testing - RMSE: 0.0034, MAE: 0.0019, MAPE: 0.00%, R²: 0.9999\n",
      "Intercept: 100.5051\n",
      "Generating 24 period forecast...\n",
      "\n",
      "Future cpi_mom forecasts:\n",
      "2025-01-01    100.290497\n",
      "2025-02-01    100.290497\n",
      "2025-03-01    100.290497\n",
      "2025-04-01    100.290497\n",
      "2025-05-01    100.290497\n",
      "2025-06-01    100.290497\n",
      "2025-07-01    100.290497\n",
      "2025-08-01    100.290497\n",
      "2025-09-01    100.290497\n",
      "2025-10-01    100.290497\n",
      "2025-11-01    100.290497\n",
      "2025-12-01    100.290497\n",
      "2026-01-01    100.290497\n",
      "2026-02-01    100.290497\n",
      "2026-03-01    100.290497\n",
      "2026-04-01    100.290497\n",
      "2026-05-01    100.290497\n",
      "2026-06-01    100.290497\n",
      "2026-07-01    100.290497\n",
      "2026-08-01    100.290497\n",
      "2026-09-01    100.290497\n",
      "2026-10-01    100.290497\n",
      "2026-11-01    100.290497\n",
      "2026-12-01    100.290497\n",
      "Freq: MS, dtype: float64\n",
      "\n",
      "\n",
      "Model Comparison - CPI Year-over-Year:\n",
      "--------------------------------------------------------------------------------\n",
      "Model           Train RMSE   Test RMSE    Train R²     Test R²     \n",
      "--------------------------------------------------------------------------------\n",
      "Linear          0.0000       0.0000       1.0000       1.0000      \n",
      "Ridge           0.0819       0.2090       0.9998       0.9706      \n",
      "Lasso           0.0259       0.0154       1.0000       0.9998      \n",
      "Elastic_net     0.0243       0.0321       1.0000       0.9993      \n",
      "\n",
      "\n",
      "Model Comparison - CPI Month-over-Month:\n",
      "--------------------------------------------------------------------------------\n",
      "Model           Train RMSE   Test RMSE    Train R²     Test R²     \n",
      "--------------------------------------------------------------------------------\n",
      "Linear          0.0000       0.0000       1.0000       1.0000      \n",
      "Ridge           0.0005       0.0007       1.0000       1.0000      \n",
      "Lasso           0.0050       0.0036       1.0000       0.9999      \n",
      "Elastic_net     0.0047       0.0034       1.0000       0.9999      \n",
      "\n",
      "Metrics saved to plots/linear_regression_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load and prepare the time series data\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert time column to datetime\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    \n",
    "    # Set time as index\n",
    "    df.set_index('time', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"Create additional features for modeling\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Create lag features\n",
    "    for i in range(1, 13):\n",
    "        if 'cpi_mom' in df_copy.columns:\n",
    "            df_copy[f'cpi_mom_lag_{i}'] = df_copy['cpi_mom'].shift(i)\n",
    "        if 'cpi_yoy' in df_copy.columns:\n",
    "            df_copy[f'cpi_yoy_lag_{i}'] = df_copy['cpi_yoy'].shift(i)\n",
    "    \n",
    "    # Create rolling window features\n",
    "    for window in [3, 6, 12]:\n",
    "        if 'cpi_mom' in df_copy.columns:\n",
    "            df_copy[f'cpi_mom_rolling_mean_{window}'] = df_copy['cpi_mom'].rolling(window=window).mean()\n",
    "            df_copy[f'cpi_mom_rolling_std_{window}'] = df_copy['cpi_mom'].rolling(window=window).std()\n",
    "        if 'cpi_yoy' in df_copy.columns:\n",
    "            df_copy[f'cpi_yoy_rolling_mean_{window}'] = df_copy['cpi_yoy'].rolling(window=window).mean()\n",
    "            df_copy[f'cpi_yoy_rolling_std_{window}'] = df_copy['cpi_yoy'].rolling(window=window).std()\n",
    "    \n",
    "    # Create economic indicator lag features\n",
    "    for i in range(1, 4):\n",
    "        if 'oil_price' in df_copy.columns:\n",
    "            df_copy[f'oil_price_lag_{i}'] = df_copy['oil_price'].shift(i)\n",
    "        if 'gold_price' in df_copy.columns:\n",
    "            df_copy[f'gold_price_lag_{i}'] = df_copy['gold_price'].shift(i)\n",
    "        if 'interest_rate' in df_copy.columns:\n",
    "            df_copy[f'interest_rate_lag_{i}'] = df_copy['interest_rate'].shift(i)\n",
    "    \n",
    "    # Add month and year as cyclical features\n",
    "    if 'month' in df_copy.columns:\n",
    "        df_copy['month_sin'] = np.sin(2 * np.pi * df_copy['month']/12)\n",
    "        df_copy['month_cos'] = np.cos(2 * np.pi * df_copy['month']/12)\n",
    "    \n",
    "    # Create interaction features\n",
    "    if all(col in df_copy.columns for col in ['oil_price', 'gold_price']):\n",
    "        df_copy['oil_gold_ratio'] = df_copy['oil_price'] / df_copy['gold_price']\n",
    "    \n",
    "    # Drop rows with NaN values (due to lag features)\n",
    "    df_clean = df_copy.dropna()\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def calculate_metrics(actual, predicted):\n",
    "    \"\"\"Calculate evaluation metrics\"\"\"\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    r2 = r2_score(actual, predicted)\n",
    "    mape = np.mean(np.abs((actual - predicted) / actual)) * 100\n",
    "    \n",
    "    return rmse, mae, mape, r2\n",
    "\n",
    "def run_linear_regression(df, target_col, test_size=0.2, model_type='linear', tune_hyperparams=True):\n",
    "    \"\"\"\n",
    "    Train and evaluate a Linear Regression model for time series forecasting\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with engineered features\n",
    "    target_col : str\n",
    "        Name of the target column\n",
    "    test_size : float\n",
    "        Proportion of data to use for testing\n",
    "    model_type : str\n",
    "        Type of linear model ('linear', 'ridge', 'lasso', or 'elastic_net')\n",
    "    tune_hyperparams : bool\n",
    "        Whether to perform hyperparameter tuning (only for ridge, lasso, elastic_net)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        Model, predictions, coefficients, metrics\n",
    "    \"\"\"\n",
    "    print(f\"Running {model_type.capitalize()} Regression model for {target_col}...\")\n",
    "    \n",
    "    # Define features and target\n",
    "    X = df.drop([col for col in ['cpi_mom', 'cpi_yoy', target_col] if col in df.columns], axis=1)\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Split data into training and testing sets (time-based split)\n",
    "    split_idx = int(len(df) * (1 - test_size))\n",
    "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "    \n",
    "    print(f\"Training data: {X_train.shape}, Test data: {X_test.shape}\")\n",
    "    \n",
    "    # Create pipeline with scaling\n",
    "    if model_type == 'linear':\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', LinearRegression())\n",
    "        ])\n",
    "        \n",
    "        # No hyperparameters to tune for standard linear regression\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "    elif model_type == 'ridge' and tune_hyperparams:\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', Ridge(random_state=42))\n",
    "        ])\n",
    "        \n",
    "        # Define parameter grid\n",
    "        param_grid = {\n",
    "            'model__alpha': np.logspace(-3, 3, 20)\n",
    "        }\n",
    "        \n",
    "        # Use TimeSeriesSplit for cross-validation\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        \n",
    "        # Create and fit GridSearchCV\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=pipeline,\n",
    "            param_grid=param_grid,\n",
    "            cv=tscv,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Get best parameters and model\n",
    "        best_params = grid_search.best_params_\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "        \n",
    "        # Use the best model\n",
    "        pipeline = grid_search.best_estimator_\n",
    "        \n",
    "    elif model_type == 'lasso' and tune_hyperparams:\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', Lasso(random_state=42))\n",
    "        ])\n",
    "        \n",
    "        # Define parameter grid\n",
    "        param_grid = {\n",
    "            'model__alpha': np.logspace(-3, 3, 20)\n",
    "        }\n",
    "        \n",
    "        # Use TimeSeriesSplit for cross-validation\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        \n",
    "        # Create and fit GridSearchCV\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=pipeline,\n",
    "            param_grid=param_grid,\n",
    "            cv=tscv,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Get best parameters and model\n",
    "        best_params = grid_search.best_params_\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "        \n",
    "        # Use the best model\n",
    "        pipeline = grid_search.best_estimator_\n",
    "        \n",
    "    elif model_type == 'elastic_net' and tune_hyperparams:\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', ElasticNet(random_state=42))\n",
    "        ])\n",
    "        \n",
    "        # Define parameter grid\n",
    "        param_grid = {\n",
    "            'model__alpha': np.logspace(-3, 3, 10),\n",
    "            'model__l1_ratio': np.linspace(0.1, 0.9, 9)\n",
    "        }\n",
    "        \n",
    "        # Use TimeSeriesSplit for cross-validation\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        \n",
    "        # Create and fit GridSearchCV\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=pipeline,\n",
    "            param_grid=param_grid,\n",
    "            cv=tscv,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Get best parameters and model\n",
    "        best_params = grid_search.best_params_\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "        \n",
    "        # Use the best model\n",
    "        pipeline = grid_search.best_estimator_\n",
    "        \n",
    "    else:\n",
    "        # Use default parameters if not tuning\n",
    "        if model_type == 'ridge':\n",
    "            pipeline = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('model', Ridge(alpha=1.0, random_state=42))\n",
    "            ])\n",
    "        elif model_type == 'lasso':\n",
    "            pipeline = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('model', Lasso(alpha=0.1, random_state=42))\n",
    "            ])\n",
    "        elif model_type == 'elastic_net':\n",
    "            pipeline = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('model', ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42))\n",
    "            ])\n",
    "        else:\n",
    "            pipeline = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('model', LinearRegression())\n",
    "            ])\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = pipeline.predict(X_train)\n",
    "    y_pred_test = pipeline.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_rmse, train_mae, train_mape, train_r2 = calculate_metrics(y_train, y_pred_train)\n",
    "    test_rmse, test_mae, test_mape, test_r2 = calculate_metrics(y_test, y_pred_test)\n",
    "    \n",
    "    print(f\"Training - RMSE: {train_rmse:.4f}, MAE: {train_mae:.4f}, MAPE: {train_mape:.2f}%, R²: {train_r2:.4f}\")\n",
    "    print(f\"Testing - RMSE: {test_rmse:.4f}, MAE: {test_mae:.4f}, MAPE: {test_mape:.2f}%, R²: {test_r2:.4f}\")\n",
    "    \n",
    "    # Get coefficients\n",
    "    model = pipeline.named_steps['model']\n",
    "    coefficients = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Coefficient': model.coef_\n",
    "    }).sort_values('Coefficient', key=abs, ascending=False)\n",
    "    \n",
    "    # Add intercept\n",
    "    if hasattr(model, 'intercept_'):\n",
    "        print(f\"Intercept: {model.intercept_:.4f}\")\n",
    "    \n",
    "    # Plot coefficients\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = coefficients.head(20)\n",
    "    sns.barplot(x='Coefficient', y='Feature', data=top_features)\n",
    "    plt.title(f'Top 20 Coefficients for {model_type.capitalize()} Regression - {target_col}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/linear_reg_coefficients_{model_type}_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot actual vs predicted\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df.index[-len(y_test):], y_test, label='Actual', color='blue')\n",
    "    plt.plot(df.index[-len(y_test):], y_pred_test, label='Predicted', color='red', linestyle='--')\n",
    "    plt.title(f'{model_type.capitalize()} Regression: Actual vs Predicted {target_col}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(target_col)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/linear_reg_predictions_{model_type}_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create a DataFrame with predictions\n",
    "    predictions = pd.DataFrame({\n",
    "        'Actual': y_test,\n",
    "        'Predicted': y_pred_test,\n",
    "        'Error': y_test - y_pred_test\n",
    "    })\n",
    "    \n",
    "    # Plot error distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(predictions['Error'], kde=True)\n",
    "    plt.title(f'Error Distribution for {model_type.capitalize()} Regression - {target_col}')\n",
    "    plt.xlabel('Error')\n",
    "    plt.savefig(f'plots/linear_reg_error_distribution_{model_type}_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot residuals vs fitted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_pred_test, predictions['Error'])\n",
    "    plt.axhline(y=0, color='r', linestyle='-')\n",
    "    plt.title(f'Residuals vs Fitted Values for {model_type.capitalize()} Regression - {target_col}')\n",
    "    plt.xlabel('Fitted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.savefig(f'plots/linear_reg_residuals_{model_type}_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Return results\n",
    "    metrics = {\n",
    "        'train_rmse': train_rmse,\n",
    "        'train_mae': train_mae,\n",
    "        'train_mape': train_mape,\n",
    "        'train_r2': train_r2,\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_mae': test_mae,\n",
    "        'test_mape': test_mape,\n",
    "        'test_r2': test_r2\n",
    "    }\n",
    "    \n",
    "    return pipeline, predictions, coefficients, metrics\n",
    "\n",
    "def recursive_forecast(model, df, target_col, forecast_horizon=12):\n",
    "    \"\"\"\n",
    "    Generate recursive forecasts using the trained model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : Pipeline\n",
    "        Trained model pipeline\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with features\n",
    "    target_col : str\n",
    "        Name of the target column\n",
    "    forecast_horizon : int\n",
    "        Number of periods to forecast\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        Forecasted values\n",
    "    \"\"\"\n",
    "    print(f\"Generating {forecast_horizon} period forecast...\")\n",
    "    \n",
    "    # Create a copy of the dataframe\n",
    "    df_forecast = df.copy()\n",
    "    \n",
    "    # Get the last date in the dataframe\n",
    "    last_date = df_forecast.index[-1]\n",
    "    \n",
    "    # Create a list to store forecasts\n",
    "    forecasts = []\n",
    "    \n",
    "    # Generate forecasts recursively\n",
    "    for i in range(forecast_horizon):\n",
    "        # Get the features for the next period\n",
    "        if i == 0:\n",
    "            # For the first forecast, use the last row of the dataframe\n",
    "            X_next = df_forecast.drop([col for col in ['cpi_mom', 'cpi_yoy', target_col] if col in df_forecast.columns], axis=1).iloc[-1:].values\n",
    "        else:\n",
    "            # Update the dataframe with the previous forecast\n",
    "            # This requires updating all the lag features, rolling means, etc.\n",
    "            # For simplicity, we'll just use the last forecast as a naive prediction\n",
    "            X_next = df_forecast.drop([col for col in ['cpi_mom', 'cpi_yoy', target_col] if col in df_forecast.columns], axis=1).iloc[-1:].values\n",
    "        \n",
    "        # Make prediction\n",
    "        forecast = model.predict(X_next)[0]\n",
    "        \n",
    "        # Store forecast\n",
    "        forecasts.append(forecast)\n",
    "        \n",
    "        # Create next date\n",
    "        next_date = last_date + pd.DateOffset(months=i+1)\n",
    "        \n",
    "        # For a more accurate implementation, we would need to update all features\n",
    "        # based on the new forecast, but this is a simplified version\n",
    "    \n",
    "    # Create a Series with the forecasts\n",
    "    future_dates = pd.date_range(start=last_date + pd.DateOffset(months=1), periods=forecast_horizon, freq='MS')\n",
    "    forecast_series = pd.Series(forecasts, index=future_dates)\n",
    "    \n",
    "    # Plot historical data with forecasts\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df[target_col].index, df[target_col], label='Historical Data')\n",
    "    plt.plot(forecast_series.index, forecast_series, label='Forecast', color='red', linestyle='--')\n",
    "    plt.title(f'Linear Regression: {target_col} Forecast')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(target_col)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/linear_reg_future_forecast_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return forecast_series\n",
    "\n",
    "def main():\n",
    "    # Create plots directory if it doesn't exist\n",
    "    import os\n",
    "    if not os.path.exists('plots'):\n",
    "        os.makedirs('plots')\n",
    "    \n",
    "    # Load data\n",
    "    file_path = 'data/analyzed_time_series.csv'\n",
    "    df = load_data(file_path)\n",
    "    \n",
    "    # Engineer features\n",
    "    df_engineered = engineer_features(df)\n",
    "    \n",
    "    # List of models to run\n",
    "    models = ['linear', 'ridge', 'lasso', 'elastic_net']\n",
    "    \n",
    "    # Dictionary to store results\n",
    "    results = {}\n",
    "    \n",
    "    # Run models for CPI Year-over-Year\n",
    "    target_col = 'cpi_yoy'\n",
    "    for model_type in models:\n",
    "        print(f\"\\n{'-'*50}\")\n",
    "        print(f\"Running {model_type.capitalize()} Regression for {target_col}\")\n",
    "        print(f\"{'-'*50}\")\n",
    "        \n",
    "        model, predictions, coefficients, metrics = run_linear_regression(\n",
    "            df_engineered, target_col, test_size=0.2, model_type=model_type, tune_hyperparams=True\n",
    "        )\n",
    "        \n",
    "        results[f\"{model_type}_yoy\"] = {\n",
    "            'model': model,\n",
    "            'predictions': predictions,\n",
    "            'coefficients': coefficients,\n",
    "            'metrics': metrics\n",
    "        }\n",
    "        \n",
    "        # Generate future forecasts\n",
    "        forecast = recursive_forecast(model, df_engineered, target_col, forecast_horizon=24)\n",
    "        print(f\"\\nFuture {target_col} forecasts:\")\n",
    "        print(forecast)\n",
    "        \n",
    "        results[f\"{model_type}_yoy\"]['forecast'] = forecast\n",
    "    \n",
    "    # Run models for CPI Month-over-Month\n",
    "    target_col = 'cpi_mom'\n",
    "    for model_type in models:\n",
    "        print(f\"\\n{'-'*50}\")\n",
    "        print(f\"Running {model_type.capitalize()} Regression for {target_col}\")\n",
    "        print(f\"{'-'*50}\")\n",
    "        \n",
    "        model, predictions, coefficients, metrics = run_linear_regression(\n",
    "            df_engineered, target_col, test_size=0.2, model_type=model_type, tune_hyperparams=True\n",
    "        )\n",
    "        \n",
    "        results[f\"{model_type}_mom\"] = {\n",
    "            'model': model,\n",
    "            'predictions': predictions,\n",
    "            'coefficients': coefficients,\n",
    "            'metrics': metrics\n",
    "        }\n",
    "        \n",
    "        # Generate future forecasts\n",
    "        forecast = recursive_forecast(model, df_engineered, target_col, forecast_horizon=24)\n",
    "        print(f\"\\nFuture {target_col} forecasts:\")\n",
    "        print(forecast)\n",
    "        \n",
    "        results[f\"{model_type}_mom\"]['forecast'] = forecast\n",
    "    \n",
    "    # Compare models\n",
    "    print(\"\\n\\nModel Comparison - CPI Year-over-Year:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Model':<15} {'Train RMSE':<12} {'Test RMSE':<12} {'Train R²':<12} {'Test R²':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    for model_type in models:\n",
    "        metrics = results[f\"{model_type}_yoy\"]['metrics']\n",
    "        print(f\"{model_type.capitalize():<15} {metrics['train_rmse']:<12.4f} {metrics['test_rmse']:<12.4f} {metrics['train_r2']:<12.4f} {metrics['test_r2']:<12.4f}\")\n",
    "    \n",
    "    print(\"\\n\\nModel Comparison - CPI Month-over-Month:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Model':<15} {'Train RMSE':<12} {'Test RMSE':<12} {'Train R²':<12} {'Test R²':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    for model_type in models:\n",
    "        metrics = results[f\"{model_type}_mom\"]['metrics']\n",
    "        print(f\"{model_type.capitalize():<15} {metrics['train_rmse']:<12.4f} {metrics['test_rmse']:<12.4f} {metrics['train_r2']:<12.4f} {metrics['test_r2']:<12.4f}\")\n",
    "    \n",
    "    # Save metrics to CSV\n",
    "    metrics_data = []\n",
    "    for model_type in models:\n",
    "        metrics_yoy = results[f\"{model_type}_yoy\"]['metrics']\n",
    "        metrics_mom = results[f\"{model_type}_mom\"]['metrics']\n",
    "        \n",
    "        metrics_data.append({\n",
    "            'Model': f\"{model_type.capitalize()} Regression\",\n",
    "            'Target': 'CPI YoY',\n",
    "            'Train_RMSE': metrics_yoy['train_rmse'],\n",
    "            'Train_MAE': metrics_yoy['train_mae'],\n",
    "            'Train_MAPE': metrics_yoy['train_mape'],\n",
    "            'Train_R2': metrics_yoy['train_r2'],\n",
    "            'Test_RMSE': metrics_yoy['test_rmse'],\n",
    "            'Test_MAE': metrics_yoy['test_mae'],\n",
    "            'Test_MAPE': metrics_yoy['test_mape'],\n",
    "            'Test_R2': metrics_yoy['test_r2']\n",
    "        })\n",
    "        \n",
    "        metrics_data.append({\n",
    "            'Model': f\"{model_type.capitalize()} Regression\",\n",
    "            'Target': 'CPI MoM',\n",
    "            'Train_RMSE': metrics_mom['train_rmse'],\n",
    "            'Train_MAE': metrics_mom['train_mae'],\n",
    "            'Train_MAPE': metrics_mom['train_mape'],\n",
    "            'Train_R2': metrics_mom['train_r2'],\n",
    "            'Test_RMSE': metrics_mom['test_rmse'],\n",
    "            'Test_MAE': metrics_mom['test_mae'],\n",
    "            'Test_MAPE': metrics_mom['test_mape'],\n",
    "            'Test_R2': metrics_mom['test_r2']\n",
    "        })\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "    metrics_df.to_csv('plots/linear_regression_metrics.csv', index=False)\n",
    "    print(\"\\nMetrics saved to plots/linear_regression_metrics.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eea5f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Random Forest model for cpi_yoy...\n",
      "Training data: (269, 63), Test data: (68, 63)\n",
      "Performing hyperparameter tuning...\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Best parameters: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Training - RMSE: 0.1181, MAE: 0.0510, R²: 0.9996\n",
      "Testing - RMSE: 0.0550, MAE: 0.0405, R²: 0.9980\n",
      "Generating 24 period forecast...\n",
      "\n",
      "Future cpi_yoy forecasts:\n",
      "2025-01-01    102.985725\n",
      "2025-02-01    102.985725\n",
      "2025-03-01    102.985725\n",
      "2025-04-01    102.985725\n",
      "2025-05-01    102.985725\n",
      "2025-06-01    102.985725\n",
      "2025-07-01    102.985725\n",
      "2025-08-01    102.985725\n",
      "2025-09-01    102.985725\n",
      "2025-10-01    102.985725\n",
      "2025-11-01    102.985725\n",
      "2025-12-01    102.985725\n",
      "2026-01-01    102.985725\n",
      "2026-02-01    102.985725\n",
      "2026-03-01    102.985725\n",
      "2026-04-01    102.985725\n",
      "2026-05-01    102.985725\n",
      "2026-06-01    102.985725\n",
      "2026-07-01    102.985725\n",
      "2026-08-01    102.985725\n",
      "2026-09-01    102.985725\n",
      "2026-10-01    102.985725\n",
      "2026-11-01    102.985725\n",
      "2026-12-01    102.985725\n",
      "Freq: MS, dtype: float64\n",
      "Running Random Forest model for cpi_mom...\n",
      "Training data: (269, 63), Test data: (68, 63)\n",
      "Performing hyperparameter tuning...\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Best parameters: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Training - RMSE: 0.0321, MAE: 0.0097, R²: 0.9984\n",
      "Testing - RMSE: 0.0892, MAE: 0.0202, R²: 0.9647\n",
      "Generating 24 period forecast...\n",
      "\n",
      "Future cpi_mom forecasts:\n",
      "2025-01-01    100.3002\n",
      "2025-02-01    100.3002\n",
      "2025-03-01    100.3002\n",
      "2025-04-01    100.3002\n",
      "2025-05-01    100.3002\n",
      "2025-06-01    100.3002\n",
      "2025-07-01    100.3002\n",
      "2025-08-01    100.3002\n",
      "2025-09-01    100.3002\n",
      "2025-10-01    100.3002\n",
      "2025-11-01    100.3002\n",
      "2025-12-01    100.3002\n",
      "2026-01-01    100.3002\n",
      "2026-02-01    100.3002\n",
      "2026-03-01    100.3002\n",
      "2026-04-01    100.3002\n",
      "2026-05-01    100.3002\n",
      "2026-06-01    100.3002\n",
      "2026-07-01    100.3002\n",
      "2026-08-01    100.3002\n",
      "2026-09-01    100.3002\n",
      "2026-10-01    100.3002\n",
      "2026-11-01    100.3002\n",
      "2026-12-01    100.3002\n",
      "Freq: MS, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load and prepare the time series data\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert time column to datetime\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    \n",
    "    # Set time as index\n",
    "    df.set_index('time', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"Create additional features for modeling\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Create lag features\n",
    "    for i in range(1, 13):\n",
    "        if 'cpi_mom' in df_copy.columns:\n",
    "            df_copy[f'cpi_mom_lag_{i}'] = df_copy['cpi_mom'].shift(i)\n",
    "        if 'cpi_yoy' in df_copy.columns:\n",
    "            df_copy[f'cpi_yoy_lag_{i}'] = df_copy['cpi_yoy'].shift(i)\n",
    "    \n",
    "    # Create rolling window features\n",
    "    for window in [3, 6, 12]:\n",
    "        if 'cpi_mom' in df_copy.columns:\n",
    "            df_copy[f'cpi_mom_rolling_mean_{window}'] = df_copy['cpi_mom'].rolling(window=window).mean()\n",
    "            df_copy[f'cpi_mom_rolling_std_{window}'] = df_copy['cpi_mom'].rolling(window=window).std()\n",
    "        if 'cpi_yoy' in df_copy.columns:\n",
    "            df_copy[f'cpi_yoy_rolling_mean_{window}'] = df_copy['cpi_yoy'].rolling(window=window).mean()\n",
    "            df_copy[f'cpi_yoy_rolling_std_{window}'] = df_copy['cpi_yoy'].rolling(window=window).std()\n",
    "    \n",
    "    # Create economic indicator lag features\n",
    "    for i in range(1, 4):\n",
    "        if 'oil_price' in df_copy.columns:\n",
    "            df_copy[f'oil_price_lag_{i}'] = df_copy['oil_price'].shift(i)\n",
    "        if 'gold_price' in df_copy.columns:\n",
    "            df_copy[f'gold_price_lag_{i}'] = df_copy['gold_price'].shift(i)\n",
    "        if 'interest_rate' in df_copy.columns:\n",
    "            df_copy[f'interest_rate_lag_{i}'] = df_copy['interest_rate'].shift(i)\n",
    "    \n",
    "    # Add month and year as cyclical features\n",
    "    if 'month' in df_copy.columns:\n",
    "        df_copy['month_sin'] = np.sin(2 * np.pi * df_copy['month']/12)\n",
    "        df_copy['month_cos'] = np.cos(2 * np.pi * df_copy['month']/12)\n",
    "    \n",
    "    # Create interaction features\n",
    "    if all(col in df_copy.columns for col in ['oil_price', 'gold_price']):\n",
    "        df_copy['oil_gold_ratio'] = df_copy['oil_price'] / df_copy['gold_price']\n",
    "    \n",
    "    # Drop rows with NaN values (due to lag features)\n",
    "    df_clean = df_copy.dropna()\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def run_rf(df, target_col, test_size=0.2, tune_hyperparams=True):\n",
    "    \"\"\"\n",
    "    Train and evaluate a Random Forest model for time series forecasting\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with engineered features\n",
    "    target_col : str\n",
    "        Name of the target column\n",
    "    test_size : float\n",
    "        Proportion of data to use for testing\n",
    "    tune_hyperparams : bool\n",
    "        Whether to perform hyperparameter tuning\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        Model, predictions, feature importances, and metrics\n",
    "    \"\"\"\n",
    "    print(f\"Running Random Forest model for {target_col}...\")\n",
    "    \n",
    "    # Define features and target\n",
    "    X = df.drop([col for col in ['cpi_mom', 'cpi_yoy', target_col] if col in df.columns], axis=1)\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Split data into training and testing sets (time-based split)\n",
    "    split_idx = int(len(df) * (1 - test_size))\n",
    "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "    \n",
    "    print(f\"Training data: {X_train.shape}, Test data: {X_test.shape}\")\n",
    "    \n",
    "    if tune_hyperparams:\n",
    "        print(\"Performing hyperparameter tuning...\")\n",
    "        # Define parameter grid\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "        \n",
    "        # Use TimeSeriesSplit for cross-validation\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        \n",
    "        # Create and fit GridSearchCV\n",
    "        rf_grid = GridSearchCV(\n",
    "            estimator=RandomForestRegressor(random_state=42),\n",
    "            param_grid=param_grid,\n",
    "            cv=tscv,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        rf_grid.fit(X_train, y_train)\n",
    "        \n",
    "        # Get best parameters and model\n",
    "        best_params = rf_grid.best_params_\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "        \n",
    "        # Train model with best parameters\n",
    "        model = RandomForestRegressor(random_state=42, **best_params)\n",
    "    else:\n",
    "        # Use default parameters\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=None,\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "    train_rmse = np.sqrt(train_mse)\n",
    "    train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "    train_mape = np.mean(np.abs((y_train - y_pred_train) / y_train)) * 100\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    \n",
    "    test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "    test_rmse = np.sqrt(test_mse)\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    test_mape = np.mean(np.abs((y_test - y_pred_test) / y_test)) * 100\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    print(f\"Training - RMSE: {train_rmse:.4f}, MAE: {train_mae:.4f}, R²: {train_r2:.4f}\")\n",
    "    print(f\"Testing - RMSE: {test_rmse:.4f}, MAE: {test_mae:.4f}, R²: {test_r2:.4f}\")\n",
    "    \n",
    "    # Get feature importances\n",
    "    feature_importances = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Importance': model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Plot feature importances\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importances.head(20))\n",
    "    plt.title(f'Top 20 Feature Importances for {target_col}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/rf_feature_importance_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot actual vs predicted\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df.index[-len(y_test):], y_test, label='Actual', color='blue')\n",
    "    plt.plot(df.index[-len(y_test):], y_pred_test, label='Predicted', color='red', linestyle='--')\n",
    "    plt.title(f'Random Forest: Actual vs Predicted {target_col}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(target_col)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/rf_predictions_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create a DataFrame with predictions\n",
    "    predictions = pd.DataFrame({\n",
    "        'Actual': y_test,\n",
    "        'Predicted': y_pred_test,\n",
    "        'Error': y_test - y_pred_test\n",
    "    })\n",
    "    \n",
    "    # Plot error distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(predictions['Error'], kde=True)\n",
    "    plt.title(f'Error Distribution for {target_col}')\n",
    "    plt.xlabel('Error')\n",
    "    plt.savefig(f'plots/rf_error_distribution_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Return results\n",
    "    metrics = {\n",
    "        'train_rmse': train_rmse,\n",
    "        'train_mae': train_mae,\n",
    "        'train_mape': train_mape,\n",
    "        'train_r2': train_r2,\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_mae': test_mae,\n",
    "        'test_mape': test_mape,\n",
    "        'test_r2': test_r2\n",
    "    }\n",
    "    \n",
    "    return model, predictions, feature_importances, metrics\n",
    "\n",
    "def recursive_forecast(model, df, target_col, forecast_horizon=12):\n",
    "    \"\"\"\n",
    "    Generate recursive forecasts using the trained model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : RandomForestRegressor\n",
    "        Trained model\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with features\n",
    "    target_col : str\n",
    "        Name of the target column\n",
    "    forecast_horizon : int\n",
    "        Number of periods to forecast\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        Forecasted values\n",
    "    \"\"\"\n",
    "    print(f\"Generating {forecast_horizon} period forecast...\")\n",
    "    \n",
    "    # Create a copy of the dataframe\n",
    "    df_forecast = df.copy()\n",
    "    \n",
    "    # Get the last date in the dataframe\n",
    "    last_date = df_forecast.index[-1]\n",
    "    \n",
    "    # Create a list to store forecasts\n",
    "    forecasts = []\n",
    "    \n",
    "    # Generate forecasts recursively\n",
    "    for i in range(forecast_horizon):\n",
    "        # Get the features for the next period\n",
    "        if i == 0:\n",
    "            # For the first forecast, use the last row of the dataframe\n",
    "            X_next = df_forecast.drop([col for col in ['cpi_mom', 'cpi_yoy', target_col] if col in df_forecast.columns], axis=1).iloc[-1:].values\n",
    "        else:\n",
    "            # Update the dataframe with the previous forecast\n",
    "            # This requires updating all the lag features, rolling means, etc.\n",
    "            # For simplicity, we'll just use the last forecast as a naive prediction\n",
    "            X_next = df_forecast.drop([col for col in ['cpi_mom', 'cpi_yoy', target_col] if col in df_forecast.columns], axis=1).iloc[-1:].values\n",
    "        \n",
    "        # Make prediction\n",
    "        forecast = model.predict(X_next)[0]\n",
    "        \n",
    "        # Store forecast\n",
    "        forecasts.append(forecast)\n",
    "        \n",
    "        # Create next date\n",
    "        next_date = last_date + pd.DateOffset(months=i+1)\n",
    "        \n",
    "        # For a more accurate implementation, we would need to update all features\n",
    "        # based on the new forecast, but this is a simplified version\n",
    "    \n",
    "    # Create a Series with the forecasts\n",
    "    future_dates = pd.date_range(start=last_date + pd.DateOffset(months=1), periods=forecast_horizon, freq='MS')\n",
    "    forecast_series = pd.Series(forecasts, index=future_dates)\n",
    "    \n",
    "    # Plot historical data with forecasts\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df[target_col].index, df[target_col], label='Historical Data')\n",
    "    plt.plot(forecast_series.index, forecast_series, label='Forecast', color='red', linestyle='--')\n",
    "    plt.title(f'Random Forest: {target_col} Forecast')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(target_col)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/rf_future_forecast_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return forecast_series\n",
    "\n",
    "def main():\n",
    "    # Create plots directory if it doesn't exist\n",
    "    import os\n",
    "    if not os.path.exists('plots'):\n",
    "        os.makedirs('plots')\n",
    "    \n",
    "    # Load data\n",
    "    file_path = 'data/analyzed_time_series.csv'\n",
    "    df = load_data(file_path)\n",
    "    \n",
    "    # Engineer features\n",
    "    df_engineered = engineer_features(df)\n",
    "    \n",
    "    # Run Random Forest for CPI Year-over-Year\n",
    "    target_col = 'cpi_yoy'\n",
    "    model_yoy, predictions_yoy, feature_importances_yoy, metrics_yoy = run_rf(\n",
    "        df_engineered, target_col, test_size=0.2, tune_hyperparams=True\n",
    "    )\n",
    "    \n",
    "    # Generate future forecasts for YoY\n",
    "    forecast_yoy = recursive_forecast(model_yoy, df_engineered, target_col, forecast_horizon=24)\n",
    "    print(f\"\\nFuture {target_col} forecasts:\")\n",
    "    print(forecast_yoy)\n",
    "    \n",
    "    # Run Random Forest for CPI Month-over-Month\n",
    "    target_col = 'cpi_mom'\n",
    "    model_mom, predictions_mom, feature_importances_mom, metrics_mom = run_rf(\n",
    "        df_engineered, target_col, test_size=0.2, tune_hyperparams=True\n",
    "    )\n",
    "    \n",
    "    # Generate future forecasts for MoM\n",
    "    forecast_mom = recursive_forecast(model_mom, df_engineered, target_col, forecast_horizon=24)\n",
    "    print(f\"\\nFuture {target_col} forecasts:\")\n",
    "    print(forecast_mom)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8f1c521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running XGBoost model for cpi_yoy...\n",
      "Training data: (269, 65), Test data: (68, 65)\n",
      "Performing hyperparameter tuning...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters: {'subsample': 1.0, 'n_estimators': 100, 'min_child_weight': 3, 'max_depth': 3, 'learning_rate': 0.1, 'gamma': 0, 'colsample_bytree': 1.0}\n",
      "Training - RMSE: 0.0362, MAE: 0.0228, MAPE: 0.02%, R²: 1.0000\n",
      "Testing - RMSE: 0.0989, MAE: 0.0631, MAPE: 0.06%, R²: 0.9934\n",
      "Generating 24 period forecast...\n",
      "\n",
      "Future cpi_yoy forecasts:\n",
      "2025-01-01    102.901375\n",
      "2025-02-01    102.901375\n",
      "2025-03-01    102.901375\n",
      "2025-04-01    102.901375\n",
      "2025-05-01    102.901375\n",
      "2025-06-01    102.901375\n",
      "2025-07-01    102.901375\n",
      "2025-08-01    102.901375\n",
      "2025-09-01    102.901375\n",
      "2025-10-01    102.901375\n",
      "2025-11-01    102.901375\n",
      "2025-12-01    102.901375\n",
      "2026-01-01    102.901375\n",
      "2026-02-01    102.901375\n",
      "2026-03-01    102.901375\n",
      "2026-04-01    102.901375\n",
      "2026-05-01    102.901375\n",
      "2026-06-01    102.901375\n",
      "2026-07-01    102.901375\n",
      "2026-08-01    102.901375\n",
      "2026-09-01    102.901375\n",
      "2026-10-01    102.901375\n",
      "2026-11-01    102.901375\n",
      "2026-12-01    102.901375\n",
      "Freq: MS, dtype: float32\n",
      "Running XGBoost model for cpi_mom...\n",
      "Training data: (269, 65), Test data: (68, 65)\n",
      "Performing hyperparameter tuning...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters: {'subsample': 1.0, 'n_estimators': 200, 'min_child_weight': 1, 'max_depth': 5, 'learning_rate': 0.1, 'gamma': 0, 'colsample_bytree': 1.0}\n",
      "Training - RMSE: 0.0004, MAE: 0.0002, MAPE: 0.00%, R²: 1.0000\n",
      "Testing - RMSE: 0.0764, MAE: 0.0199, MAPE: 0.02%, R²: 0.9741\n",
      "Generating 24 period forecast...\n",
      "\n",
      "Future cpi_mom forecasts:\n",
      "2025-01-01    100.270111\n",
      "2025-02-01    100.270111\n",
      "2025-03-01    100.270111\n",
      "2025-04-01    100.270111\n",
      "2025-05-01    100.270111\n",
      "2025-06-01    100.270111\n",
      "2025-07-01    100.270111\n",
      "2025-08-01    100.270111\n",
      "2025-09-01    100.270111\n",
      "2025-10-01    100.270111\n",
      "2025-11-01    100.270111\n",
      "2025-12-01    100.270111\n",
      "2026-01-01    100.270111\n",
      "2026-02-01    100.270111\n",
      "2026-03-01    100.270111\n",
      "2026-04-01    100.270111\n",
      "2026-05-01    100.270111\n",
      "2026-06-01    100.270111\n",
      "2026-07-01    100.270111\n",
      "2026-08-01    100.270111\n",
      "2026-09-01    100.270111\n",
      "2026-10-01    100.270111\n",
      "2026-11-01    100.270111\n",
      "2026-12-01    100.270111\n",
      "Freq: MS, dtype: float32\n",
      "\n",
      "Metrics saved to plots/xgboost_metrics.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load and prepare the time series data\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert time column to datetime\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    \n",
    "    # Set time as index\n",
    "    df.set_index('time', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"Create additional features for modeling\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Create lag features\n",
    "    for i in range(1, 13):\n",
    "        if 'cpi_mom' in df_copy.columns:\n",
    "            df_copy[f'cpi_mom_lag_{i}'] = df_copy['cpi_mom'].shift(i)\n",
    "        if 'cpi_yoy' in df_copy.columns:\n",
    "            df_copy[f'cpi_yoy_lag_{i}'] = df_copy['cpi_yoy'].shift(i)\n",
    "    \n",
    "    # Create rolling window features\n",
    "    for window in [3, 6, 12]:\n",
    "        if 'cpi_mom' in df_copy.columns:\n",
    "            df_copy[f'cpi_mom_rolling_mean_{window}'] = df_copy['cpi_mom'].rolling(window=window).mean()\n",
    "            df_copy[f'cpi_mom_rolling_std_{window}'] = df_copy['cpi_mom'].rolling(window=window).std()\n",
    "        if 'cpi_yoy' in df_copy.columns:\n",
    "            df_copy[f'cpi_yoy_rolling_mean_{window}'] = df_copy['cpi_yoy'].rolling(window=window).mean()\n",
    "            df_copy[f'cpi_yoy_rolling_std_{window}'] = df_copy['cpi_yoy'].rolling(window=window).std()\n",
    "    \n",
    "    # Create economic indicator lag features\n",
    "    for i in range(1, 4):\n",
    "        if 'oil_price' in df_copy.columns:\n",
    "            df_copy[f'oil_price_lag_{i}'] = df_copy['oil_price'].shift(i)\n",
    "        if 'gold_price' in df_copy.columns:\n",
    "            df_copy[f'gold_price_lag_{i}'] = df_copy['gold_price'].shift(i)\n",
    "        if 'interest_rate' in df_copy.columns:\n",
    "            df_copy[f'interest_rate_lag_{i}'] = df_copy['interest_rate'].shift(i)\n",
    "    \n",
    "    # Add month and year as cyclical features\n",
    "    if 'month' in df_copy.columns:\n",
    "        df_copy['month_sin'] = np.sin(2 * np.pi * df_copy['month']/12)\n",
    "        df_copy['month_cos'] = np.cos(2 * np.pi * df_copy['month']/12)\n",
    "    \n",
    "    # Create interaction features\n",
    "    if all(col in df_copy.columns for col in ['oil_price', 'gold_price']):\n",
    "        df_copy['oil_gold_ratio'] = df_copy['oil_price'] / df_copy['gold_price']\n",
    "    \n",
    "    # Create momentum features\n",
    "    if 'cpi_yoy' in df_copy.columns:\n",
    "        df_copy['cpi_yoy_momentum'] = df_copy['cpi_yoy'].diff()\n",
    "    if 'cpi_mom' in df_copy.columns:\n",
    "        df_copy['cpi_mom_momentum'] = df_copy['cpi_mom'].diff()\n",
    "    \n",
    "    # Drop rows with NaN values (due to lag features)\n",
    "    df_clean = df_copy.dropna()\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def calculate_metrics(actual, predicted):\n",
    "    \"\"\"Calculate evaluation metrics\"\"\"\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    r2 = r2_score(actual, predicted)\n",
    "    mape = np.mean(np.abs((actual - predicted) / actual)) * 100\n",
    "    \n",
    "    return rmse, mae, mape, r2\n",
    "\n",
    "def run_xgboost(df, target_col, test_size=0.2, tune_hyperparams=True):\n",
    "    \"\"\"\n",
    "    Train and evaluate an XGBoost model for time series forecasting\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with engineered features\n",
    "    target_col : str\n",
    "        Name of the target column\n",
    "    test_size : float\n",
    "        Proportion of data to use for testing\n",
    "    tune_hyperparams : bool\n",
    "        Whether to perform hyperparameter tuning\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        Model, predictions, feature importances, metrics, and SHAP values\n",
    "    \"\"\"\n",
    "    print(f\"Running XGBoost model for {target_col}...\")\n",
    "    \n",
    "    # Define features and target\n",
    "    X = df.drop([col for col in ['cpi_mom', 'cpi_yoy', target_col] if col in df.columns], axis=1)\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Split data into training and testing sets (time-based split)\n",
    "    split_idx = int(len(df) * (1 - test_size))\n",
    "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "    \n",
    "    print(f\"Training data: {X_train.shape}, Test data: {X_test.shape}\")\n",
    "    \n",
    "    if tune_hyperparams:\n",
    "        print(\"Performing hyperparameter tuning...\")\n",
    "        # Define parameter grid\n",
    "        # param_grid = {\n",
    "        #     'n_estimators': [100, 200, 300],\n",
    "        #     'max_depth': [3, 5, 7, 9],\n",
    "        #     'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        #     'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "        #     'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "        #     'gamma': [0, 0.1, 0.2],\n",
    "        #     'min_child_weight': [1, 3, 5]\n",
    "        # }\n",
    "        \n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [3, 5],\n",
    "            'learning_rate': [0.05, 0.1],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'colsample_bytree': [0.8, 1.0],\n",
    "            'gamma': [0],\n",
    "            'min_child_weight': [1, 3]\n",
    "        }\n",
    "        # Use TimeSeriesSplit for cross-validation\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        \n",
    "        # Create and fit GridSearchCV\n",
    "        # xgb_grid = GridSearchCV(\n",
    "        #     estimator=XGBRegressor(objective='reg:squarederror', random_state=42),\n",
    "        #     param_grid=param_grid,\n",
    "        #     cv=tscv,\n",
    "        #     scoring='neg_mean_squared_error',\n",
    "        #     n_jobs=-1,\n",
    "        #     verbose=1\n",
    "        # )\n",
    "        xgb_random = RandomizedSearchCV(\n",
    "            estimator=XGBRegressor(objective='reg:squarederror', random_state=42),\n",
    "            param_distributions=param_grid,\n",
    "            n_iter=10,  # ít hơn nhiều so với grid search\n",
    "            cv=TimeSeriesSplit(n_splits=3),  # ít split hơn\n",
    "            scoring='neg_mean_squared_error',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        xgb_random.fit(X_train, y_train)\n",
    "        \n",
    "        # Get best parameters and model\n",
    "        best_params = xgb_random.best_params_\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "        \n",
    "        # Train model with best parameters\n",
    "        model = XGBRegressor(objective='reg:squarederror', random_state=42, **best_params)\n",
    "    else:\n",
    "        # Use default parameters\n",
    "        model = XGBRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=5,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            gamma=0,\n",
    "            min_child_weight=1,\n",
    "            objective='reg:squarederror',\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_rmse, train_mae, train_mape, train_r2 = calculate_metrics(y_train, y_pred_train)\n",
    "    test_rmse, test_mae, test_mape, test_r2 = calculate_metrics(y_test, y_pred_test)\n",
    "    \n",
    "    print(f\"Training - RMSE: {train_rmse:.4f}, MAE: {train_mae:.4f}, MAPE: {train_mape:.2f}%, R²: {train_r2:.4f}\")\n",
    "    print(f\"Testing - RMSE: {test_rmse:.4f}, MAE: {test_mae:.4f}, MAPE: {test_mape:.2f}%, R²: {test_r2:.4f}\")\n",
    "    \n",
    "    # Get feature importances\n",
    "    feature_importances = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Importance': model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Plot feature importances\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importances.head(20))\n",
    "    plt.title(f'Top 20 Feature Importances for {target_col}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/xgb_feature_importance_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot actual vs predicted\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df.index[-len(y_test):], y_test, label='Actual', color='blue')\n",
    "    plt.plot(df.index[-len(y_test):], y_pred_test, label='Predicted', color='red', linestyle='--')\n",
    "    plt.title(f'XGBoost: Actual vs Predicted {target_col}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(target_col)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/xgb_predictions_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create a DataFrame with predictions\n",
    "    predictions = pd.DataFrame({\n",
    "        'Actual': y_test,\n",
    "        'Predicted': y_pred_test,\n",
    "        'Error': y_test - y_pred_test\n",
    "    })\n",
    "    \n",
    "    # Plot error distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(predictions['Error'], kde=True)\n",
    "    plt.title(f'Error Distribution for {target_col}')\n",
    "    plt.xlabel('Error')\n",
    "    plt.savefig(f'plots/xgb_error_distribution_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Calculate SHAP values\n",
    "    try:\n",
    "        explainer = shap.Explainer(model)\n",
    "        shap_values = explainer(X_test)\n",
    "        \n",
    "        # Plot SHAP summary\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        shap.summary_plot(shap_values, X_test, show=False)\n",
    "        plt.title(f'SHAP Feature Importance for {target_col}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/xgb_shap_summary_{target_col}.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Plot SHAP dependence plots for top features\n",
    "        top_features = feature_importances['Feature'].head(3).tolist()\n",
    "        for feature in top_features:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            shap.dependence_plot(feature, shap_values.values, X_test, show=False)\n",
    "            plt.title(f'SHAP Dependence Plot for {feature}')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'plots/xgb_shap_dependence_{feature}_{target_col}.png')\n",
    "            plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating SHAP values: {e}\")\n",
    "        shap_values = None\n",
    "    \n",
    "    # Return results\n",
    "    metrics = {\n",
    "        'train_rmse': train_rmse,\n",
    "        'train_mae': train_mae,\n",
    "        'train_mape': train_mape,\n",
    "        'train_r2': train_r2,\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_mae': test_mae,\n",
    "        'test_mape': test_mape,\n",
    "        'test_r2': test_r2\n",
    "    }\n",
    "    \n",
    "    return model, predictions, feature_importances, metrics, shap_values\n",
    "\n",
    "def recursive_forecast(model, df, target_col, forecast_horizon=12):\n",
    "    \"\"\"\n",
    "    Generate recursive forecasts using the trained model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : XGBRegressor\n",
    "        Trained model\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with features\n",
    "    target_col : str\n",
    "        Name of the target column\n",
    "    forecast_horizon : int\n",
    "        Number of periods to forecast\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        Forecasted values\n",
    "    \"\"\"\n",
    "    print(f\"Generating {forecast_horizon} period forecast...\")\n",
    "    \n",
    "    # Create a copy of the dataframe\n",
    "    df_forecast = df.copy()\n",
    "    \n",
    "    # Get the last date in the dataframe\n",
    "    last_date = df_forecast.index[-1]\n",
    "    \n",
    "    # Create a list to store forecasts\n",
    "    forecasts = []\n",
    "    \n",
    "    # Generate forecasts recursively\n",
    "    for i in range(forecast_horizon):\n",
    "        # Get the features for the next period\n",
    "        if i == 0:\n",
    "            # For the first forecast, use the last row of the dataframe\n",
    "            X_next = df_forecast.drop([col for col in ['cpi_mom', 'cpi_yoy', target_col] if col in df_forecast.columns], axis=1).iloc[-1:].values\n",
    "        else:\n",
    "            # Update the dataframe with the previous forecast\n",
    "            # This requires updating all the lag features, rolling means, etc.\n",
    "            # For simplicity, we'll just use the last forecast as a naive prediction\n",
    "            X_next = df_forecast.drop([col for col in ['cpi_mom', 'cpi_yoy', target_col] if col in df_forecast.columns], axis=1).iloc[-1:].values\n",
    "        \n",
    "        # Make prediction\n",
    "        forecast = model.predict(X_next)[0]\n",
    "        \n",
    "        # Store forecast\n",
    "        forecasts.append(forecast)\n",
    "        \n",
    "        # Create next date\n",
    "        next_date = last_date + pd.DateOffset(months=i+1)\n",
    "        \n",
    "        # For a more accurate implementation, we would need to update all features\n",
    "        # based on the new forecast, but this is a simplified version\n",
    "    \n",
    "    # Create a Series with the forecasts\n",
    "    future_dates = pd.date_range(start=last_date + pd.DateOffset(months=1), periods=forecast_horizon, freq='MS')\n",
    "    forecast_series = pd.Series(forecasts, index=future_dates)\n",
    "    \n",
    "    # Plot historical data with forecasts\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df[target_col].index, df[target_col], label='Historical Data')\n",
    "    plt.plot(forecast_series.index, forecast_series, label='Forecast', color='red', linestyle='--')\n",
    "    plt.title(f'XGBoost: {target_col} Forecast')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(target_col)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/xgb_future_forecast_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return forecast_series\n",
    "\n",
    "def main():\n",
    "    # Create plots directory if it doesn't exist\n",
    "    import os\n",
    "    if not os.path.exists('plots'):\n",
    "        os.makedirs('plots')\n",
    "    \n",
    "    # Load data\n",
    "    file_path = 'data/analyzed_time_series.csv'\n",
    "    df = load_data(file_path)\n",
    "    \n",
    "    # Engineer features\n",
    "    df_engineered = engineer_features(df)\n",
    "    \n",
    "    # Run XGBoost for CPI Year-over-Year\n",
    "    target_col = 'cpi_yoy'\n",
    "    model_yoy, predictions_yoy, feature_importances_yoy, metrics_yoy, shap_values_yoy = run_xgboost(\n",
    "        df_engineered, target_col, test_size=0.2, tune_hyperparams=True\n",
    "    )\n",
    "    \n",
    "    # Generate future forecasts for YoY\n",
    "    forecast_yoy = recursive_forecast(model_yoy, df_engineered, target_col, forecast_horizon=24)\n",
    "    print(f\"\\nFuture {target_col} forecasts:\")\n",
    "    print(forecast_yoy)\n",
    "    \n",
    "    # Run XGBoost for CPI Month-over-Month\n",
    "    target_col = 'cpi_mom'\n",
    "    model_mom, predictions_mom, feature_importances_mom, metrics_mom, shap_values_mom = run_xgboost(\n",
    "        df_engineered, target_col, test_size=0.2, tune_hyperparams=True\n",
    "    )\n",
    "    \n",
    "    # Generate future forecasts for MoM\n",
    "    forecast_mom = recursive_forecast(model_mom, df_engineered, target_col, forecast_horizon=24)\n",
    "    print(f\"\\nFuture {target_col} forecasts:\")\n",
    "    print(forecast_mom)\n",
    "    \n",
    "    # Save metrics to CSV\n",
    "    metrics_data = [\n",
    "        {\n",
    "            'Model': 'XGBoost', 'Target': 'CPI MoM',\n",
    "            'Train_RMSE': metrics_mom['train_rmse'], 'Train_MAE': metrics_mom['train_mae'], \n",
    "            'Train_MAPE': metrics_mom['train_mape'], 'Train_R2': metrics_mom['train_r2'],\n",
    "            'Test_RMSE': metrics_mom['test_rmse'], 'Test_MAE': metrics_mom['test_mae'], \n",
    "            'Test_MAPE': metrics_mom['test_mape'], 'Test_R2': metrics_mom['test_r2']\n",
    "        },\n",
    "        {\n",
    "            'Model': 'XGBoost', 'Target': 'CPI YoY',\n",
    "            'Train_RMSE': metrics_yoy['train_rmse'], 'Train_MAE': metrics_yoy['train_mae'], \n",
    "            'Train_MAPE': metrics_yoy['train_mape'], 'Train_R2': metrics_yoy['train_r2'],\n",
    "            'Test_RMSE': metrics_yoy['test_rmse'], 'Test_MAE': metrics_yoy['test_mae'], \n",
    "            'Test_MAPE': metrics_yoy['test_mape'], 'Test_R2': metrics_yoy['test_r2']\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "    metrics_df.to_csv('plots/xgboost_metrics.csv', index=False)\n",
    "    print(\"\\nMetrics saved to plots/xgboost_metrics.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb58020d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LightGBM model for cpi_yoy...\n",
      "Training data: (269, 65), Test data: (68, 65)\n",
      "Performing hyperparameter tuning...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001057 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4914\n",
      "[LightGBM] [Info] Number of data points in the train set: 269, number of used features: 65\n",
      "[LightGBM] [Info] Start training from score 106.334610\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Best parameters: {'subsample': 1.0, 'reg_lambda': 0.5, 'reg_alpha': 0, 'num_leaves': 127, 'n_estimators': 200, 'max_depth': 9, 'learning_rate': 0.2, 'colsample_bytree': 0.7}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000519 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4914\n",
      "[LightGBM] [Info] Number of data points in the train set: 269, number of used features: 65\n",
      "[LightGBM] [Info] Start training from score 106.334610\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training - RMSE: 0.0401, MAE: 0.0149, MAPE: 0.01%, R²: 1.0000\n",
      "Testing - RMSE: 0.2337, MAE: 0.1949, MAPE: 0.19%, R²: 0.9633\n",
      "Generating 24 period forecast...\n",
      "\n",
      "Future cpi_yoy forecasts:\n",
      "2025-01-01    102.761138\n",
      "2025-02-01    102.761138\n",
      "2025-03-01    102.761138\n",
      "2025-04-01    102.761138\n",
      "2025-05-01    102.761138\n",
      "2025-06-01    102.761138\n",
      "2025-07-01    102.761138\n",
      "2025-08-01    102.761138\n",
      "2025-09-01    102.761138\n",
      "2025-10-01    102.761138\n",
      "2025-11-01    102.761138\n",
      "2025-12-01    102.761138\n",
      "2026-01-01    102.761138\n",
      "2026-02-01    102.761138\n",
      "2026-03-01    102.761138\n",
      "2026-04-01    102.761138\n",
      "2026-05-01    102.761138\n",
      "2026-06-01    102.761138\n",
      "2026-07-01    102.761138\n",
      "2026-08-01    102.761138\n",
      "2026-09-01    102.761138\n",
      "2026-10-01    102.761138\n",
      "2026-11-01    102.761138\n",
      "2026-12-01    102.761138\n",
      "Freq: MS, dtype: float64\n",
      "Running LightGBM model for cpi_mom...\n",
      "Training data: (269, 65), Test data: (68, 65)\n",
      "Performing hyperparameter tuning...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000695 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4914\n",
      "[LightGBM] [Info] Number of data points in the train set: 269, number of used features: 65\n",
      "[LightGBM] [Info] Start training from score 100.505093\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Best parameters: {'subsample': 0.8, 'reg_lambda': 0.1, 'reg_alpha': 0.5, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.05, 'colsample_bytree': 0.7}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000512 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4914\n",
      "[LightGBM] [Info] Number of data points in the train set: 269, number of used features: 65\n",
      "[LightGBM] [Info] Start training from score 100.505093\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training - RMSE: 0.1332, MAE: 0.0534, MAPE: 0.05%, R²: 0.9726\n",
      "Testing - RMSE: 0.1206, MAE: 0.0431, MAPE: 0.04%, R²: 0.9355\n",
      "Generating 24 period forecast...\n",
      "\n",
      "Future cpi_mom forecasts:\n",
      "2025-01-01    100.297099\n",
      "2025-02-01    100.297099\n",
      "2025-03-01    100.297099\n",
      "2025-04-01    100.297099\n",
      "2025-05-01    100.297099\n",
      "2025-06-01    100.297099\n",
      "2025-07-01    100.297099\n",
      "2025-08-01    100.297099\n",
      "2025-09-01    100.297099\n",
      "2025-10-01    100.297099\n",
      "2025-11-01    100.297099\n",
      "2025-12-01    100.297099\n",
      "2026-01-01    100.297099\n",
      "2026-02-01    100.297099\n",
      "2026-03-01    100.297099\n",
      "2026-04-01    100.297099\n",
      "2026-05-01    100.297099\n",
      "2026-06-01    100.297099\n",
      "2026-07-01    100.297099\n",
      "2026-08-01    100.297099\n",
      "2026-09-01    100.297099\n",
      "2026-10-01    100.297099\n",
      "2026-11-01    100.297099\n",
      "2026-12-01    100.297099\n",
      "Freq: MS, dtype: float64\n",
      "\n",
      "Metrics saved to plots/lightgbm_metrics.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load and prepare the time series data\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert time column to datetime\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    \n",
    "    # Set time as index\n",
    "    df.set_index('time', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"Create additional features for modeling\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Create lag features\n",
    "    for i in range(1, 13):\n",
    "        if 'cpi_mom' in df_copy.columns:\n",
    "            df_copy[f'cpi_mom_lag_{i}'] = df_copy['cpi_mom'].shift(i)\n",
    "        if 'cpi_yoy' in df_copy.columns:\n",
    "            df_copy[f'cpi_yoy_lag_{i}'] = df_copy['cpi_yoy'].shift(i)\n",
    "    \n",
    "    # Create rolling window features\n",
    "    for window in [3, 6, 12]:\n",
    "        if 'cpi_mom' in df_copy.columns:\n",
    "            df_copy[f'cpi_mom_rolling_mean_{window}'] = df_copy['cpi_mom'].rolling(window=window).mean()\n",
    "            df_copy[f'cpi_mom_rolling_std_{window}'] = df_copy['cpi_mom'].rolling(window=window).std()\n",
    "        if 'cpi_yoy' in df_copy.columns:\n",
    "            df_copy[f'cpi_yoy_rolling_mean_{window}'] = df_copy['cpi_yoy'].rolling(window=window).mean()\n",
    "            df_copy[f'cpi_yoy_rolling_std_{window}'] = df_copy['cpi_yoy'].rolling(window=window).std()\n",
    "    \n",
    "    # Create economic indicator lag features\n",
    "    for i in range(1, 4):\n",
    "        if 'oil_price' in df_copy.columns:\n",
    "            df_copy[f'oil_price_lag_{i}'] = df_copy['oil_price'].shift(i)\n",
    "        if 'gold_price' in df_copy.columns:\n",
    "            df_copy[f'gold_price_lag_{i}'] = df_copy['gold_price'].shift(i)\n",
    "        if 'interest_rate' in df_copy.columns:\n",
    "            df_copy[f'interest_rate_lag_{i}'] = df_copy['interest_rate'].shift(i)\n",
    "    \n",
    "    # Add month and year as cyclical features\n",
    "    if 'month' in df_copy.columns:\n",
    "        df_copy['month_sin'] = np.sin(2 * np.pi * df_copy['month']/12)\n",
    "        df_copy['month_cos'] = np.cos(2 * np.pi * df_copy['month']/12)\n",
    "    \n",
    "    # Create interaction features\n",
    "    if all(col in df_copy.columns for col in ['oil_price', 'gold_price']):\n",
    "        df_copy['oil_gold_ratio'] = df_copy['oil_price'] / df_copy['gold_price']\n",
    "    \n",
    "    # Create momentum features\n",
    "    if 'cpi_yoy' in df_copy.columns:\n",
    "        df_copy['cpi_yoy_momentum'] = df_copy['cpi_yoy'].diff()\n",
    "    if 'cpi_mom' in df_copy.columns:\n",
    "        df_copy['cpi_mom_momentum'] = df_copy['cpi_mom'].diff()\n",
    "    \n",
    "    # Drop rows with NaN values (due to lag features)\n",
    "    df_clean = df_copy.dropna()\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def calculate_metrics(actual, predicted):\n",
    "    \"\"\"Calculate evaluation metrics\"\"\"\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    r2 = r2_score(actual, predicted)\n",
    "    mape = np.mean(np.abs((actual - predicted) / actual)) * 100\n",
    "    \n",
    "    return rmse, mae, mape, r2\n",
    "\n",
    "def run_lightgbm(df, target_col, test_size=0.2, tune_hyperparams=True):\n",
    "    \"\"\"\n",
    "    Train and evaluate a LightGBM model for time series forecasting\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with engineered features\n",
    "    target_col : str\n",
    "        Name of the target column\n",
    "    test_size : float\n",
    "        Proportion of data to use for testing\n",
    "    tune_hyperparams : bool\n",
    "        Whether to perform hyperparameter tuning\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        Model, predictions, feature importances, metrics, and SHAP values\n",
    "    \"\"\"\n",
    "    print(f\"Running LightGBM model for {target_col}...\")\n",
    "    \n",
    "    # Define features and target\n",
    "    X = df.drop([col for col in ['cpi_mom', 'cpi_yoy', target_col] if col in df.columns], axis=1)\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Split data into training and testing sets (time-based split)\n",
    "    split_idx = int(len(df) * (1 - test_size))\n",
    "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "    \n",
    "    print(f\"Training data: {X_train.shape}, Test data: {X_test.shape}\")\n",
    "    \n",
    "    if tune_hyperparams:\n",
    "        print(\"Performing hyperparameter tuning...\")\n",
    "        # Define parameter grid\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 5, 7, 9],\n",
    "            'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "            'num_leaves': [31, 63, 127],\n",
    "            'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "            'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "            'reg_alpha': [0, 0.1, 0.5],\n",
    "            'reg_lambda': [0, 0.1, 0.5]\n",
    "        }\n",
    "        \n",
    "        # Use TimeSeriesSplit for cross-validation\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        \n",
    "        # Create and fit GridSearchCV\n",
    "        # lgbm_grid = GridSearchCV(\n",
    "        #     estimator=LGBMRegressor(objective='regression', random_state=42),\n",
    "        #     param_grid=param_grid,\n",
    "        #     cv=tscv,\n",
    "        #     scoring='neg_mean_squared_error',\n",
    "        #     n_jobs=-1,\n",
    "        #     verbose=1\n",
    "        # )\n",
    "\n",
    "        # Replace GridSearchCV with RandomizedSearchCV\n",
    "        lgbm_search = RandomizedSearchCV(\n",
    "            estimator=LGBMRegressor(objective='regression', random_state=42),\n",
    "            param_distributions=param_grid,\n",
    "            n_iter=30,             # Giảm số tổ hợp kiểm tra\n",
    "            cv=tscv,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            n_jobs=-1,\n",
    "            verbose=1,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        lgbm_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Get best parameters and model\n",
    "        best_params = lgbm_search.best_params_\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "        \n",
    "        # Train model with best parameters\n",
    "        model = LGBMRegressor(objective='regression', random_state=42, **best_params)\n",
    "    else:\n",
    "        # Use default parameters\n",
    "        model = LGBMRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=7,\n",
    "            learning_rate=0.1,\n",
    "            num_leaves=31,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=0.1,\n",
    "            objective='regression',\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_rmse, train_mae, train_mape, train_r2 = calculate_metrics(y_train, y_pred_train)\n",
    "    test_rmse, test_mae, test_mape, test_r2 = calculate_metrics(y_test, y_pred_test)\n",
    "    \n",
    "    print(f\"Training - RMSE: {train_rmse:.4f}, MAE: {train_mae:.4f}, MAPE: {train_mape:.2f}%, R²: {train_r2:.4f}\")\n",
    "    print(f\"Testing - RMSE: {test_rmse:.4f}, MAE: {test_mae:.4f}, MAPE: {test_mape:.2f}%, R²: {test_r2:.4f}\")\n",
    "    \n",
    "    # Get feature importances\n",
    "    feature_importances = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Importance': model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Plot feature importances\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importances.head(20))\n",
    "    plt.title(f'Top 20 Feature Importances for {target_col}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/lgbm_feature_importance_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot actual vs predicted\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df.index[-len(y_test):], y_test, label='Actual', color='blue')\n",
    "    plt.plot(df.index[-len(y_test):], y_pred_test, label='Predicted', color='red', linestyle='--')\n",
    "    plt.title(f'LightGBM: Actual vs Predicted {target_col}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(target_col)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/lgbm_predictions_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create a DataFrame with predictions\n",
    "    predictions = pd.DataFrame({\n",
    "        'Actual': y_test,\n",
    "        'Predicted': y_pred_test,\n",
    "        'Error': y_test - y_pred_test\n",
    "    })\n",
    "    \n",
    "    # Plot error distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(predictions['Error'], kde=True)\n",
    "    plt.title(f'Error Distribution for {target_col}')\n",
    "    plt.xlabel('Error')\n",
    "    plt.savefig(f'plots/lgbm_error_distribution_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Calculate SHAP values\n",
    "    try:\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "        \n",
    "        # Plot SHAP summary\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        shap.summary_plot(shap_values, X_test, show=False)\n",
    "        plt.title(f'SHAP Feature Importance for {target_col}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/lgbm_shap_summary_{target_col}.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Plot SHAP dependence plots for top features\n",
    "        top_features = feature_importances['Feature'].head(3).tolist()\n",
    "        for feature in top_features:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            shap.dependence_plot(feature, shap_values, X_test, show=False)\n",
    "            plt.title(f'SHAP Dependence Plot for {feature}')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'plots/lgbm_shap_dependence_{feature}_{target_col}.png')\n",
    "            plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating SHAP values: {e}\")\n",
    "        shap_values = None\n",
    "    \n",
    "    # Return results\n",
    "    metrics = {\n",
    "        'train_rmse': train_rmse,\n",
    "        'train_mae': train_mae,\n",
    "        'train_mape': train_mape,\n",
    "        'train_r2': train_r2,\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_mae': test_mae,\n",
    "        'test_mape': test_mape,\n",
    "        'test_r2': test_r2\n",
    "    }\n",
    "    \n",
    "    return model, predictions, feature_importances, metrics, shap_values\n",
    "\n",
    "def recursive_forecast(model, df, target_col, forecast_horizon=12):\n",
    "    \"\"\"\n",
    "    Generate recursive forecasts using the trained model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : LGBMRegressor\n",
    "        Trained model\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with features\n",
    "    target_col : str\n",
    "        Name of the target column\n",
    "    forecast_horizon : int\n",
    "        Number of periods to forecast\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        Forecasted values\n",
    "    \"\"\"\n",
    "    print(f\"Generating {forecast_horizon} period forecast...\")\n",
    "    \n",
    "    # Create a copy of the dataframe\n",
    "    df_forecast = df.copy()\n",
    "    \n",
    "    # Get the last date in the dataframe\n",
    "    last_date = df_forecast.index[-1]\n",
    "    \n",
    "    # Create a list to store forecasts\n",
    "    forecasts = []\n",
    "    \n",
    "    # Generate forecasts recursively\n",
    "    for i in range(forecast_horizon):\n",
    "        # Get the features for the next period\n",
    "        if i == 0:\n",
    "            # For the first forecast, use the last row of the dataframe\n",
    "            X_next = df_forecast.drop([col for col in ['cpi_mom', 'cpi_yoy', target_col] if col in df_forecast.columns], axis=1).iloc[-1:].values\n",
    "        else:\n",
    "            # Update the dataframe with the previous forecast\n",
    "            # This requires updating all the lag features, rolling means, etc.\n",
    "            # For simplicity, we'll just use the last forecast as a naive prediction\n",
    "            X_next = df_forecast.drop([col for col in ['cpi_mom', 'cpi_yoy', target_col] if col in df_forecast.columns], axis=1).iloc[-1:].values\n",
    "        \n",
    "        # Make prediction\n",
    "        forecast = model.predict(X_next)[0]\n",
    "        \n",
    "        # Store forecast\n",
    "        forecasts.append(forecast)\n",
    "        \n",
    "        # Create next date\n",
    "        next_date = last_date + pd.DateOffset(months=i+1)\n",
    "        \n",
    "        # For a more accurate implementation, we would need to update all features\n",
    "        # based on the new forecast, but this is a simplified version\n",
    "    \n",
    "    # Create a Series with the forecasts\n",
    "    future_dates = pd.date_range(start=last_date + pd.DateOffset(months=1), periods=forecast_horizon, freq='MS')\n",
    "    forecast_series = pd.Series(forecasts, index=future_dates)\n",
    "    \n",
    "    # Plot historical data with forecasts\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df[target_col].index, df[target_col], label='Historical Data')\n",
    "    plt.plot(forecast_series.index, forecast_series, label='Forecast', color='red', linestyle='--')\n",
    "    plt.title(f'LightGBM: {target_col} Forecast')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(target_col)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/lgbm_future_forecast_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return forecast_series\n",
    "\n",
    "def main():\n",
    "    # Create plots directory if it doesn't exist\n",
    "    import os\n",
    "    if not os.path.exists('plots'):\n",
    "        os.makedirs('plots')\n",
    "    \n",
    "    # Load data\n",
    "    file_path = 'data/analyzed_time_series.csv'\n",
    "    df = load_data(file_path)\n",
    "    \n",
    "    # Engineer features\n",
    "    df_engineered = engineer_features(df)\n",
    "    \n",
    "    # Run LightGBM for CPI Year-over-Year\n",
    "    target_col = 'cpi_yoy'\n",
    "    model_yoy, predictions_yoy, feature_importances_yoy, metrics_yoy, shap_values_yoy = run_lightgbm(\n",
    "        df_engineered, target_col, test_size=0.2, tune_hyperparams=True\n",
    "    )\n",
    "    \n",
    "    # Generate future forecasts for YoY\n",
    "    forecast_yoy = recursive_forecast(model_yoy, df_engineered, target_col, forecast_horizon=24)\n",
    "    print(f\"\\nFuture {target_col} forecasts:\")\n",
    "    print(forecast_yoy)\n",
    "    \n",
    "    # Run LightGBM for CPI Month-over-Month\n",
    "    target_col = 'cpi_mom'\n",
    "    model_mom, predictions_mom, feature_importances_mom, metrics_mom, shap_values_mom = run_lightgbm(\n",
    "        df_engineered, target_col, test_size=0.2, tune_hyperparams=True\n",
    "    )\n",
    "    \n",
    "    # Generate future forecasts for MoM\n",
    "    forecast_mom = recursive_forecast(model_mom, df_engineered, target_col, forecast_horizon=24)\n",
    "    print(f\"\\nFuture {target_col} forecasts:\")\n",
    "    print(forecast_mom)\n",
    "    \n",
    "    # Save metrics to CSV\n",
    "    metrics_data = [\n",
    "        {\n",
    "            'Model': 'LightGBM', 'Target': 'CPI MoM',\n",
    "            'Train_RMSE': metrics_mom['train_rmse'], 'Train_MAE': metrics_mom['train_mae'], \n",
    "            'Train_MAPE': metrics_mom['train_mape'], 'Train_R2': metrics_mom['train_r2'],\n",
    "            'Test_RMSE': metrics_mom['test_rmse'], 'Test_MAE': metrics_mom['test_mae'], \n",
    "            'Test_MAPE': metrics_mom['test_mape'], 'Test_R2': metrics_mom['test_r2']\n",
    "        },\n",
    "        {\n",
    "            'Model': 'LightGBM', 'Target': 'CPI YoY',\n",
    "            'Train_RMSE': metrics_yoy['train_rmse'], 'Train_MAE': metrics_yoy['train_mae'], \n",
    "            'Train_MAPE': metrics_yoy['train_mape'], 'Train_R2': metrics_yoy['train_r2'],\n",
    "            'Test_RMSE': metrics_yoy['test_rmse'], 'Test_MAE': metrics_yoy['test_mae'], \n",
    "            'Test_MAPE': metrics_yoy['test_mape'], 'Test_R2': metrics_yoy['test_r2']\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "    metrics_df.to_csv('plots/lightgbm_metrics.csv', index=False)\n",
    "    print(\"\\nMetrics saved to plots/lightgbm_metrics.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02d949f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Running SVR with linear kernel for cpi_yoy\n",
      "--------------------------------------------------\n",
      "Running SVR model with linear kernel for cpi_yoy...\n",
      "Training data: (269, 63), Test data: (68, 63)\n",
      "Performing hyperparameter tuning...\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Best parameters: {'svr__C': 10, 'svr__epsilon': 0.1}\n",
      "Training - RMSE: 0.0659, MAE: 0.0573, MAPE: 0.05%, R²: 0.9999\n",
      "Testing - RMSE: 0.1576, MAE: 0.1353, MAPE: 0.13%, R²: 0.9833\n",
      "\n",
      "--------------------------------------------------\n",
      "Running SVR with rbf kernel for cpi_yoy\n",
      "--------------------------------------------------\n",
      "Running SVR model with rbf kernel for cpi_yoy...\n",
      "Training data: (269, 63), Test data: (68, 63)\n",
      "Performing hyperparameter tuning...\n",
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n",
      "Best parameters: {'svr__C': 100, 'svr__epsilon': 0.01, 'svr__gamma': 0.01}\n",
      "Training - RMSE: 0.0098, MAE: 0.0097, MAPE: 0.01%, R²: 1.0000\n",
      "Testing - RMSE: 1.3641, MAE: 1.1568, MAPE: 1.13%, R²: -0.2510\n",
      "\n",
      "--------------------------------------------------\n",
      "Running SVR with poly kernel for cpi_yoy\n",
      "--------------------------------------------------\n",
      "Running SVR model with poly kernel for cpi_yoy...\n",
      "Training data: (269, 63), Test data: (68, 63)\n",
      "Performing hyperparameter tuning...\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n",
      "Best parameters: {'svr__C': 0.1, 'svr__degree': 3, 'svr__epsilon': 0.5, 'svr__gamma': 0.01}\n",
      "Training - RMSE: 3.2707, MAE: 2.6232, MAPE: 2.44%, R²: 0.6917\n",
      "Testing - RMSE: 2.1222, MAE: 1.8507, MAPE: 1.81%, R²: -2.0279\n",
      "\n",
      "Best kernel for cpi_yoy: linear\n",
      "Generating 24 period forecast...\n",
      "\n",
      "Future cpi_yoy forecasts:\n",
      "2025-01-01    102.962345\n",
      "2025-02-01    102.962345\n",
      "2025-03-01    102.962345\n",
      "2025-04-01    102.962345\n",
      "2025-05-01    102.962345\n",
      "2025-06-01    102.962345\n",
      "2025-07-01    102.962345\n",
      "2025-08-01    102.962345\n",
      "2025-09-01    102.962345\n",
      "2025-10-01    102.962345\n",
      "2025-11-01    102.962345\n",
      "2025-12-01    102.962345\n",
      "2026-01-01    102.962345\n",
      "2026-02-01    102.962345\n",
      "2026-03-01    102.962345\n",
      "2026-04-01    102.962345\n",
      "2026-05-01    102.962345\n",
      "2026-06-01    102.962345\n",
      "2026-07-01    102.962345\n",
      "2026-08-01    102.962345\n",
      "2026-09-01    102.962345\n",
      "2026-10-01    102.962345\n",
      "2026-11-01    102.962345\n",
      "2026-12-01    102.962345\n",
      "Freq: MS, dtype: float64\n",
      "\n",
      "--------------------------------------------------\n",
      "Running SVR with linear kernel for cpi_mom\n",
      "--------------------------------------------------\n",
      "Running SVR model with linear kernel for cpi_mom...\n",
      "Training data: (269, 63), Test data: (68, 63)\n",
      "Performing hyperparameter tuning...\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Best parameters: {'svr__C': 1, 'svr__epsilon': 0.01}\n",
      "Training - RMSE: 0.0065, MAE: 0.0056, MAPE: 0.01%, R²: 0.9999\n",
      "Testing - RMSE: 0.0126, MAE: 0.0089, MAPE: 0.01%, R²: 0.9993\n",
      "\n",
      "--------------------------------------------------\n",
      "Running SVR with rbf kernel for cpi_mom\n",
      "--------------------------------------------------\n",
      "Running SVR model with rbf kernel for cpi_mom...\n",
      "Training data: (269, 63), Test data: (68, 63)\n",
      "Performing hyperparameter tuning...\n",
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n",
      "Best parameters: {'svr__C': 10, 'svr__epsilon': 0.01, 'svr__gamma': 0.01}\n",
      "Training - RMSE: 0.0092, MAE: 0.0088, MAPE: 0.01%, R²: 0.9999\n",
      "Testing - RMSE: 0.1491, MAE: 0.1066, MAPE: 0.11%, R²: 0.9014\n",
      "\n",
      "--------------------------------------------------\n",
      "Running SVR with poly kernel for cpi_mom\n",
      "--------------------------------------------------\n",
      "Running SVR model with poly kernel for cpi_mom...\n",
      "Training data: (269, 63), Test data: (68, 63)\n",
      "Performing hyperparameter tuning...\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n",
      "Best parameters: {'svr__C': 0.1, 'svr__degree': 2, 'svr__epsilon': 0.2, 'svr__gamma': 0.01}\n",
      "Training - RMSE: 0.6851, MAE: 0.4962, MAPE: 0.49%, R²: 0.2751\n",
      "Testing - RMSE: 0.5024, MAE: 0.3565, MAPE: 0.36%, R²: -0.1195\n",
      "\n",
      "Best kernel for cpi_mom: linear\n",
      "Generating 24 period forecast...\n",
      "\n",
      "Future cpi_mom forecasts:\n",
      "2025-01-01    100.294875\n",
      "2025-02-01    100.294875\n",
      "2025-03-01    100.294875\n",
      "2025-04-01    100.294875\n",
      "2025-05-01    100.294875\n",
      "2025-06-01    100.294875\n",
      "2025-07-01    100.294875\n",
      "2025-08-01    100.294875\n",
      "2025-09-01    100.294875\n",
      "2025-10-01    100.294875\n",
      "2025-11-01    100.294875\n",
      "2025-12-01    100.294875\n",
      "2026-01-01    100.294875\n",
      "2026-02-01    100.294875\n",
      "2026-03-01    100.294875\n",
      "2026-04-01    100.294875\n",
      "2026-05-01    100.294875\n",
      "2026-06-01    100.294875\n",
      "2026-07-01    100.294875\n",
      "2026-08-01    100.294875\n",
      "2026-09-01    100.294875\n",
      "2026-10-01    100.294875\n",
      "2026-11-01    100.294875\n",
      "2026-12-01    100.294875\n",
      "Freq: MS, dtype: float64\n",
      "\n",
      "\n",
      "Model Comparison - CPI Year-over-Year:\n",
      "--------------------------------------------------------------------------------\n",
      "Kernel     Train RMSE   Test RMSE    Train R²     Test R²     \n",
      "--------------------------------------------------------------------------------\n",
      "linear     0.0659       0.1576       0.9999       0.9833      \n",
      "rbf        0.0098       1.3641       1.0000       -0.2510     \n",
      "poly       3.2707       2.1222       0.6917       -2.0279     \n",
      "\n",
      "\n",
      "Model Comparison - CPI Month-over-Month:\n",
      "--------------------------------------------------------------------------------\n",
      "Kernel     Train RMSE   Test RMSE    Train R²     Test R²     \n",
      "--------------------------------------------------------------------------------\n",
      "linear     0.0065       0.0126       0.9999       0.9993      \n",
      "rbf        0.0092       0.1491       0.9999       0.9014      \n",
      "poly       0.6851       0.5024       0.2751       -0.1195     \n",
      "\n",
      "Metrics saved to plots/svr_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load and prepare the time series data\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert time column to datetime\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    \n",
    "    # Set time as index\n",
    "    df.set_index('time', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"Create additional features for modeling\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Create lag features\n",
    "    for i in range(1, 13):\n",
    "        if 'cpi_mom' in df_copy.columns:\n",
    "            df_copy[f'cpi_mom_lag_{i}'] = df_copy['cpi_mom'].shift(i)\n",
    "        if 'cpi_yoy' in df_copy.columns:\n",
    "            df_copy[f'cpi_yoy_lag_{i}'] = df_copy['cpi_yoy'].shift(i)\n",
    "    \n",
    "    # Create rolling window features\n",
    "    for window in [3, 6, 12]:\n",
    "        if 'cpi_mom' in df_copy.columns:\n",
    "            df_copy[f'cpi_mom_rolling_mean_{window}'] = df_copy['cpi_mom'].rolling(window=window).mean()\n",
    "            df_copy[f'cpi_mom_rolling_std_{window}'] = df_copy['cpi_mom'].rolling(window=window).std()\n",
    "        if 'cpi_yoy' in df_copy.columns:\n",
    "            df_copy[f'cpi_yoy_rolling_mean_{window}'] = df_copy['cpi_yoy'].rolling(window=window).mean()\n",
    "            df_copy[f'cpi_yoy_rolling_std_{window}'] = df_copy['cpi_yoy'].rolling(window=window).std()\n",
    "    \n",
    "    # Create economic indicator lag features\n",
    "    for i in range(1, 4):\n",
    "        if 'oil_price' in df_copy.columns:\n",
    "            df_copy[f'oil_price_lag_{i}'] = df_copy['oil_price'].shift(i)\n",
    "        if 'gold_price' in df_copy.columns:\n",
    "            df_copy[f'gold_price_lag_{i}'] = df_copy['gold_price'].shift(i)\n",
    "        if 'interest_rate' in df_copy.columns:\n",
    "            df_copy[f'interest_rate_lag_{i}'] = df_copy['interest_rate'].shift(i)\n",
    "    \n",
    "    # Add month and year as cyclical features\n",
    "    if 'month' in df_copy.columns:\n",
    "        df_copy['month_sin'] = np.sin(2 * np.pi * df_copy['month']/12)\n",
    "        df_copy['month_cos'] = np.cos(2 * np.pi * df_copy['month']/12)\n",
    "    \n",
    "    # Create interaction features\n",
    "    if all(col in df_copy.columns for col in ['oil_price', 'gold_price']):\n",
    "        df_copy['oil_gold_ratio'] = df_copy['oil_price'] / df_copy['gold_price']\n",
    "    \n",
    "    # Drop rows with NaN values (due to lag features)\n",
    "    df_clean = df_copy.dropna()\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def calculate_metrics(actual, predicted):\n",
    "    \"\"\"Calculate evaluation metrics\"\"\"\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    r2 = r2_score(actual, predicted)\n",
    "    mape = np.mean(np.abs((actual - predicted) / actual)) * 100\n",
    "    \n",
    "    return rmse, mae, mape, r2\n",
    "\n",
    "def run_svr(df, target_col, test_size=0.2, kernel='rbf', tune_hyperparams=True):\n",
    "    \"\"\"\n",
    "    Train and evaluate a Support Vector Regression model for time series forecasting\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with engineered features\n",
    "    target_col : str\n",
    "        Name of the target column\n",
    "    test_size : float\n",
    "        Proportion of data to use for testing\n",
    "    kernel : str\n",
    "        Kernel type to be used in the algorithm ('linear', 'poly', 'rbf', 'sigmoid')\n",
    "    tune_hyperparams : bool\n",
    "        Whether to perform hyperparameter tuning\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        Model, predictions, metrics\n",
    "    \"\"\"\n",
    "    print(f\"Running SVR model with {kernel} kernel for {target_col}...\")\n",
    "    \n",
    "    # Define features and target\n",
    "    X = df.drop([col for col in ['cpi_mom', 'cpi_yoy', target_col] if col in df.columns], axis=1)\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Split data into training and testing sets (time-based split)\n",
    "    split_idx = int(len(df) * (1 - test_size))\n",
    "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "    \n",
    "    print(f\"Training data: {X_train.shape}, Test data: {X_test.shape}\")\n",
    "    \n",
    "    # Create pipeline with scaling (important for SVR)\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('svr', SVR(kernel=kernel))\n",
    "    ])\n",
    "    \n",
    "    if tune_hyperparams:\n",
    "        print(\"Performing hyperparameter tuning...\")\n",
    "        # Define parameter grid based on kernel\n",
    "        if kernel == 'linear':\n",
    "            param_grid = {\n",
    "                'svr__C': [0.1, 1, 10, 100],\n",
    "                'svr__epsilon': [0.01, 0.1, 0.2, 0.5]\n",
    "            }\n",
    "        elif kernel == 'poly':\n",
    "            param_grid = {\n",
    "                'svr__C': [0.1, 1, 10, 100],\n",
    "                'svr__epsilon': [0.01, 0.1, 0.2, 0.5],\n",
    "                'svr__degree': [2, 3, 4],\n",
    "                'svr__gamma': ['scale', 'auto', 0.1, 0.01]\n",
    "            }\n",
    "        else:  # rbf or sigmoid\n",
    "            param_grid = {\n",
    "                'svr__C': [0.1, 1, 10, 100],\n",
    "                'svr__epsilon': [0.01, 0.1, 0.2, 0.5],\n",
    "                'svr__gamma': ['scale', 'auto', 0.1, 0.01]\n",
    "            }\n",
    "        \n",
    "        # Use TimeSeriesSplit for cross-validation\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        \n",
    "        # Create and fit GridSearchCV\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=pipeline,\n",
    "            param_grid=param_grid,\n",
    "            cv=tscv,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Get best parameters and model\n",
    "        best_params = grid_search.best_params_\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "        \n",
    "        # Use the best model\n",
    "        pipeline = grid_search.best_estimator_\n",
    "    else:\n",
    "        # Use default parameters\n",
    "        pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = pipeline.predict(X_train)\n",
    "    y_pred_test = pipeline.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_rmse, train_mae, train_mape, train_r2 = calculate_metrics(y_train, y_pred_train)\n",
    "    test_rmse, test_mae, test_mape, test_r2 = calculate_metrics(y_test, y_pred_test)\n",
    "    \n",
    "    print(f\"Training - RMSE: {train_rmse:.4f}, MAE: {train_mae:.4f}, MAPE: {train_mape:.2f}%, R²: {train_r2:.4f}\")\n",
    "    print(f\"Testing - RMSE: {test_rmse:.4f}, MAE: {test_mae:.4f}, MAPE: {test_mape:.2f}%, R²: {test_r2:.4f}\")\n",
    "    \n",
    "    # Plot actual vs predicted\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df.index[-len(y_test):], y_test, label='Actual', color='blue')\n",
    "    plt.plot(df.index[-len(y_test):], y_pred_test, label='Predicted', color='red', linestyle='--')\n",
    "    plt.title(f'SVR ({kernel}): Actual vs Predicted {target_col}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(target_col)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/svr_{kernel}_predictions_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create a DataFrame with predictions\n",
    "    predictions = pd.DataFrame({\n",
    "        'Actual': y_test,\n",
    "        'Predicted': y_pred_test,\n",
    "        'Error': y_test - y_pred_test\n",
    "    })\n",
    "    \n",
    "    # Plot error distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(predictions['Error'], kde=True)\n",
    "    plt.title(f'Error Distribution for SVR ({kernel}) - {target_col}')\n",
    "    plt.xlabel('Error')\n",
    "    plt.savefig(f'plots/svr_{kernel}_error_distribution_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot residuals vs fitted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_pred_test, predictions['Error'])\n",
    "    plt.axhline(y=0, color='r', linestyle='-')\n",
    "    plt.title(f'Residuals vs Fitted Values for SVR ({kernel}) - {target_col}')\n",
    "    plt.xlabel('Fitted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.savefig(f'plots/svr_{kernel}_residuals_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Return results\n",
    "    metrics = {\n",
    "        'train_rmse': train_rmse,\n",
    "        'train_mae': train_mae,\n",
    "        'train_mape': train_mape,\n",
    "        'train_r2': train_r2,\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_mae': test_mae,\n",
    "        'test_mape': test_mape,\n",
    "        'test_r2': test_r2\n",
    "    }\n",
    "    \n",
    "    return pipeline, predictions, metrics\n",
    "\n",
    "def recursive_forecast(model, df, target_col, forecast_horizon=12):\n",
    "    \"\"\"\n",
    "    Generate recursive forecasts using the trained model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : Pipeline\n",
    "        Trained model pipeline\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with features\n",
    "    target_col : str\n",
    "        Name of the target column\n",
    "    forecast_horizon : int\n",
    "        Number of periods to forecast\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        Forecasted values\n",
    "    \"\"\"\n",
    "    print(f\"Generating {forecast_horizon} period forecast...\")\n",
    "    \n",
    "    # Create a copy of the dataframe\n",
    "    df_forecast = df.copy()\n",
    "    \n",
    "    # Get the last date in the dataframe\n",
    "    last_date = df_forecast.index[-1]\n",
    "    \n",
    "    # Create a list to store forecasts\n",
    "    forecasts = []\n",
    "    \n",
    "    # Generate forecasts recursively\n",
    "    for i in range(forecast_horizon):\n",
    "        # Get the features for the next period\n",
    "        if i == 0:\n",
    "            # For the first forecast, use the last row of the dataframe\n",
    "            X_next = df_forecast.drop([col for col in ['cpi_mom', 'cpi_yoy', target_col] if col in df_forecast.columns], axis=1).iloc[-1:].values\n",
    "        else:\n",
    "            # Update the dataframe with the previous forecast\n",
    "            # This requires updating all the lag features, rolling means, etc.\n",
    "            # For simplicity, we'll just use the last forecast as a naive prediction\n",
    "            X_next = df_forecast.drop([col for col in ['cpi_mom', 'cpi_yoy', target_col] if col in df_forecast.columns], axis=1).iloc[-1:].values\n",
    "        \n",
    "        # Make prediction\n",
    "        forecast = model.predict(X_next)[0]\n",
    "        \n",
    "        # Store forecast\n",
    "        forecasts.append(forecast)\n",
    "        \n",
    "        # Create next date\n",
    "        next_date = last_date + pd.DateOffset(months=i+1)\n",
    "        \n",
    "        # For a more accurate implementation, we would need to update all features\n",
    "        # based on the new forecast, but this is a simplified version\n",
    "    \n",
    "    # Create a Series with the forecasts\n",
    "    future_dates = pd.date_range(start=last_date + pd.DateOffset(months=1), periods=forecast_horizon, freq='MS')\n",
    "    forecast_series = pd.Series(forecasts, index=future_dates)\n",
    "    \n",
    "    # Plot historical data with forecasts\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df[target_col].index, df[target_col], label='Historical Data')\n",
    "    plt.plot(forecast_series.index, forecast_series, label='Forecast', color='red', linestyle='--')\n",
    "    plt.title(f'SVR: {target_col} Forecast')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(target_col)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/svr_future_forecast_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return forecast_series\n",
    "\n",
    "def main():\n",
    "    # Create plots directory if it doesn't exist\n",
    "    import os\n",
    "    if not os.path.exists('plots'):\n",
    "        os.makedirs('plots')\n",
    "    \n",
    "    # Load data\n",
    "    file_path = 'data/analyzed_time_series.csv'\n",
    "    df = load_data(file_path)\n",
    "    \n",
    "    # Engineer features\n",
    "    df_engineered = engineer_features(df)\n",
    "    \n",
    "    # List of kernels to try\n",
    "    kernels = ['linear', 'rbf', 'poly']\n",
    "    \n",
    "    # Dictionary to store results\n",
    "    results = {}\n",
    "    \n",
    "    # Run SVR with different kernels for CPI Year-over-Year\n",
    "    target_col = 'cpi_yoy'\n",
    "    for kernel in kernels:\n",
    "        print(f\"\\n{'-'*50}\")\n",
    "        print(f\"Running SVR with {kernel} kernel for {target_col}\")\n",
    "        print(f\"{'-'*50}\")\n",
    "        \n",
    "        model, predictions, metrics = run_svr(\n",
    "            df_engineered, target_col, test_size=0.2, kernel=kernel, tune_hyperparams=True\n",
    "        )\n",
    "        \n",
    "        results[f\"{kernel}_yoy\"] = {\n",
    "            'model': model,\n",
    "            'predictions': predictions,\n",
    "            'metrics': metrics\n",
    "        }\n",
    "    \n",
    "    # Find the best kernel for YoY based on test RMSE\n",
    "    best_kernel_yoy = min(kernels, key=lambda k: results[f\"{k}_yoy\"]['metrics']['test_rmse'])\n",
    "    print(f\"\\nBest kernel for {target_col}: {best_kernel_yoy}\")\n",
    "    \n",
    "    # Generate future forecasts using the best model for YoY\n",
    "    forecast_yoy = recursive_forecast(results[f\"{best_kernel_yoy}_yoy\"]['model'], df_engineered, target_col, forecast_horizon=24)\n",
    "    print(f\"\\nFuture {target_col} forecasts:\")\n",
    "    print(forecast_yoy)\n",
    "    \n",
    "    # Run SVR with different kernels for CPI Month-over-Month\n",
    "    target_col = 'cpi_mom'\n",
    "    for kernel in kernels:\n",
    "        print(f\"\\n{'-'*50}\")\n",
    "        print(f\"Running SVR with {kernel} kernel for {target_col}\")\n",
    "        print(f\"{'-'*50}\")\n",
    "        \n",
    "        model, predictions, metrics = run_svr(\n",
    "            df_engineered, target_col, test_size=0.2, kernel=kernel, tune_hyperparams=True\n",
    "        )\n",
    "        \n",
    "        results[f\"{kernel}_mom\"] = {\n",
    "            'model': model,\n",
    "            'predictions': predictions,\n",
    "            'metrics': metrics\n",
    "        }\n",
    "    \n",
    "    # Find the best kernel for MoM based on test RMSE\n",
    "    best_kernel_mom = min(kernels, key=lambda k: results[f\"{k}_mom\"]['metrics']['test_rmse'])\n",
    "    print(f\"\\nBest kernel for {target_col}: {best_kernel_mom}\")\n",
    "    \n",
    "    # Generate future forecasts using the best model for MoM\n",
    "    forecast_mom = recursive_forecast(results[f\"{best_kernel_mom}_mom\"]['model'], df_engineered, target_col, forecast_horizon=24)\n",
    "    print(f\"\\nFuture {target_col} forecasts:\")\n",
    "    print(forecast_mom)\n",
    "    \n",
    "    # Compare models\n",
    "    print(\"\\n\\nModel Comparison - CPI Year-over-Year:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Kernel':<10} {'Train RMSE':<12} {'Test RMSE':<12} {'Train R²':<12} {'Test R²':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    for kernel in kernels:\n",
    "        metrics = results[f\"{kernel}_yoy\"]['metrics']\n",
    "        print(f\"{kernel:<10} {metrics['train_rmse']:<12.4f} {metrics['test_rmse']:<12.4f} {metrics['train_r2']:<12.4f} {metrics['test_r2']:<12.4f}\")\n",
    "    \n",
    "    print(\"\\n\\nModel Comparison - CPI Month-over-Month:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Kernel':<10} {'Train RMSE':<12} {'Test RMSE':<12} {'Train R²':<12} {'Test R²':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    for kernel in kernels:\n",
    "        metrics = results[f\"{kernel}_mom\"]['metrics']\n",
    "        print(f\"{kernel:<10} {metrics['train_rmse']:<12.4f} {metrics['test_rmse']:<12.4f} {metrics['train_r2']:<12.4f} {metrics['test_r2']:<12.4f}\")\n",
    "    \n",
    "    # Save metrics to CSV\n",
    "    metrics_data = []\n",
    "    for kernel in kernels:\n",
    "        metrics_yoy = results[f\"{kernel}_yoy\"]['metrics']\n",
    "        metrics_mom = results[f\"{kernel}_mom\"]['metrics']\n",
    "        \n",
    "        metrics_data.append({\n",
    "            'Model': f\"SVR ({kernel})\",\n",
    "            'Target': 'CPI YoY',\n",
    "            'Train_RMSE': metrics_yoy['train_rmse'],\n",
    "            'Train_MAE': metrics_yoy['train_mae'],\n",
    "            'Train_MAPE': metrics_yoy['train_mape'],\n",
    "            'Train_R2': metrics_yoy['train_r2'],\n",
    "            'Test_RMSE': metrics_yoy['test_rmse'],\n",
    "            'Test_MAE': metrics_yoy['test_mae'],\n",
    "            'Test_MAPE': metrics_yoy['test_mape'],\n",
    "            'Test_R2': metrics_yoy['test_r2']\n",
    "        })\n",
    "        \n",
    "        metrics_data.append({\n",
    "            'Model': f\"SVR ({kernel})\",\n",
    "            'Target': 'CPI MoM',\n",
    "            'Train_RMSE': metrics_mom['train_rmse'],\n",
    "            'Train_MAE': metrics_mom['train_mae'],\n",
    "            'Train_MAPE': metrics_mom['train_mape'],\n",
    "            'Train_R2': metrics_mom['train_r2'],\n",
    "            'Test_RMSE': metrics_mom['test_rmse'],\n",
    "            'Test_MAE': metrics_mom['test_mae'],\n",
    "            'Test_MAPE': metrics_mom['test_mape'],\n",
    "            'Test_R2': metrics_mom['test_r2']\n",
    "        })\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "    metrics_df.to_csv('plots/svr_metrics.csv', index=False)\n",
    "    print(\"\\nMetrics saved to plots/svr_metrics.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
