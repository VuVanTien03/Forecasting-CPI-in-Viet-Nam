{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6d86676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Stacking Ensemble for cpi_yoy...\n",
      "Training data: (242, 63), Validation data: (27, 63), Test data: (68, 63)\n",
      "Training Linear Regression...\n",
      "Linear Regression - Validation RMSE: 0.0000, MAE: 0.0000, MAPE: 0.00%, R²: 1.0000\n",
      "Training Ridge...\n",
      "Ridge - Validation RMSE: 0.0532, MAE: 0.0498, MAPE: 0.05%, R²: 0.9951\n",
      "Training Lasso...\n",
      "Lasso - Validation RMSE: 0.0522, MAE: 0.0407, MAPE: 0.04%, R²: 0.9953\n",
      "Training ElasticNet...\n",
      "ElasticNet - Validation RMSE: 0.1017, MAE: 0.0882, MAPE: 0.09%, R²: 0.9821\n",
      "Training Random Forest...\n",
      "Random Forest - Validation RMSE: 0.0539, MAE: 0.0461, MAPE: 0.04%, R²: 0.9950\n",
      "Training Gradient Boosting...\n",
      "Gradient Boosting - Validation RMSE: 0.0556, MAE: 0.0434, MAPE: 0.04%, R²: 0.9946\n",
      "Training XGBoost...\n",
      "XGBoost - Validation RMSE: 0.0809, MAE: 0.0613, MAPE: 0.06%, R²: 0.9886\n",
      "Training SVR...\n",
      "SVR - Validation RMSE: 0.4099, MAE: 0.3248, MAPE: 0.31%, R²: 0.7086\n",
      "\n",
      "Training Ridge meta-model...\n",
      "\n",
      "Meta-model coefficients:\n",
      "Linear Regression: 0.1994\n",
      "Ridge: 0.1892\n",
      "Lasso: 0.0875\n",
      "ElasticNet: 0.1271\n",
      "Random Forest: 0.1166\n",
      "Gradient Boosting: 0.1868\n",
      "XGBoost: 0.0580\n",
      "SVR: 0.0526\n",
      "\n",
      "Stacking Ensemble - Test RMSE: 397.1979, MAE: 397.1882, MAPE: 385.62%, R²: -106072.5720\n",
      "\n",
      "Base Model Test Performance:\n",
      "Linear Regression - Test RMSE: 615.3767, MAE: 615.3475, MAPE: 597.42%, R²: -254608.9766\n",
      "Ridge - Test RMSE: 757.1255, MAE: 757.1101, MAPE: 735.07%, R²: -385414.4061\n",
      "Lasso - Test RMSE: 476.7382, MAE: 476.7232, MAPE: 462.84%, R²: -152809.4763\n",
      "ElasticNet - Test RMSE: 701.8940, MAE: 701.8839, MAPE: 681.47%, R²: -331234.1195\n",
      "Random Forest - Test RMSE: 0.0624, MAE: 0.0510, MAPE: 0.05%, R²: 0.9974\n",
      "Gradient Boosting - Test RMSE: 0.0862, MAE: 0.0674, MAPE: 0.07%, R²: 0.9950\n",
      "XGBoost - Test RMSE: 0.0979, MAE: 0.0748, MAPE: 0.07%, R²: 0.9936\n",
      "SVR - Test RMSE: 5.7427, MAE: 5.6117, MAPE: 5.46%, R²: -21.1729\n",
      "Generating 24 period forecast...\n",
      "\n",
      "Future cpi_yoy forecasts:\n",
      "2025-01-01    499.055297\n",
      "2025-02-01    499.055297\n",
      "2025-03-01    499.055297\n",
      "2025-04-01    499.055297\n",
      "2025-05-01    499.055297\n",
      "2025-06-01    499.055297\n",
      "2025-07-01    499.055297\n",
      "2025-08-01    499.055297\n",
      "2025-09-01    499.055297\n",
      "2025-10-01    499.055297\n",
      "2025-11-01    499.055297\n",
      "2025-12-01    499.055297\n",
      "2026-01-01    499.055297\n",
      "2026-02-01    499.055297\n",
      "2026-03-01    499.055297\n",
      "2026-04-01    499.055297\n",
      "2026-05-01    499.055297\n",
      "2026-06-01    499.055297\n",
      "2026-07-01    499.055297\n",
      "2026-08-01    499.055297\n",
      "2026-09-01    499.055297\n",
      "2026-10-01    499.055297\n",
      "2026-11-01    499.055297\n",
      "2026-12-01    499.055297\n",
      "Freq: MS, dtype: float64\n",
      "Running Stacking Ensemble for cpi_mom...\n",
      "Training data: (242, 63), Validation data: (27, 63), Test data: (68, 63)\n",
      "Training Linear Regression...\n",
      "Linear Regression - Validation RMSE: 0.0000, MAE: 0.0000, MAPE: 0.00%, R²: 1.0000\n",
      "Training Ridge...\n",
      "Ridge - Validation RMSE: 0.0034, MAE: 0.0030, MAPE: 0.00%, R²: 0.9999\n",
      "Training Lasso...\n",
      "Lasso - Validation RMSE: 0.0048, MAE: 0.0045, MAPE: 0.00%, R²: 0.9998\n",
      "Training ElasticNet...\n",
      "ElasticNet - Validation RMSE: 0.0056, MAE: 0.0050, MAPE: 0.00%, R²: 0.9998\n",
      "Training Random Forest...\n",
      "Random Forest - Validation RMSE: 0.0154, MAE: 0.0113, MAPE: 0.01%, R²: 0.9982\n",
      "Training Gradient Boosting...\n",
      "Gradient Boosting - Validation RMSE: 0.0107, MAE: 0.0080, MAPE: 0.01%, R²: 0.9991\n",
      "Training XGBoost...\n",
      "XGBoost - Validation RMSE: 0.0168, MAE: 0.0088, MAPE: 0.01%, R²: 0.9979\n",
      "Training SVR...\n",
      "SVR - Validation RMSE: 0.1198, MAE: 0.0993, MAPE: 0.10%, R²: 0.8934\n",
      "\n",
      "Training Ridge meta-model...\n",
      "\n",
      "Meta-model coefficients:\n",
      "Linear Regression: 0.1366\n",
      "Ridge: 0.1362\n",
      "Lasso: 0.1361\n",
      "ElasticNet: 0.1349\n",
      "Random Forest: 0.1332\n",
      "Gradient Boosting: 0.1366\n",
      "XGBoost: 0.1344\n",
      "SVR: 0.0720\n",
      "\n",
      "Stacking Ensemble - Test RMSE: 12.4997, MAE: 12.4976, MAPE: 12.47%, R²: -692.0968\n",
      "\n",
      "Base Model Test Performance:\n",
      "Linear Regression - Test RMSE: 51.4125, MAE: 51.4123, MAPE: 51.29%, R²: -11724.4365\n",
      "Ridge - Test RMSE: 29.0724, MAE: 29.0692, MAPE: 29.00%, R²: -3748.3443\n",
      "Lasso - Test RMSE: 4.1073, MAE: 4.0802, MAPE: 4.07%, R²: -73.8357\n",
      "ElasticNet - Test RMSE: 6.8927, MAE: 6.8773, MAPE: 6.86%, R²: -209.7500\n",
      "Random Forest - Test RMSE: 0.0879, MAE: 0.0202, MAPE: 0.02%, R²: 0.9657\n",
      "Gradient Boosting - Test RMSE: 0.0613, MAE: 0.0178, MAPE: 0.02%, R²: 0.9833\n",
      "XGBoost - Test RMSE: 0.0739, MAE: 0.0210, MAPE: 0.02%, R²: 0.9758\n",
      "SVR - Test RMSE: 0.6335, MAE: 0.5309, MAPE: 0.53%, R²: -0.7800\n",
      "Generating 24 period forecast...\n",
      "\n",
      "Future cpi_mom forecasts:\n",
      "2025-01-01    112.795031\n",
      "2025-02-01    112.795031\n",
      "2025-03-01    112.795031\n",
      "2025-04-01    112.795031\n",
      "2025-05-01    112.795031\n",
      "2025-06-01    112.795031\n",
      "2025-07-01    112.795031\n",
      "2025-08-01    112.795031\n",
      "2025-09-01    112.795031\n",
      "2025-10-01    112.795031\n",
      "2025-11-01    112.795031\n",
      "2025-12-01    112.795031\n",
      "2026-01-01    112.795031\n",
      "2026-02-01    112.795031\n",
      "2026-03-01    112.795031\n",
      "2026-04-01    112.795031\n",
      "2026-05-01    112.795031\n",
      "2026-06-01    112.795031\n",
      "2026-07-01    112.795031\n",
      "2026-08-01    112.795031\n",
      "2026-09-01    112.795031\n",
      "2026-10-01    112.795031\n",
      "2026-11-01    112.795031\n",
      "2026-12-01    112.795031\n",
      "Freq: MS, dtype: float64\n",
      "Could not compare with other models: [Errno 2] No such file or directory: 'plots/all_models_comparison_metrics.csv'\n",
      "\n",
      "Metrics saved to plots/stacking_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_palette(\"deep\")\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load and prepare the time series data\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert time column to datetime\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    \n",
    "    # Set time as index\n",
    "    df.set_index('time', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"Create additional features for modeling\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Create lag features\n",
    "    for i in range(1, 13):\n",
    "        if 'cpi_mom' in df_copy.columns:\n",
    "            df_copy[f'cpi_mom_lag_{i}'] = df_copy['cpi_mom'].shift(i)\n",
    "        if 'cpi_yoy' in df_copy.columns:\n",
    "            df_copy[f'cpi_yoy_lag_{i}'] = df_copy['cpi_yoy'].shift(i)\n",
    "    \n",
    "    # Create rolling window features\n",
    "    for window in [3, 6, 12]:\n",
    "        if 'cpi_mom' in df_copy.columns:\n",
    "            df_copy[f'cpi_mom_rolling_mean_{window}'] = df_copy['cpi_mom'].rolling(window=window).mean()\n",
    "            df_copy[f'cpi_mom_rolling_std_{window}'] = df_copy['cpi_mom'].rolling(window=window).std()\n",
    "        if 'cpi_yoy' in df_copy.columns:\n",
    "            df_copy[f'cpi_yoy_rolling_mean_{window}'] = df_copy['cpi_yoy'].rolling(window=window).mean()\n",
    "            df_copy[f'cpi_yoy_rolling_std_{window}'] = df_copy['cpi_yoy'].rolling(window=window).std()\n",
    "    \n",
    "    # Create economic indicator lag features\n",
    "    for i in range(1, 4):\n",
    "        if 'oil_price' in df_copy.columns:\n",
    "            df_copy[f'oil_price_lag_{i}'] = df_copy['oil_price'].shift(i)\n",
    "        if 'gold_price' in df_copy.columns:\n",
    "            df_copy[f'gold_price_lag_{i}'] = df_copy['gold_price'].shift(i)\n",
    "        if 'interest_rate' in df_copy.columns:\n",
    "            df_copy[f'interest_rate_lag_{i}'] = df_copy['interest_rate'].shift(i)\n",
    "    \n",
    "    # Add month and year as cyclical features\n",
    "    if 'month' in df_copy.columns:\n",
    "        df_copy['month_sin'] = np.sin(2 * np.pi * df_copy['month']/12)\n",
    "        df_copy['month_cos'] = np.cos(2 * np.pi * df_copy['month']/12)\n",
    "    \n",
    "    # Create interaction features\n",
    "    if all(col in df_copy.columns for col in ['oil_price', 'gold_price']):\n",
    "        df_copy['oil_gold_ratio'] = df_copy['oil_price'] / df_copy['gold_price']\n",
    "    \n",
    "    # Drop rows with NaN values (due to lag features)\n",
    "    df_clean = df_copy.dropna()\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def calculate_metrics(actual, predicted):\n",
    "    \"\"\"Calculate evaluation metrics\"\"\"\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    r2 = r2_score(actual, predicted)\n",
    "    mape = np.mean(np.abs((actual - predicted) / actual)) * 100\n",
    "    \n",
    "    return rmse, mae, mape, r2\n",
    "\n",
    "def prepare_data_for_modeling(df, target_col, test_size=0.2, validation_size=0.1):\n",
    "    \"\"\"\n",
    "    Split data into features and target, and into training, validation and testing sets\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with features and target\n",
    "    target_col : str\n",
    "        Name of the target column\n",
    "    test_size : float\n",
    "        Proportion of data to use for testing\n",
    "    validation_size : float\n",
    "        Proportion of remaining data to use for validation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test, scaler\n",
    "    \"\"\"\n",
    "    # Define features and target\n",
    "    X = df.drop([col for col in ['cpi_mom', 'cpi_yoy', target_col] if col in df.columns], axis=1)\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Split data into training and testing sets (time-based split)\n",
    "    split_test_idx = int(len(df) * (1 - test_size))\n",
    "    X_temp, X_test = X.iloc[:split_test_idx], X.iloc[split_test_idx:]\n",
    "    y_temp, y_test = y.iloc[:split_test_idx], y.iloc[split_test_idx:]\n",
    "    \n",
    "    # Split remaining data into training and validation\n",
    "    split_val_idx = int(len(X_temp) * (1 - validation_size))\n",
    "    X_train, X_val = X_temp.iloc[:split_val_idx], X_temp.iloc[split_val_idx:]\n",
    "    y_train, y_val = y_temp.iloc[:split_val_idx], y_temp.iloc[split_val_idx:]\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, X_train_scaled, X_val_scaled, X_test_scaled, scaler\n",
    "\n",
    "def train_base_models(X_train, X_val, y_train, y_val, X_train_scaled, X_val_scaled):\n",
    "    \"\"\"\n",
    "    Train base models for stacking\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train, X_val : pd.DataFrame\n",
    "        Training and validation features\n",
    "    y_train, y_val : pd.Series\n",
    "        Training and validation targets\n",
    "    X_train_scaled, X_val_scaled : np.array\n",
    "        Scaled training and validation features\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary of trained models and their validation predictions\n",
    "    \"\"\"\n",
    "    # Define base models\n",
    "    base_models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Ridge': Ridge(alpha=0.1),\n",
    "        'Lasso': Lasso(alpha=0.01),\n",
    "        'ElasticNet': ElasticNet(alpha=0.01, l1_ratio=0.5),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "        'XGBoost': xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42),\n",
    "        'SVR': SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "    }\n",
    "    \n",
    "    # Train base models and get validation predictions\n",
    "    base_model_preds = {}\n",
    "    \n",
    "    for name, model in base_models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        \n",
    "        # Use scaled data for linear models and SVR, unscaled for tree-based models\n",
    "        if name in ['Linear Regression', 'Ridge', 'Lasso', 'ElasticNet', 'SVR']:\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            val_pred = model.predict(X_val_scaled)\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "            val_pred = model.predict(X_val)\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        val_rmse, val_mae, val_mape, val_r2 = calculate_metrics(y_val, val_pred)\n",
    "        print(f\"{name} - Validation RMSE: {val_rmse:.4f}, MAE: {val_mae:.4f}, MAPE: {val_mape:.2f}%, R²: {val_r2:.4f}\")\n",
    "        \n",
    "        # Store model and validation predictions\n",
    "        base_model_preds[name] = {\n",
    "            'model': model,\n",
    "            'val_pred': val_pred,\n",
    "            'val_rmse': val_rmse,\n",
    "            'val_mae': val_mae,\n",
    "            'val_mape': val_mape,\n",
    "            'val_r2': val_r2\n",
    "        }\n",
    "    \n",
    "    return base_model_preds\n",
    "\n",
    "def train_meta_model(base_model_preds, X_val, y_val, X_test, meta_model_type='Ridge'):\n",
    "    \"\"\"\n",
    "    Train meta-model for stacking\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_model_preds : dict\n",
    "        Dictionary of base models and their validation predictions\n",
    "    X_val, y_val : pd.DataFrame, pd.Series\n",
    "        Validation features and target\n",
    "    X_test : pd.DataFrame\n",
    "        Test features\n",
    "    meta_model_type : str\n",
    "        Type of meta-model to use\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        Meta-model, test predictions, and base model test predictions\n",
    "    \"\"\"\n",
    "    # Create meta-features from base model predictions\n",
    "    meta_features = pd.DataFrame({\n",
    "        name: preds['val_pred'] for name, preds in base_model_preds.items()\n",
    "    }, index=X_val.index)\n",
    "    \n",
    "    # Select meta-model\n",
    "    if meta_model_type == 'Ridge':\n",
    "        meta_model = Ridge(alpha=0.1)\n",
    "    elif meta_model_type == 'Lasso':\n",
    "        meta_model = Lasso(alpha=0.01)\n",
    "    elif meta_model_type == 'ElasticNet':\n",
    "        meta_model = ElasticNet(alpha=0.01, l1_ratio=0.5)\n",
    "    elif meta_model_type == 'Linear':\n",
    "        meta_model = LinearRegression()\n",
    "    else:\n",
    "        meta_model = Ridge(alpha=0.1)  # Default\n",
    "    \n",
    "    # Train meta-model\n",
    "    print(f\"\\nTraining {meta_model_type} meta-model...\")\n",
    "    meta_model.fit(meta_features, y_val)\n",
    "    \n",
    "    # Get base model predictions on test set\n",
    "    base_test_preds = {}\n",
    "    for name, model_dict in base_model_preds.items():\n",
    "        model = model_dict['model']\n",
    "        \n",
    "        # Use scaled data for linear models and SVR, unscaled for tree-based models\n",
    "        if name in ['Linear Regression', 'Ridge', 'Lasso', 'ElasticNet', 'SVR']:\n",
    "            test_pred = model.predict(X_test)\n",
    "        else:\n",
    "            test_pred = model.predict(X_test)\n",
    "        \n",
    "        base_test_preds[name] = test_pred\n",
    "    \n",
    "    # Create meta-features for test set\n",
    "    meta_features_test = pd.DataFrame(base_test_preds, index=X_test.index)\n",
    "    \n",
    "    # Make predictions with meta-model\n",
    "    test_pred = meta_model.predict(meta_features_test)\n",
    "    \n",
    "    # Get meta-model coefficients\n",
    "    if hasattr(meta_model, 'coef_'):\n",
    "        print(\"\\nMeta-model coefficients:\")\n",
    "        for name, coef in zip(base_test_preds.keys(), meta_model.coef_):\n",
    "            print(f\"{name}: {coef:.4f}\")\n",
    "    \n",
    "    return meta_model, test_pred, base_test_preds, meta_features_test\n",
    "\n",
    "def run_stacking(df, target_col, test_size=0.2, validation_size=0.1, meta_model_type='Ridge'):\n",
    "    \"\"\"\n",
    "    Run stacking ensemble for time series forecasting\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with engineered features\n",
    "    target_col : str\n",
    "        Name of the target column\n",
    "    test_size : float\n",
    "        Proportion of data to use for testing\n",
    "    validation_size : float\n",
    "        Proportion of remaining data to use for validation\n",
    "    meta_model_type : str\n",
    "        Type of meta-model to use\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        Meta-model, predictions, metrics\n",
    "    \"\"\"\n",
    "    print(f\"Running Stacking Ensemble for {target_col}...\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test, X_train_scaled, X_val_scaled, X_test_scaled, scaler = prepare_data_for_modeling(\n",
    "        df, target_col, test_size, validation_size\n",
    "    )\n",
    "    \n",
    "    print(f\"Training data: {X_train.shape}, Validation data: {X_val.shape}, Test data: {X_test.shape}\")\n",
    "    \n",
    "    # Train base models\n",
    "    base_model_preds = train_base_models(X_train, X_val, y_train, y_val, X_train_scaled, X_val_scaled)\n",
    "    \n",
    "    # Train meta-model\n",
    "    meta_model, test_pred, base_test_preds, meta_features_test = train_meta_model(\n",
    "        base_model_preds, X_val, y_val, X_test, meta_model_type\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics for stacking ensemble\n",
    "    stack_rmse, stack_mae, stack_mape, stack_r2 = calculate_metrics(y_test, test_pred)\n",
    "    print(f\"\\nStacking Ensemble - Test RMSE: {stack_rmse:.4f}, MAE: {stack_mae:.4f}, MAPE: {stack_mape:.2f}%, R²: {stack_r2:.4f}\")\n",
    "    \n",
    "    # Calculate metrics for individual base models on test set\n",
    "    print(\"\\nBase Model Test Performance:\")\n",
    "    base_metrics = {}\n",
    "    for name, pred in base_test_preds.items():\n",
    "        rmse, mae, mape, r2 = calculate_metrics(y_test, pred)\n",
    "        print(f\"{name} - Test RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.2f}%, R²: {r2:.4f}\")\n",
    "        base_metrics[name] = {'rmse': rmse, 'mae': mae, 'mape': mape, 'r2': r2}\n",
    "    \n",
    "    # Plot actual vs predicted for stacking and base models\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(y_test.index, y_test, label='Actual', color='black', linewidth=2)\n",
    "    plt.plot(y_test.index, test_pred, label='Stacking Ensemble', color='red', linewidth=2)\n",
    "    \n",
    "    # Plot top 3 base models\n",
    "    top_models = sorted(base_metrics.items(), key=lambda x: x[1]['rmse'])[:3]\n",
    "    for name, metrics in top_models:\n",
    "        plt.plot(y_test.index, base_test_preds[name], label=f'{name}', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.title(f'Stacking Ensemble: Actual vs Predicted {target_col}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(target_col)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/stacking_predictions_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot error distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    errors = y_test - test_pred\n",
    "    sns.histplot(errors, kde=True)\n",
    "    plt.title(f'Stacking Ensemble: Error Distribution for {target_col}')\n",
    "    plt.xlabel('Error')\n",
    "    plt.savefig(f'plots/stacking_error_distribution_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create a DataFrame with predictions\n",
    "    predictions = pd.DataFrame({\n",
    "        'Actual': y_test,\n",
    "        'Stacking': test_pred,\n",
    "    })\n",
    "    \n",
    "    # Add top 3 base models\n",
    "    for name, _ in top_models:\n",
    "        predictions[name] = base_test_preds[name]\n",
    "    \n",
    "    # Add errors\n",
    "    predictions['Error'] = predictions['Actual'] - predictions['Stacking']\n",
    "    \n",
    "    # Return results\n",
    "    metrics = {\n",
    "        'rmse': stack_rmse,\n",
    "        'mae': stack_mae,\n",
    "        'mape': stack_mape,\n",
    "        'r2': stack_r2,\n",
    "        'base_metrics': base_metrics\n",
    "    }\n",
    "    \n",
    "    return meta_model, predictions, metrics, base_model_preds, meta_features_test\n",
    "\n",
    "def forecast_future_stacking(meta_model, base_models, df, target_col, forecast_horizon=24, scaler=None):\n",
    "    \"\"\"\n",
    "    Generate future forecasts using the stacking ensemble\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    meta_model : sklearn model\n",
    "        Trained meta-model\n",
    "    base_models : dict\n",
    "        Dictionary of trained base models\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with features\n",
    "    target_col : str\n",
    "        Name of the target column\n",
    "    forecast_horizon : int\n",
    "        Number of periods to forecast\n",
    "    scaler : StandardScaler\n",
    "        Scaler used for features\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        Forecasted values\n",
    "    \"\"\"\n",
    "    print(f\"Generating {forecast_horizon} period forecast...\")\n",
    "    \n",
    "    # Create a copy of the dataframe\n",
    "    df_forecast = df.copy()\n",
    "    \n",
    "    # Get the last date in the dataframe\n",
    "    last_date = df_forecast.index[-1]\n",
    "    \n",
    "    # Create a list to store forecasts\n",
    "    forecasts = []\n",
    "    \n",
    "    # Generate forecasts recursively\n",
    "    for i in range(forecast_horizon):\n",
    "        # Get the features for the next period\n",
    "        if i == 0:\n",
    "            # For the first forecast, use the last row of the dataframe\n",
    "            X_next = df_forecast.drop([col for col in ['cpi_mom', 'cpi_yoy', target_col] if col in df_forecast.columns], axis=1).iloc[-1:].copy()\n",
    "        else:\n",
    "            # For a more accurate implementation, we would need to update all features\n",
    "            # based on the new forecast, but this is a simplified version\n",
    "            # Here we just use the last known features\n",
    "            X_next = df_forecast.drop([col for col in ['cpi_mom', 'cpi_yoy', target_col] if col in df_forecast.columns], axis=1).iloc[-1:].copy()\n",
    "        \n",
    "        # Get base model predictions\n",
    "        base_preds = {}\n",
    "        for name, model_dict in base_models.items():\n",
    "            model = model_dict['model']\n",
    "            \n",
    "            # Use scaled data for linear models and SVR, unscaled for tree-based models\n",
    "            if name in ['Linear Regression', 'Ridge', 'Lasso', 'ElasticNet', 'SVR']:\n",
    "                if scaler:\n",
    "                    X_next_scaled = scaler.transform(X_next)\n",
    "                    pred = model.predict(X_next_scaled)[0]\n",
    "                else:\n",
    "                    pred = model.predict(X_next)[0]\n",
    "            else:\n",
    "                pred = model.predict(X_next)[0]\n",
    "            \n",
    "            base_preds[name] = pred\n",
    "        \n",
    "        # Create meta-features\n",
    "        meta_features = pd.DataFrame([base_preds])\n",
    "        \n",
    "        # Make prediction with meta-model\n",
    "        forecast = meta_model.predict(meta_features)[0]\n",
    "        \n",
    "        # Store forecast\n",
    "        forecasts.append(forecast)\n",
    "        \n",
    "        # Create next date\n",
    "        next_date = last_date + pd.DateOffset(months=i+1)\n",
    "    \n",
    "    # Create a Series with the forecasts\n",
    "    future_dates = pd.date_range(start=last_date + pd.DateOffset(months=1), periods=forecast_horizon, freq='MS')\n",
    "    forecast_series = pd.Series(forecasts, index=future_dates)\n",
    "    \n",
    "    # Plot historical data with forecasts\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df[target_col].index, df[target_col], label='Historical Data')\n",
    "    plt.plot(forecast_series.index, forecast_series, label='Stacking Forecast', color='red', linestyle='--')\n",
    "    plt.title(f'Stacking Ensemble: {target_col} Forecast')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(target_col)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/stacking_future_forecast_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return forecast_series\n",
    "\n",
    "def main():\n",
    "    # Create plots directory if it doesn't exist\n",
    "    import os\n",
    "    if not os.path.exists('plots'):\n",
    "        os.makedirs('plots')\n",
    "    \n",
    "    # Load data\n",
    "    file_path = 'data/analyzed_time_series.csv'\n",
    "    df = load_data(file_path)\n",
    "    \n",
    "    # Engineer features\n",
    "    df_engineered = engineer_features(df)\n",
    "    \n",
    "    # Run stacking for CPI Year-over-Year\n",
    "    target_col = 'cpi_yoy'\n",
    "    meta_model_yoy, predictions_yoy, metrics_yoy, base_models_yoy, meta_features_test_yoy = run_stacking(\n",
    "        df_engineered, target_col, test_size=0.2, validation_size=0.1, meta_model_type='Ridge'\n",
    "    )\n",
    "    \n",
    "    # Generate future forecasts for YoY\n",
    "    forecast_yoy = forecast_future_stacking(\n",
    "        meta_model_yoy, base_models_yoy, df_engineered, target_col, forecast_horizon=24\n",
    "    )\n",
    "    print(f\"\\nFuture {target_col} forecasts:\")\n",
    "    print(forecast_yoy)\n",
    "    \n",
    "    # Run stacking for CPI Month-over-Month\n",
    "    target_col = 'cpi_mom'\n",
    "    meta_model_mom, predictions_mom, metrics_mom, base_models_mom, meta_features_test_mom = run_stacking(\n",
    "        df_engineered, target_col, test_size=0.2, validation_size=0.1, meta_model_type='Ridge'\n",
    "    )\n",
    "    \n",
    "    # Generate future forecasts for MoM\n",
    "    forecast_mom = forecast_future_stacking(\n",
    "        meta_model_mom, base_models_mom, df_engineered, target_col, forecast_horizon=24\n",
    "    )\n",
    "    print(f\"\\nFuture {target_col} forecasts:\")\n",
    "    print(forecast_mom)\n",
    "    \n",
    "    # Compare with other models (if available)\n",
    "    try:\n",
    "        # Try to load metrics from other models\n",
    "        all_metrics = pd.read_csv('plots/all_models_comparison_metrics.csv')\n",
    "        \n",
    "        # Create stacking metrics dataframe\n",
    "        stacking_metrics = pd.DataFrame([\n",
    "            {\n",
    "                'Model': 'Stacking', 'Target': 'CPI MoM',\n",
    "                'Train_RMSE': None, 'Train_MAE': None, \n",
    "                'Train_MAPE': None, 'Train_R2': None,\n",
    "                'Test_RMSE': metrics_mom['rmse'], 'Test_MAE': metrics_mom['mae'], \n",
    "                'Test_MAPE': metrics_mom['mape'], 'Test_R2': metrics_mom['r2']\n",
    "            },\n",
    "            {\n",
    "                'Model': 'Stacking', 'Target': 'CPI YoY',\n",
    "                'Train_RMSE': None, 'Train_MAE': None, \n",
    "                'Train_MAPE': None, 'Train_R2': None,\n",
    "                'Test_RMSE': metrics_yoy['rmse'], 'Test_MAE': metrics_yoy['mae'], \n",
    "                'Test_MAPE': metrics_yoy['mape'], 'Test_R2': metrics_yoy['r2']\n",
    "            }\n",
    "        ])\n",
    "        \n",
    "        # Combine metrics\n",
    "        combined_metrics = pd.concat([all_metrics, stacking_metrics])\n",
    "        combined_metrics.to_csv('plots/final_model_comparison_metrics.csv', index=False)\n",
    "        print(\"\\nComparison with all models saved to plots/final_model_comparison_metrics.csv\")\n",
    "        \n",
    "        # Create comparison plots\n",
    "        for target, target_name in [('CPI MoM', 'cpi_mom'), ('CPI YoY', 'cpi_yoy')]:\n",
    "            # Filter metrics for the target\n",
    "            target_metrics = combined_metrics[combined_metrics['Target'] == target]\n",
    "            \n",
    "            # Compare test metrics\n",
    "            metrics_to_compare = ['Test_RMSE', 'Test_MAE', 'Test_MAPE']\n",
    "            \n",
    "            plt.figure(figsize=(16, 8))\n",
    "            \n",
    "            # Create grouped bar chart\n",
    "            x = np.arange(len(metrics_to_compare))\n",
    "            width = 0.1\n",
    "            \n",
    "            # Plot bars for each model\n",
    "            models = target_metrics['Model'].unique()\n",
    "            for i, model_name in enumerate(models):\n",
    "                model_data = target_metrics[target_metrics['Model'] == model_name]\n",
    "                if not model_data.empty and not model_data[metrics_to_compare].isnull().any().any():\n",
    "                    values = [model_data[metric].values[0] for metric in metrics_to_compare]\n",
    "                    plt.bar(x + (i-len(models)/2+0.5)*width, values, width, label=model_name)\n",
    "            \n",
    "            plt.xlabel('Metric')\n",
    "            plt.ylabel('Value')\n",
    "            plt.title(f'All Models Comparison for {target}')\n",
    "            plt.xticks(x, metrics_to_compare)\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'plots/final_model_comparison_{target_name}.png')\n",
    "            plt.close()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Could not compare with other models: {e}\")\n",
    "        # Save metrics to CSV\n",
    "        metrics_data = [\n",
    "            {\n",
    "                'Model': 'Stacking', 'Target': 'CPI MoM',\n",
    "                'Test_RMSE': metrics_mom['rmse'], 'Test_MAE': metrics_mom['mae'], \n",
    "                'Test_MAPE': metrics_mom['mape'], 'Test_R2': metrics_mom['r2']\n",
    "            },\n",
    "            {\n",
    "                'Model': 'Stacking', 'Target': 'CPI YoY',\n",
    "                'Test_RMSE': metrics_yoy['rmse'], 'Test_MAE': metrics_yoy['mae'], \n",
    "                'Test_MAPE': metrics_yoy['mape'], 'Test_R2': metrics_yoy['r2']\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        metrics_df = pd.DataFrame(metrics_data)\n",
    "        metrics_df.to_csv('plots/stacking_metrics.csv', index=False)\n",
    "        print(\"\\nMetrics saved to plots/stacking_metrics.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c61442a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 23:26:46,908 - INFO - Optimizing hybrid weights for cpi_yoy\n",
      "2025-05-09 23:26:46,918 - INFO - Running ARIMA component with seasonal=True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing stepwise search to minimize aic\n",
      " ARIMA(0,1,0)(0,0,0)[12] intercept   : AIC=771.285, Time=0.03 sec\n",
      " ARIMA(1,1,0)(1,0,0)[12] intercept   : AIC=516.945, Time=0.18 sec\n",
      " ARIMA(0,1,1)(0,0,1)[12] intercept   : AIC=523.336, Time=0.22 sec\n",
      " ARIMA(0,1,0)(0,0,0)[12]             : AIC=769.294, Time=0.03 sec\n",
      " ARIMA(1,1,0)(0,0,0)[12] intercept   : AIC=590.401, Time=0.06 sec\n",
      " ARIMA(1,1,0)(2,0,0)[12] intercept   : AIC=489.897, Time=0.53 sec\n",
      " ARIMA(1,1,0)(2,0,1)[12] intercept   : AIC=471.281, Time=0.90 sec\n",
      " ARIMA(1,1,0)(1,0,1)[12] intercept   : AIC=470.575, Time=0.25 sec\n",
      " ARIMA(1,1,0)(0,0,1)[12] intercept   : AIC=469.818, Time=0.31 sec\n",
      " ARIMA(1,1,0)(0,0,2)[12] intercept   : AIC=470.313, Time=0.59 sec\n",
      " ARIMA(1,1,0)(1,0,2)[12] intercept   : AIC=inf, Time=1.64 sec\n",
      " ARIMA(0,1,0)(0,0,1)[12] intercept   : AIC=654.754, Time=0.19 sec\n",
      " ARIMA(2,1,0)(0,0,1)[12] intercept   : AIC=471.498, Time=0.28 sec\n",
      " ARIMA(1,1,1)(0,0,1)[12] intercept   : AIC=471.366, Time=0.53 sec\n",
      " ARIMA(2,1,1)(0,0,1)[12] intercept   : AIC=471.363, Time=0.67 sec\n",
      " ARIMA(1,1,0)(0,0,1)[12]             : AIC=467.818, Time=0.19 sec\n",
      " ARIMA(1,1,0)(0,0,0)[12]             : AIC=588.402, Time=0.04 sec\n",
      " ARIMA(1,1,0)(1,0,1)[12]             : AIC=468.576, Time=0.17 sec\n",
      " ARIMA(1,1,0)(0,0,2)[12]             : AIC=468.314, Time=0.63 sec\n",
      " ARIMA(1,1,0)(1,0,0)[12]             : AIC=514.945, Time=0.15 sec\n",
      " ARIMA(1,1,0)(1,0,2)[12]             : AIC=inf, Time=1.75 sec\n",
      " ARIMA(0,1,0)(0,0,1)[12]             : AIC=652.771, Time=0.13 sec\n",
      " ARIMA(2,1,0)(0,0,1)[12]             : AIC=469.499, Time=0.20 sec\n",
      " ARIMA(1,1,1)(0,0,1)[12]             : AIC=469.366, Time=0.23 sec\n",
      " ARIMA(0,1,1)(0,0,1)[12]             : AIC=521.343, Time=0.19 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 23:26:57,535 - INFO - Best ARIMA parameters: (1, 1, 0), seasonal_order: (0, 0, 1, 12)\n",
      "2025-05-09 23:26:57,547 - INFO - ARIMA component - RMSE: 1.5938, MAE: 1.2600, MAPE: 1.23%, R²: -0.7079\n",
      "2025-05-09 23:26:57,548 - INFO - Running ML component with model_type=XGBoost\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ARIMA(2,1,1)(0,0,1)[12]             : AIC=469.363, Time=0.41 sec\n",
      "\n",
      "Best model:  ARIMA(1,1,0)(0,0,1)[12]          \n",
      "Total fit time: 10.585 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 23:26:59,771 - INFO - ML component (XGBoost) - RMSE: 0.0808, MAE: 0.0510, MAPE: 0.05%, R²: 0.9956\n",
      "2025-05-09 23:26:59,843 - INFO - Best weight for ARIMA component: 0.00 with RMSE: 0.0808\n",
      "2025-05-09 23:27:00,067 - INFO - Running hybrid model for cpi_yoy with arima_weight=0.0, ml_model_type=XGBoost\n",
      "2025-05-09 23:27:00,093 - INFO - Running ARIMA component with seasonal=True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing stepwise search to minimize aic\n",
      " ARIMA(0,1,0)(0,0,0)[12] intercept   : AIC=771.285, Time=0.03 sec\n",
      " ARIMA(1,1,0)(1,0,0)[12] intercept   : AIC=516.945, Time=0.18 sec\n",
      " ARIMA(0,1,1)(0,0,1)[12] intercept   : AIC=523.336, Time=0.22 sec\n",
      " ARIMA(0,1,0)(0,0,0)[12]             : AIC=769.294, Time=0.03 sec\n",
      " ARIMA(1,1,0)(0,0,0)[12] intercept   : AIC=590.401, Time=0.14 sec\n",
      " ARIMA(1,1,0)(2,0,0)[12] intercept   : AIC=489.897, Time=0.59 sec\n",
      " ARIMA(1,1,0)(2,0,1)[12] intercept   : AIC=471.281, Time=0.84 sec\n",
      " ARIMA(1,1,0)(1,0,1)[12] intercept   : AIC=470.575, Time=0.25 sec\n",
      " ARIMA(1,1,0)(0,0,1)[12] intercept   : AIC=469.818, Time=0.28 sec\n",
      " ARIMA(1,1,0)(0,0,2)[12] intercept   : AIC=470.313, Time=0.59 sec\n",
      " ARIMA(1,1,0)(1,0,2)[12] intercept   : AIC=inf, Time=1.86 sec\n",
      " ARIMA(0,1,0)(0,0,1)[12] intercept   : AIC=654.754, Time=0.18 sec\n",
      " ARIMA(2,1,0)(0,0,1)[12] intercept   : AIC=471.498, Time=0.32 sec\n",
      " ARIMA(1,1,1)(0,0,1)[12] intercept   : AIC=471.366, Time=0.51 sec\n",
      " ARIMA(2,1,1)(0,0,1)[12] intercept   : AIC=471.363, Time=0.71 sec\n",
      " ARIMA(1,1,0)(0,0,1)[12]             : AIC=467.818, Time=0.19 sec\n",
      " ARIMA(1,1,0)(0,0,0)[12]             : AIC=588.402, Time=0.04 sec\n",
      " ARIMA(1,1,0)(1,0,1)[12]             : AIC=468.576, Time=0.21 sec\n",
      " ARIMA(1,1,0)(0,0,2)[12]             : AIC=468.314, Time=0.56 sec\n",
      " ARIMA(1,1,0)(1,0,0)[12]             : AIC=514.945, Time=0.11 sec\n",
      " ARIMA(1,1,0)(1,0,2)[12]             : AIC=inf, Time=1.49 sec\n",
      " ARIMA(0,1,0)(0,0,1)[12]             : AIC=652.771, Time=0.11 sec\n",
      " ARIMA(2,1,0)(0,0,1)[12]             : AIC=469.499, Time=0.21 sec\n",
      " ARIMA(1,1,1)(0,0,1)[12]             : AIC=469.366, Time=0.27 sec\n",
      " ARIMA(0,1,1)(0,0,1)[12]             : AIC=521.343, Time=0.18 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 23:27:10,652 - INFO - Best ARIMA parameters: (1, 1, 0), seasonal_order: (0, 0, 1, 12)\n",
      "2025-05-09 23:27:10,666 - INFO - ARIMA component - RMSE: 1.5938, MAE: 1.2600, MAPE: 1.23%, R²: -0.7079\n",
      "2025-05-09 23:27:10,666 - INFO - Running ML component with model_type=XGBoost\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ARIMA(2,1,1)(0,0,1)[12]             : AIC=469.363, Time=0.41 sec\n",
      "\n",
      "Best model:  ARIMA(1,1,0)(0,0,1)[12]          \n",
      "Total fit time: 10.521 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 23:27:10,927 - INFO - ML component (XGBoost) - RMSE: 0.0808, MAE: 0.0510, MAPE: 0.05%, R²: 0.9956\n",
      "2025-05-09 23:27:10,932 - INFO - Hybrid model - RMSE: 0.0808, MAE: 0.0510, MAPE: 0.05%, R²: 0.9956\n",
      "2025-05-09 23:27:11,348 - INFO - Generating 24 period forecast\n",
      "2025-05-09 23:27:12,010 - INFO - \n",
      "Future cpi_yoy forecasts:\n",
      "2025-05-09 23:27:12,012 - INFO - 2025-01-01   NaN\n",
      "2025-02-01   NaN\n",
      "2025-03-01   NaN\n",
      "2025-04-01   NaN\n",
      "2025-05-01   NaN\n",
      "2025-06-01   NaN\n",
      "2025-07-01   NaN\n",
      "2025-08-01   NaN\n",
      "2025-09-01   NaN\n",
      "2025-10-01   NaN\n",
      "2025-11-01   NaN\n",
      "2025-12-01   NaN\n",
      "2026-01-01   NaN\n",
      "2026-02-01   NaN\n",
      "2026-03-01   NaN\n",
      "2026-04-01   NaN\n",
      "2026-05-01   NaN\n",
      "2026-06-01   NaN\n",
      "2026-07-01   NaN\n",
      "2026-08-01   NaN\n",
      "2026-09-01   NaN\n",
      "2026-10-01   NaN\n",
      "2026-11-01   NaN\n",
      "2026-12-01   NaN\n",
      "Freq: MS, dtype: float64\n",
      "2025-05-09 23:27:12,015 - INFO - Optimizing hybrid weights for cpi_mom\n",
      "2025-05-09 23:27:12,026 - INFO - Running ARIMA component with seasonal=True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing stepwise search to minimize aic\n",
      " ARIMA(0,0,0)(0,0,0)[12] intercept   : AIC=650.475, Time=0.05 sec\n",
      " ARIMA(1,0,0)(1,0,0)[12] intercept   : AIC=inf, Time=1.23 sec\n",
      " ARIMA(0,0,1)(0,0,1)[12] intercept   : AIC=546.290, Time=0.15 sec\n",
      " ARIMA(0,0,0)(0,0,0)[12]             : AIC=3245.698, Time=0.02 sec\n",
      " ARIMA(0,0,1)(0,0,0)[12] intercept   : AIC=582.149, Time=0.07 sec\n",
      " ARIMA(0,0,1)(1,0,1)[12] intercept   : AIC=548.279, Time=0.80 sec\n",
      " ARIMA(0,0,1)(0,0,2)[12] intercept   : AIC=533.499, Time=0.45 sec\n",
      " ARIMA(0,0,1)(1,0,2)[12] intercept   : AIC=inf, Time=3.00 sec\n",
      " ARIMA(0,0,0)(0,0,2)[12] intercept   : AIC=625.746, Time=0.24 sec\n",
      " ARIMA(1,0,1)(0,0,2)[12] intercept   : AIC=496.220, Time=1.86 sec\n",
      " ARIMA(1,0,1)(0,0,1)[12] intercept   : AIC=515.101, Time=0.76 sec\n",
      " ARIMA(1,0,1)(1,0,2)[12] intercept   : AIC=inf, Time=3.93 sec\n",
      " ARIMA(1,0,1)(1,0,1)[12] intercept   : AIC=503.227, Time=0.91 sec\n",
      " ARIMA(1,0,0)(0,0,2)[12] intercept   : AIC=494.641, Time=1.35 sec\n",
      " ARIMA(1,0,0)(0,0,1)[12] intercept   : AIC=515.724, Time=0.99 sec\n",
      " ARIMA(1,0,0)(1,0,2)[12] intercept   : AIC=inf, Time=4.10 sec\n",
      " ARIMA(1,0,0)(1,0,1)[12] intercept   : AIC=522.156, Time=0.85 sec\n",
      " ARIMA(2,0,0)(0,0,2)[12] intercept   : AIC=497.084, Time=1.18 sec\n",
      " ARIMA(2,0,1)(0,0,2)[12] intercept   : AIC=496.936, Time=2.59 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 23:27:36,814 - INFO - Best ARIMA parameters: (1, 0, 0), seasonal_order: (0, 0, 2, 12)\n",
      "2025-05-09 23:27:36,831 - INFO - ARIMA component - RMSE: 0.5176, MAE: 0.3955, MAPE: 0.39%, R²: -0.1886\n",
      "2025-05-09 23:27:36,832 - INFO - Running ML component with model_type=XGBoost\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ARIMA(1,0,0)(0,0,2)[12]             : AIC=552.384, Time=0.23 sec\n",
      "\n",
      "Best model:  ARIMA(1,0,0)(0,0,2)[12] intercept\n",
      "Total fit time: 24.762 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 23:27:37,166 - INFO - ML component (XGBoost) - RMSE: 0.0784, MAE: 0.0203, MAPE: 0.02%, R²: 0.9728\n",
      "2025-05-09 23:27:37,239 - INFO - Best weight for ARIMA component: 0.00 with RMSE: 0.0784\n",
      "2025-05-09 23:27:37,415 - INFO - Running hybrid model for cpi_mom with arima_weight=0.0, ml_model_type=XGBoost\n",
      "2025-05-09 23:27:37,426 - INFO - Running ARIMA component with seasonal=True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing stepwise search to minimize aic\n",
      " ARIMA(0,0,0)(0,0,0)[12] intercept   : AIC=650.475, Time=0.05 sec\n",
      " ARIMA(1,0,0)(1,0,0)[12] intercept   : AIC=inf, Time=1.18 sec\n",
      " ARIMA(0,0,1)(0,0,1)[12] intercept   : AIC=546.290, Time=0.18 sec\n",
      " ARIMA(0,0,0)(0,0,0)[12]             : AIC=3245.698, Time=0.02 sec\n",
      " ARIMA(0,0,1)(0,0,0)[12] intercept   : AIC=582.149, Time=0.07 sec\n",
      " ARIMA(0,0,1)(1,0,1)[12] intercept   : AIC=548.279, Time=0.84 sec\n",
      " ARIMA(0,0,1)(0,0,2)[12] intercept   : AIC=533.499, Time=0.40 sec\n",
      " ARIMA(0,0,1)(1,0,2)[12] intercept   : AIC=inf, Time=3.13 sec\n",
      " ARIMA(0,0,0)(0,0,2)[12] intercept   : AIC=625.746, Time=0.23 sec\n",
      " ARIMA(1,0,1)(0,0,2)[12] intercept   : AIC=496.220, Time=1.91 sec\n",
      " ARIMA(1,0,1)(0,0,1)[12] intercept   : AIC=515.101, Time=0.86 sec\n",
      " ARIMA(1,0,1)(1,0,2)[12] intercept   : AIC=inf, Time=3.87 sec\n",
      " ARIMA(1,0,1)(1,0,1)[12] intercept   : AIC=503.227, Time=1.03 sec\n",
      " ARIMA(1,0,0)(0,0,2)[12] intercept   : AIC=494.641, Time=1.40 sec\n",
      " ARIMA(1,0,0)(0,0,1)[12] intercept   : AIC=515.724, Time=0.98 sec\n",
      " ARIMA(1,0,0)(1,0,2)[12] intercept   : AIC=inf, Time=3.92 sec\n",
      " ARIMA(1,0,0)(1,0,1)[12] intercept   : AIC=522.156, Time=0.87 sec\n",
      " ARIMA(2,0,0)(0,0,2)[12] intercept   : AIC=497.084, Time=1.20 sec\n",
      " ARIMA(2,0,1)(0,0,2)[12] intercept   : AIC=496.936, Time=2.60 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 23:28:02,430 - INFO - Best ARIMA parameters: (1, 0, 0), seasonal_order: (0, 0, 2, 12)\n",
      "2025-05-09 23:28:02,445 - INFO - ARIMA component - RMSE: 0.5176, MAE: 0.3955, MAPE: 0.39%, R²: -0.1886\n",
      "2025-05-09 23:28:02,446 - INFO - Running ML component with model_type=XGBoost\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ARIMA(1,0,0)(0,0,2)[12]             : AIC=552.384, Time=0.22 sec\n",
      "\n",
      "Best model:  ARIMA(1,0,0)(0,0,2)[12] intercept\n",
      "Total fit time: 24.980 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 23:28:02,738 - INFO - ML component (XGBoost) - RMSE: 0.0784, MAE: 0.0203, MAPE: 0.02%, R²: 0.9728\n",
      "2025-05-09 23:28:02,743 - INFO - Hybrid model - RMSE: 0.0784, MAE: 0.0203, MAPE: 0.02%, R²: 0.9728\n",
      "2025-05-09 23:28:03,229 - INFO - Generating 24 period forecast\n",
      "2025-05-09 23:28:03,859 - INFO - \n",
      "Future cpi_mom forecasts:\n",
      "2025-05-09 23:28:03,860 - INFO - 2025-01-01   NaN\n",
      "2025-02-01   NaN\n",
      "2025-03-01   NaN\n",
      "2025-04-01   NaN\n",
      "2025-05-01   NaN\n",
      "2025-06-01   NaN\n",
      "2025-07-01   NaN\n",
      "2025-08-01   NaN\n",
      "2025-09-01   NaN\n",
      "2025-10-01   NaN\n",
      "2025-11-01   NaN\n",
      "2025-12-01   NaN\n",
      "2026-01-01   NaN\n",
      "2026-02-01   NaN\n",
      "2026-03-01   NaN\n",
      "2026-04-01   NaN\n",
      "2026-05-01   NaN\n",
      "2026-06-01   NaN\n",
      "2026-07-01   NaN\n",
      "2026-08-01   NaN\n",
      "2026-09-01   NaN\n",
      "2026-10-01   NaN\n",
      "2026-11-01   NaN\n",
      "2026-12-01   NaN\n",
      "Freq: MS, dtype: float64\n",
      "2025-05-09 23:28:03,862 - WARNING - Could not compare with other models: [Errno 2] No such file or directory: 'plots/final_model_comparison_metrics.csv'\n",
      "2025-05-09 23:28:03,867 - INFO - \n",
      "Metrics saved to plots/hybrid_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from pmdarima import auto_arima\n",
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure logging\n",
    "if not os.path.exists('logs'):\n",
    "    os.makedirs('logs')\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('logs/hybrid_model_log.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load and prepare the time series data\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert time column to datetime\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    \n",
    "    # Set time as index\n",
    "    df.set_index('time', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"Create additional features for modeling\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Create lag features\n",
    "    for i in range(1, 13):\n",
    "        if 'cpi_mom' in df_copy.columns:\n",
    "            df_copy[f'cpi_mom_lag_{i}'] = df_copy['cpi_mom'].shift(i)\n",
    "        if 'cpi_yoy' in df_copy.columns:\n",
    "            df_copy[f'cpi_yoy_lag_{i}'] = df_copy['cpi_yoy'].shift(i)\n",
    "    \n",
    "    # Create rolling window features\n",
    "    for window in [3, 6, 12]:\n",
    "        if 'cpi_mom' in df_copy.columns:\n",
    "            df_copy[f'cpi_mom_rolling_mean_{window}'] = df_copy['cpi_mom'].rolling(window=window).mean()\n",
    "            df_copy[f'cpi_mom_rolling_std_{window}'] = df_copy['cpi_mom'].rolling(window=window).std()\n",
    "        if 'cpi_yoy' in df_copy.columns:\n",
    "            df_copy[f'cpi_yoy_rolling_mean_{window}'] = df_copy['cpi_yoy'].rolling(window=window).mean()\n",
    "            df_copy[f'cpi_yoy_rolling_std_{window}'] = df_copy['cpi_yoy'].rolling(window=window).std()\n",
    "    \n",
    "    # Create economic indicator lag features\n",
    "    for i in range(1, 4):\n",
    "        if 'oil_price' in df_copy.columns:\n",
    "            df_copy[f'oil_price_lag_{i}'] = df_copy['oil_price'].shift(i)\n",
    "        if 'gold_price' in df_copy.columns:\n",
    "            df_copy[f'gold_price_lag_{i}'] = df_copy['gold_price'].shift(i)\n",
    "        if 'interest_rate' in df_copy.columns:\n",
    "            df_copy[f'interest_rate_lag_{i}'] = df_copy['interest_rate'].shift(i)\n",
    "    \n",
    "    # Add month and year as cyclical features\n",
    "    if 'month' in df_copy.columns:\n",
    "        df_copy['month_sin'] = np.sin(2 * np.pi * df_copy['month']/12)\n",
    "        df_copy['month_cos'] = np.cos(2 * np.pi * df_copy['month']/12)\n",
    "    \n",
    "    # Create interaction features\n",
    "    if all(col in df_copy.columns for col in ['oil_price', 'gold_price']):\n",
    "        df_copy['oil_gold_ratio'] = df_copy['oil_price'] / df_copy['gold_price']\n",
    "    \n",
    "    # Drop rows with NaN values (due to lag features)\n",
    "    df_clean = df_copy.dropna()\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def calculate_metrics(actual, predicted):\n",
    "    \"\"\"Calculate evaluation metrics\"\"\"\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    r2 = r2_score(actual, predicted)\n",
    "    mape = np.mean(np.abs((actual - predicted) / actual)) * 100\n",
    "    \n",
    "    return rmse, mae, mape, r2\n",
    "\n",
    "def prepare_data(df, target_col, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Prepare data for hybrid modeling\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with features and target\n",
    "    target_col : str\n",
    "        Name of the target column\n",
    "    test_size : float\n",
    "        Proportion of data to use for testing\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        Various data splits for different model components\n",
    "    \"\"\"\n",
    "    # Define features and target\n",
    "    X = df.drop([col for col in ['cpi_mom', 'cpi_yoy', target_col] if col in df.columns], axis=1)\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Split data into training and testing sets (time-based split)\n",
    "    split_test_idx = int(len(df) * (1 - test_size))\n",
    "    \n",
    "    # For time series models\n",
    "    train_ts = y[:split_test_idx]\n",
    "    test_ts = y[split_test_idx:]\n",
    "    \n",
    "    # For ML models\n",
    "    X_train = X[:split_test_idx]\n",
    "    X_test = X[split_test_idx:]\n",
    "    y_train = y[:split_test_idx]\n",
    "    y_test = y[split_test_idx:]\n",
    "    \n",
    "    # Scale features for ML models\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Convert to DataFrame for easier handling\n",
    "    X_train_scaled_df = pd.DataFrame(X_train_scaled, index=X_train.index, columns=X_train.columns)\n",
    "    X_test_scaled_df = pd.DataFrame(X_test_scaled, index=X_test.index, columns=X_test.columns)\n",
    "    \n",
    "    return train_ts, test_ts, X_train, X_test, y_train, y_test, X_train_scaled_df, X_test_scaled_df, scaler\n",
    "\n",
    "def run_arima_component(train, test, seasonal=True):\n",
    "    \"\"\"\n",
    "    Run ARIMA/SARIMA component of the hybrid model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train : pd.Series\n",
    "        Training data\n",
    "    test : pd.Series\n",
    "        Test data\n",
    "    seasonal : bool\n",
    "        Whether to include seasonal components\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        Model, predictions, residuals\n",
    "    \"\"\"\n",
    "    logger.info(f\"Running ARIMA component with seasonal={seasonal}\")\n",
    "    \n",
    "    try:\n",
    "        # Find best parameters using auto_arima\n",
    "        if seasonal:\n",
    "            model = auto_arima(\n",
    "                train,\n",
    "                start_p=0, start_q=0, max_p=3, max_q=3, max_d=2,\n",
    "                start_P=0, start_Q=0, max_P=2, max_Q=2, max_D=1,\n",
    "                m=12,  # Monthly seasonality\n",
    "                seasonal=True,\n",
    "                trace=True,\n",
    "                error_action='ignore',\n",
    "                suppress_warnings=True,\n",
    "                stepwise=True,\n",
    "                information_criterion='aic'\n",
    "            )\n",
    "        else:\n",
    "            model = auto_arima(\n",
    "                train,\n",
    "                start_p=0, start_q=0, max_p=3, max_q=3, max_d=2,\n",
    "                seasonal=False,\n",
    "                trace=True,\n",
    "                error_action='ignore',\n",
    "                suppress_warnings=True,\n",
    "                stepwise=True,\n",
    "                information_criterion='aic'\n",
    "            )\n",
    "        \n",
    "        # Print model summary\n",
    "        logger.info(f\"Best ARIMA parameters: {model.order}, seasonal_order: {model.seasonal_order if seasonal else None}\")\n",
    "        \n",
    "        # Make forecasts\n",
    "        forecast = model.predict(n_periods=len(test))\n",
    "        forecast = pd.Series(forecast, index=test.index)\n",
    "        \n",
    "        # Get residuals\n",
    "        residuals = model.resid()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        rmse, mae, mape, r2 = calculate_metrics(test, forecast)\n",
    "        logger.info(f\"ARIMA component - RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.2f}%, R²: {r2:.4f}\")\n",
    "        \n",
    "        return model, forecast, residuals\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in ARIMA component: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "def run_ml_component(X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled, model_type='XGBoost'):\n",
    "    \"\"\"\n",
    "    Run machine learning component of the hybrid model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train, X_test : pd.DataFrame\n",
    "        Training and test features\n",
    "    y_train, y_test : pd.Series\n",
    "        Training and test targets\n",
    "    X_train_scaled, X_test_scaled : pd.DataFrame\n",
    "        Scaled training and test features\n",
    "    model_type : str\n",
    "        Type of ML model to use\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        Model, predictions\n",
    "    \"\"\"\n",
    "    logger.info(f\"Running ML component with model_type={model_type}\")\n",
    "    \n",
    "    try:\n",
    "        # Select model based on type\n",
    "        if model_type == 'XGBoost':\n",
    "            model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "            model.fit(X_train, y_train)\n",
    "            forecast = model.predict(X_test)\n",
    "        elif model_type == 'RandomForest':\n",
    "            model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "            model.fit(X_train, y_train)\n",
    "            forecast = model.predict(X_test)\n",
    "        elif model_type == 'GradientBoosting':\n",
    "            model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "            model.fit(X_train, y_train)\n",
    "            forecast = model.predict(X_test)\n",
    "        elif model_type == 'Ridge':\n",
    "            model = Ridge(alpha=0.1, random_state=42)\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            forecast = model.predict(X_test_scaled)\n",
    "        else:\n",
    "            # Default to XGBoost\n",
    "            model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "            model.fit(X_train, y_train)\n",
    "            forecast = model.predict(X_test)\n",
    "        \n",
    "        # Convert to Series for easier handling\n",
    "        forecast = pd.Series(forecast, index=X_test.index)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        rmse, mae, mape, r2 = calculate_metrics(y_test, forecast)\n",
    "        logger.info(f\"ML component ({model_type}) - RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.2f}%, R²: {r2:.4f}\")\n",
    "        \n",
    "        return model, forecast\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in ML component: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def run_hybrid_model(df, target_col, test_size=0.2, arima_weight=0.5, ml_model_type='XGBoost', seasonal=True):\n",
    "    \"\"\"\n",
    "    Run hybrid model combining ARIMA and ML components\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with features and target\n",
    "    target_col : str\n",
    "        Name of the target column\n",
    "    test_size : float\n",
    "        Proportion of data to use for testing\n",
    "    arima_weight : float\n",
    "        Weight for ARIMA component in the ensemble (0-1)\n",
    "    ml_model_type : str\n",
    "        Type of ML model to use\n",
    "    seasonal : bool\n",
    "        Whether to include seasonal components in ARIMA\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        Hybrid model components, predictions, metrics\n",
    "    \"\"\"\n",
    "    logger.info(f\"Running hybrid model for {target_col} with arima_weight={arima_weight}, ml_model_type={ml_model_type}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    train_ts, test_ts, X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled, scaler = prepare_data(\n",
    "        df, target_col, test_size\n",
    "    )\n",
    "    \n",
    "    # Run ARIMA component\n",
    "    arima_model, arima_forecast, arima_residuals = run_arima_component(train_ts, test_ts, seasonal)\n",
    "    \n",
    "    # Run ML component\n",
    "    ml_model, ml_forecast = run_ml_component(\n",
    "        X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled, ml_model_type\n",
    "    )\n",
    "    \n",
    "    # Combine forecasts\n",
    "    if arima_forecast is not None and ml_forecast is not None:\n",
    "        hybrid_forecast = arima_weight * arima_forecast + (1 - arima_weight) * ml_forecast\n",
    "        \n",
    "        # Calculate metrics\n",
    "        rmse, mae, mape, r2 = calculate_metrics(test_ts, hybrid_forecast)\n",
    "        logger.info(f\"Hybrid model - RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.2f}%, R²: {r2:.4f}\")\n",
    "        \n",
    "        # Plot results\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        plt.plot(test_ts.index, test_ts, label='Actual', color='black', linewidth=2)\n",
    "        plt.plot(arima_forecast.index, arima_forecast, label='ARIMA', linestyle='--', alpha=0.7)\n",
    "        plt.plot(ml_forecast.index, ml_forecast, label=f'ML ({ml_model_type})', linestyle='--', alpha=0.7)\n",
    "        plt.plot(hybrid_forecast.index, hybrid_forecast, label='Hybrid', color='red', linewidth=2)\n",
    "        plt.title(f'Hybrid Model: Actual vs Predicted {target_col}')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel(target_col)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Create plots directory if it doesn't exist\n",
    "        if not os.path.exists('plots'):\n",
    "            os.makedirs('plots')\n",
    "        \n",
    "        plt.savefig(f'plots/hybrid_predictions_{target_col}.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Plot error distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        errors = test_ts - hybrid_forecast\n",
    "        sns.histplot(errors, kde=True)\n",
    "        plt.title(f'Hybrid Model: Error Distribution for {target_col}')\n",
    "        plt.xlabel('Error')\n",
    "        plt.savefig(f'plots/hybrid_error_distribution_{target_col}.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Create a DataFrame with predictions\n",
    "        predictions = pd.DataFrame({\n",
    "            'Actual': test_ts,\n",
    "            'ARIMA': arima_forecast,\n",
    "            f'ML ({ml_model_type})': ml_forecast,\n",
    "            'Hybrid': hybrid_forecast,\n",
    "            'Error': test_ts - hybrid_forecast\n",
    "        })\n",
    "        \n",
    "        # Return results\n",
    "        metrics = {\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'mape': mape,\n",
    "            'r2': r2\n",
    "        }\n",
    "        \n",
    "        components = {\n",
    "            'arima_model': arima_model,\n",
    "            'ml_model': ml_model,\n",
    "            'arima_weight': arima_weight,\n",
    "            'ml_model_type': ml_model_type,\n",
    "            'scaler': scaler\n",
    "        }\n",
    "        \n",
    "        return components, predictions, metrics\n",
    "    \n",
    "    else:\n",
    "        logger.error(\"One or both model components failed\")\n",
    "        return None, None, None\n",
    "\n",
    "def forecast_future_hybrid(components, df, target_col, forecast_horizon=24):\n",
    "    \"\"\"\n",
    "    Generate future forecasts using the hybrid model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    components : dict\n",
    "        Dictionary of hybrid model components\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with features\n",
    "    target_col : str\n",
    "        Name of the target column\n",
    "    forecast_horizon : int\n",
    "        Number of periods to forecast\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        Forecasted values\n",
    "    \"\"\"\n",
    "    logger.info(f\"Generating {forecast_horizon} period forecast\")\n",
    "    \n",
    "    try:\n",
    "        arima_model = components['arima_model']\n",
    "        ml_model = components['ml_model']\n",
    "        arima_weight = components['arima_weight']\n",
    "        ml_model_type = components['ml_model_type']\n",
    "        scaler = components['scaler']\n",
    "        \n",
    "        # Generate ARIMA forecasts\n",
    "        arima_forecast = arima_model.predict(n_periods=forecast_horizon)\n",
    "        \n",
    "        # Get the last date in the dataframe\n",
    "        last_date = df.index[-1]\n",
    "        future_dates = pd.date_range(start=last_date + pd.DateOffset(months=1), periods=forecast_horizon, freq='MS')\n",
    "        \n",
    "        # Create a Series with the ARIMA forecasts\n",
    "        arima_forecast = pd.Series(arima_forecast, index=future_dates)\n",
    "        \n",
    "        # Generate ML forecasts (simplified approach)\n",
    "        # For a real implementation, you would need to generate future features\n",
    "        # Here we'll use a naive approach by using the last available features\n",
    "        X_last = df.drop([col for col in ['cpi_mom', 'cpi_yoy', target_col] if col in df.columns], axis=1).iloc[-1:].copy()\n",
    "        ml_forecasts = []\n",
    "        \n",
    "        for i in range(forecast_horizon):\n",
    "            # For tree-based models\n",
    "            if ml_model_type in ['XGBoost', 'RandomForest', 'GradientBoosting']:\n",
    "                pred = ml_model.predict(X_last)[0]\n",
    "            # For linear models\n",
    "            else:\n",
    "                X_last_scaled = scaler.transform(X_last)\n",
    "                pred = ml_model.predict(X_last_scaled)[0]\n",
    "            \n",
    "            ml_forecasts.append(pred)\n",
    "        \n",
    "        # Create a Series with the ML forecasts\n",
    "        ml_forecast = pd.Series(ml_forecasts, index=future_dates)\n",
    "        \n",
    "        # Combine forecasts\n",
    "        hybrid_forecast = arima_weight * arima_forecast + (1 - arima_weight) * ml_forecast\n",
    "        \n",
    "        # Plot future forecasts\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        plt.plot(df[target_col].index, df[target_col], label='Historical Data')\n",
    "        plt.plot(arima_forecast.index, arima_forecast, label='ARIMA Forecast', linestyle='--', alpha=0.7)\n",
    "        plt.plot(ml_forecast.index, ml_forecast, label=f'ML ({ml_model_type}) Forecast', linestyle='--', alpha=0.7)\n",
    "        plt.plot(hybrid_forecast.index, hybrid_forecast, label='Hybrid Forecast', color='red', linewidth=2)\n",
    "        plt.title(f'Hybrid Model: {target_col} Future Forecast')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel(target_col)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/hybrid_future_forecast_{target_col}.png')\n",
    "        plt.close()\n",
    "        \n",
    "        return hybrid_forecast, arima_forecast, ml_forecast\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in future forecast: {e}\")\n",
    "        import traceback\n",
    "        logger.error(traceback.format_exc())\n",
    "        return None, None, None\n",
    "\n",
    "def optimize_hybrid_weights(df, target_col, test_size=0.2, ml_model_type='XGBoost', seasonal=True):\n",
    "    \"\"\"\n",
    "    Optimize the weights for the hybrid model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with features and target\n",
    "    target_col : str\n",
    "        Name of the target column\n",
    "    test_size : float\n",
    "        Proportion of data to use for testing\n",
    "    ml_model_type : str\n",
    "        Type of ML model to use\n",
    "    seasonal : bool\n",
    "        Whether to include seasonal components in ARIMA\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Optimal weight for ARIMA component\n",
    "    \"\"\"\n",
    "    logger.info(f\"Optimizing hybrid weights for {target_col}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    train_ts, test_ts, X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled, scaler = prepare_data(\n",
    "        df, target_col, test_size\n",
    "    )\n",
    "    \n",
    "    # Run ARIMA component\n",
    "    arima_model, arima_forecast, arima_residuals = run_arima_component(train_ts, test_ts, seasonal)\n",
    "    \n",
    "    # Run ML component\n",
    "    ml_model, ml_forecast = run_ml_component(\n",
    "        X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled, ml_model_type\n",
    "    )\n",
    "    \n",
    "    # Try different weights\n",
    "    weights = np.linspace(0, 1, 21)  # 0.0, 0.05, 0.1, ..., 0.95, 1.0\n",
    "    best_rmse = float('inf')\n",
    "    best_weight = 0.5  # Default\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for weight in weights:\n",
    "        hybrid_forecast = weight * arima_forecast + (1 - weight) * ml_forecast\n",
    "        rmse, mae, mape, r2 = calculate_metrics(test_ts, hybrid_forecast)\n",
    "        results.append((weight, rmse, mae, mape, r2))\n",
    "        \n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_weight = weight\n",
    "    \n",
    "    logger.info(f\"Best weight for ARIMA component: {best_weight:.2f} with RMSE: {best_rmse:.4f}\")\n",
    "    \n",
    "    # Plot weight optimization results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    weights_df = pd.DataFrame(results, columns=['Weight', 'RMSE', 'MAE', 'MAPE', 'R2'])\n",
    "    plt.plot(weights_df['Weight'], weights_df['RMSE'], marker='o')\n",
    "    plt.axvline(x=best_weight, color='r', linestyle='--', label=f'Best Weight: {best_weight:.2f}')\n",
    "    plt.title(f'Hybrid Model Weight Optimization for {target_col}')\n",
    "    plt.xlabel('ARIMA Weight')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/hybrid_weight_optimization_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return best_weight\n",
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    file_path = 'data/analyzed_time_series.csv'\n",
    "    df = load_data(file_path)\n",
    "    \n",
    "    # Engineer features\n",
    "    df_engineered = engineer_features(df)\n",
    "    \n",
    "    # Run hybrid model for CPI Year-over-Year\n",
    "    target_col = 'cpi_yoy'\n",
    "    \n",
    "    # Optimize weights\n",
    "    optimal_weight = optimize_hybrid_weights(df_engineered, target_col, test_size=0.2, ml_model_type='XGBoost')\n",
    "    \n",
    "    # Run hybrid model with optimal weight\n",
    "    components_yoy, predictions_yoy, metrics_yoy = run_hybrid_model(\n",
    "        df_engineered, target_col, test_size=0.2, arima_weight=optimal_weight, ml_model_type='XGBoost'\n",
    "    )\n",
    "    \n",
    "    # Generate future forecasts for YoY\n",
    "    if components_yoy is not None:\n",
    "        forecast_yoy, arima_forecast_yoy, ml_forecast_yoy = forecast_future_hybrid(\n",
    "            components_yoy, df_engineered, target_col, forecast_horizon=24\n",
    "        )\n",
    "        logger.info(f\"\\nFuture {target_col} forecasts:\")\n",
    "        logger.info(forecast_yoy)\n",
    "    \n",
    "    # Run hybrid model for CPI Month-over-Month\n",
    "    target_col = 'cpi_mom'\n",
    "    \n",
    "    # Optimize weights\n",
    "    optimal_weight = optimize_hybrid_weights(df_engineered, target_col, test_size=0.2, ml_model_type='XGBoost')\n",
    "    \n",
    "    # Run hybrid model with optimal weight\n",
    "    components_mom, predictions_mom, metrics_mom = run_hybrid_model(\n",
    "        df_engineered, target_col, test_size=0.2, arima_weight=optimal_weight, ml_model_type='XGBoost'\n",
    "    )\n",
    "    \n",
    "    # Generate future forecasts for MoM\n",
    "    if components_mom is not None:\n",
    "        forecast_mom, arima_forecast_mom, ml_forecast_mom = forecast_future_hybrid(\n",
    "            components_mom, df_engineered, target_col, forecast_horizon=24\n",
    "        )\n",
    "        logger.info(f\"\\nFuture {target_col} forecasts:\")\n",
    "        logger.info(forecast_mom)\n",
    "    \n",
    "    # Compare with other models (if available)\n",
    "    try:\n",
    "        # Try to load metrics from other models\n",
    "        all_metrics = pd.read_csv('plots/final_model_comparison_metrics.csv')\n",
    "        \n",
    "        # Create hybrid metrics dataframe\n",
    "        hybrid_metrics = pd.DataFrame([\n",
    "            {\n",
    "                'Model': 'Hybrid', 'Target': 'CPI MoM',\n",
    "                'Train_RMSE': None, 'Train_MAE': None, \n",
    "                'Train_MAPE': None, 'Train_R2': None,\n",
    "                'Test_RMSE': metrics_mom['rmse'], 'Test_MAE': metrics_mom['mae'], \n",
    "                'Test_MAPE': metrics_mom['mape'], 'Test_R2': metrics_mom['r2']\n",
    "            },\n",
    "            {\n",
    "                'Model': 'Hybrid', 'Target': 'CPI YoY',\n",
    "                'Train_RMSE': None, 'Train_MAE': None, \n",
    "                'Train_MAPE': None, 'Train_R2': None,\n",
    "                'Test_RMSE': metrics_yoy['rmse'], 'Test_MAE': metrics_yoy['mae'], \n",
    "                'Test_MAPE': metrics_yoy['mape'], 'Test_R2': metrics_yoy['r2']\n",
    "            }\n",
    "        ])\n",
    "        \n",
    "        # Combine metrics\n",
    "        combined_metrics = pd.concat([all_metrics, hybrid_metrics])\n",
    "        combined_metrics.to_csv('plots/final_model_comparison_metrics.csv', index=False)\n",
    "        logger.info(\"\\nComparison with all models saved to plots/final_model_comparison_metrics.csv\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not compare with other models: {e}\")\n",
    "        # Save metrics to CSV\n",
    "        metrics_data = [\n",
    "            {\n",
    "                'Model': 'Hybrid', 'Target': 'CPI MoM',\n",
    "                'Test_RMSE': metrics_mom['rmse'], 'Test_MAE': metrics_mom['mae'], \n",
    "                'Test_MAPE': metrics_mom['mape'], 'Test_R2': metrics_mom['r2']\n",
    "            },\n",
    "            {\n",
    "                'Model': 'Hybrid', 'Target': 'CPI YoY',\n",
    "                'Test_RMSE': metrics_yoy['rmse'], 'Test_MAE': metrics_yoy['mae'], \n",
    "                'Test_MAPE': metrics_yoy['mape'], 'Test_R2': metrics_yoy['r2']\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        metrics_df = pd.DataFrame(metrics_data)\n",
    "        metrics_df.to_csv('plots/hybrid_metrics.csv', index=False)\n",
    "        logger.info(\"\\nMetrics saved to plots/hybrid_metrics.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
