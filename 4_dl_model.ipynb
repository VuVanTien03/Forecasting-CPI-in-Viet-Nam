{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b305ee4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000248FA965CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000248F660C670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Model Results:\n",
      "  Target        Model  Train RMSE  Val RMSE  Test RMSE  Test MAE  Test MAPE  \\\n",
      "0    cpi         LSTM    0.488614  0.292663   0.161518  0.124461   0.124329   \n",
      "1    cpi          GRU    0.534295  0.306378   0.201700  0.157457   0.157295   \n",
      "2    cpi     CNN-LSTM    0.488801  0.286947   0.131971  0.111849   0.111583   \n",
      "3    cpi  Transformer    0.516502  0.289119   0.154192  0.134223   0.133980   \n",
      "4    cpi          TCN    0.712442  0.319567   0.147439  0.132742   0.132483   \n",
      "\n",
      "   Test sMAPE  Test NormMAPE  Test DirAcc  \n",
      "0    0.124204       0.001241        100.0  \n",
      "1    0.157104       0.001570         62.5  \n",
      "2    0.111622       0.001114        100.0  \n",
      "3    0.133940       0.001337         37.5  \n",
      "4    0.132465       0.001322         62.5  \n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Dropout, Flatten, GRU, LSTM, MultiHeadAttention\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Constants\n",
    "CONFIG = {\n",
    "    'forecast_horizon': 12,\n",
    "    'seasonal_periods': 12,\n",
    "    'min_data_length': 50,\n",
    "    'img_dir': 'dl_model_results',\n",
    "    'results_file': 'dl_model_results/deep_learning_model_results.csv',\n",
    "    'lags': list(range(1, 13)),\n",
    "    'rolling_windows': [3, 6, 12],\n",
    "    'timesteps': 3,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 100,\n",
    "    'early_stopping_patience': 10,\n",
    "    'reduce_lr_patience': 5,\n",
    "    'reduce_lr_factor': 0.2,\n",
    "    'learning_rate': 0.005,\n",
    "    'k_folds': 5,  # Added for K-fold cross-validation\n",
    "    'model_params': {\n",
    "        'LSTM': {'units': 16, 'type': 'lstm'},  # Increased units for better capacity\n",
    "        'GRU': {'units': 16, 'type': 'gru'},\n",
    "        'CNN-LSTM': {'filters': 16, 'kernel_size': 3, 'units': 32, 'type': 'cnn_lstm'},\n",
    "        'Transformer': {'num_heads': 4, 'key_dim': 16, 'dense_units': 32, 'type': 'transformer'},\n",
    "        'TCN': {'filters': 16, 'kernel_size': 3, 'type': 'tcn'}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Logging setup\n",
    "os.makedirs(CONFIG['img_dir'], exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    filename=os.path.join(CONFIG['img_dir'], 'dl_models_log.txt'),\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def validate_data(data: pd.DataFrame, required_columns: List[str] = ['cpi', 'time', 'oil_price', 'gold_price']) -> None:\n",
    "    \"\"\"Validate input data for required columns, length, and data types.\"\"\"\n",
    "    logger.info(\"Validating input data\")\n",
    "    missing_cols = [col for col in required_columns if col not in data.columns]\n",
    "    if missing_cols:\n",
    "        logger.error(f\"Missing columns: {missing_cols}\")\n",
    "        raise ValueError(f\"Missing columns: {missing_cols}\")\n",
    "\n",
    "    if len(data) < CONFIG['min_data_length'] + CONFIG['forecast_horizon'] * 2:\n",
    "        logger.error(f\"Data too short: {len(data)} rows\")\n",
    "        raise ValueError(f\"Data too short: {len(data)} rows\")\n",
    "\n",
    "    numeric_cols = [col for col in required_columns if col != 'time']\n",
    "    for col in numeric_cols:\n",
    "        if not pd.api.types.is_numeric_dtype(data[col]):\n",
    "            logger.error(f\"Column {col} is not numeric: {data[col].dtype}\")\n",
    "            raise ValueError(f\"Column {col} is not numeric: {data[col].dtype}\")\n",
    "        data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "\n",
    "    if data[numeric_cols].isna().any().any():\n",
    "        logger.error(f\"Data contains NaN: {data[numeric_cols].isna().sum().to_dict()}\")\n",
    "        raise ValueError(\"Data contains NaN\")\n",
    "    if np.isinf(data[numeric_cols]).any().any():\n",
    "        logger.error(f\"Data contains Inf: {np.isinf(data[numeric_cols]).sum().to_dict()}\")\n",
    "        raise ValueError(\"Data contains Inf\")\n",
    "\n",
    "    logger.info(f\"Data valid: {len(data)} rows, columns: {list(data.columns)}\")\n",
    "\n",
    "def create_features(\n",
    "    data: pd.DataFrame,\n",
    "    target: str,\n",
    "    lags: List[int] = CONFIG['lags'],\n",
    "    rolling_windows: List[int] = CONFIG['rolling_windows'],\n",
    "    trend_features: bool = True,\n",
    "    seasonal_features: bool = True,\n",
    "    fill_method: str = 'interpolate',\n",
    "    correlation_threshold: float = 0.2,\n",
    "    use_pca: bool = False,\n",
    "    pca_variance_ratio: float = 0.95\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Generate features for time series data.\"\"\"\n",
    "    logger.info(f\"Generating features for {target}\")\n",
    "    if not isinstance(data.index, pd.DatetimeIndex):\n",
    "        logger.error(\"Index must be DatetimeIndex\")\n",
    "        raise ValueError(\"Index must be DatetimeIndex\")\n",
    "\n",
    "    df = data.copy()\n",
    "    exog_vars = ['oil_price', 'gold_price']\n",
    "    required_cols = [target] + exog_vars\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        logger.error(f\"Missing required columns: {required_cols}\")\n",
    "        raise ValueError(f\"Missing required columns: {required_cols}\")\n",
    "\n",
    "    for col in required_cols:\n",
    "        for lag in lags:\n",
    "            df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n",
    "        for window in rolling_windows:\n",
    "            df[f'{col}_roll_mean_{window}'] = df[col].rolling(window=window).mean()\n",
    "            df[f'{col}_roll_std_{window}'] = df[col].rolling(window=window).std()\n",
    "\n",
    "    if trend_features:\n",
    "        df[f'{target}_diff_1'] = df[target].diff()\n",
    "        df[f'{target}_trend'] = np.arange(len(df))\n",
    "\n",
    "    if seasonal_features:\n",
    "        period = CONFIG['seasonal_periods']\n",
    "        df['month'] = df.index.month\n",
    "        df = pd.get_dummies(df, columns=['month'], prefix='month')\n",
    "        df['month_sin'] = np.sin(2 * np.pi * df.index.month / period)\n",
    "        df['month_cos'] = np.cos(2 * np.pi * df.index.month / period)\n",
    "\n",
    "    df['quarter'] = df.index.quarter\n",
    "    df['year'] = df.index.year\n",
    "\n",
    "    fill_methods = {\n",
    "        'interpolate': lambda x: x.interpolate(method='linear', limit_direction='both'),\n",
    "        'rolling_mean': lambda x: x.fillna(x.rolling(window=3, min_periods=1).mean()),\n",
    "        'ffill': lambda x: x.ffill(),\n",
    "        'bfill': lambda x: x.bfill()\n",
    "    }\n",
    "    for col in df.columns:\n",
    "        if df[col].isna().any():\n",
    "            df[col] = fill_methods.get(fill_method, fill_methods['interpolate'])(df[col])\n",
    "            df[col] = df[col].fillna(df[col].mean())\n",
    "\n",
    "    if df.isna().any().any():\n",
    "        logger.error(f\"Data still contains NaN: {df.isna().sum().to_dict()}\")\n",
    "        raise ValueError(\"Data still contains NaN\")\n",
    "\n",
    "    if correlation_threshold > 0:\n",
    "        correlations = df.corr()[target].drop(required_cols)\n",
    "        low_corr_cols = correlations[abs(correlations) < correlation_threshold].index\n",
    "        df.drop(columns=low_corr_cols, inplace=True)\n",
    "\n",
    "    if use_pca and len(df.columns) > len(required_cols):\n",
    "        feature_cols = [col for col in df.columns if col not in required_cols]\n",
    "        if feature_cols:\n",
    "            pca = PCA(n_components=pca_variance_ratio, svd_solver='full')\n",
    "            pca_features = pca.fit_transform(df[feature_cols])\n",
    "            pca_cols = [f'pca_{i+1}' for i in range(pca.n_components_)]\n",
    "            df = pd.concat([df[required_cols], pd.DataFrame(pca_features, index=df.index, columns=pca_cols)], axis=1)\n",
    "\n",
    "    logger.info(f\"Processed features: {list(df.columns)}\")\n",
    "    return df\n",
    "\n",
    "def calculate_metrics(actual: np.ndarray, predicted: np.ndarray) -> Tuple[float, ...]:\n",
    "    \"\"\"Calculate evaluation metrics.\"\"\"\n",
    "    actual, predicted = np.array(actual, dtype=float), np.array(predicted, dtype=float)\n",
    "    valid_mask = ~(np.isnan(actual) | np.isnan(predicted) | np.isinf(actual) | np.isinf(predicted))\n",
    "    actual, predicted = actual[valid_mask], predicted[valid_mask]\n",
    "\n",
    "    if len(actual) == 0:\n",
    "        logger.warning(\"No valid data for metrics calculation\")\n",
    "        return tuple([np.nan] * 6)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    mape = mean_absolute_percentage_error(actual, predicted) * 100 if np.all(np.abs(actual) > 1e-8) else np.nan\n",
    "    smape = 100 * np.mean(2 * np.abs(predicted - actual) / (np.abs(actual) + np.abs(predicted)))\n",
    "    norm_mape = mape / np.mean(np.abs(actual)) if not np.isnan(mape) else np.nan\n",
    "    directional_acc = np.mean((np.diff(actual) * np.diff(predicted)) > 0) * 100 if len(actual) > 1 else np.nan\n",
    "\n",
    "    return rmse, mae, mape, smape, norm_mape, directional_acc\n",
    "\n",
    "def plot_forecast(\n",
    "    historical: pd.Series,\n",
    "    val: pd.Series,\n",
    "    test: pd.Series,\n",
    "    forecast: pd.Series,\n",
    "    forecast_index: pd.DatetimeIndex,\n",
    "    title: str,\n",
    "    ylabel: str,\n",
    "    filename: str\n",
    ") -> None:\n",
    "    \"\"\"Plot forecast results.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(historical.index, historical, label='Historical', color='blue')\n",
    "    plt.plot(val.index, val, label='Validation', color='purple')\n",
    "    plt.plot(test.index, test, label='Actual', color='green')\n",
    "    plt.plot(forecast_index[:len(forecast)], forecast, label='Forecast', color='orange', linestyle='--')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CONFIG['img_dir'], filename))\n",
    "    plt.close()\n",
    "    logger.info(f\"Saved plot: {filename}\")\n",
    "\n",
    "def plot_comparison_forecasts(\n",
    "    historical: pd.Series,\n",
    "    val: pd.Series,\n",
    "    test: pd.Series,\n",
    "    forecasts: Dict[str, pd.Series],\n",
    "    forecast_index: pd.DatetimeIndex,\n",
    "    title: str,\n",
    "    ylabel: str,\n",
    "    filename: str,\n",
    "    metrics: Dict[str, Dict[str, float]]\n",
    ") -> None:\n",
    "    \"\"\"Plot comparison of forecasts from multiple models.\"\"\"\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.plot(historical.index, historical, label='Historical', color='blue')\n",
    "    plt.plot(val.index, val, label='Validation', color='purple')\n",
    "    plt.plot(test.index, test, label='Actual', color='green')\n",
    "    colors = sns.color_palette(\"husl\", len(forecasts))\n",
    "    for (model_name, forecast), color in zip(forecasts.items(), colors):\n",
    "        rmse = metrics.get(model_name, {}).get('Test RMSE', np.nan)\n",
    "        if pd.isna(rmse) or forecast is None:\n",
    "            continue\n",
    "        plt.plot(forecast_index[:len(forecast)], forecast, label=f'{model_name} (RMSE: {rmse:.4f})',\n",
    "                 linestyle='--', color=color)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CONFIG['img_dir'], filename))\n",
    "    plt.close()\n",
    "    logger.info(f\"Saved comparison plot: {filename}\")\n",
    "\n",
    "def prepare_data_for_dl(\n",
    "    train: pd.DataFrame,\n",
    "    val: pd.DataFrame,\n",
    "    test: pd.DataFrame,\n",
    "    target: str,\n",
    "    feature_cols: List[str],\n",
    "    timesteps: int = CONFIG['timesteps']\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, RobustScaler]:\n",
    "    \"\"\"Prepare data for deep learning models.\"\"\"\n",
    "    scaler = RobustScaler()\n",
    "    X_train = scaler.fit_transform(train[feature_cols])\n",
    "    X_val = scaler.transform(val[feature_cols])\n",
    "    X_test = scaler.transform(test[feature_cols])\n",
    "    y_train = scaler.fit_transform(train[[target]]).flatten()\n",
    "    y_val = scaler.transform(val[[target]]).flatten()\n",
    "    y_test = scaler.transform(test[[target]]).flatten()\n",
    "\n",
    "    def create_sequences(X: np.ndarray, y: np.ndarray, timesteps: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        X_seq, y_seq = [], []\n",
    "        for i in range(len(X) - timesteps):\n",
    "            X_seq.append(X[i:i + timesteps])\n",
    "            y_seq.append(y[i + timesteps])\n",
    "        return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train, y_train, timesteps)\n",
    "    X_val_seq, y_val_seq = create_sequences(X_val, y_val, timesteps)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test, y_test, timesteps)\n",
    "\n",
    "    logger.info(f\"Data shapes: Train={X_train_seq.shape}, Val={X_val_seq.shape}, Test={X_test_seq.shape}\")\n",
    "    return X_train_seq, y_train_seq, X_val_seq, y_val_seq, X_test_seq, y_test_seq, scaler\n",
    "\n",
    "def build_model(model_type: str, input_shape: Tuple[int, int], params: Dict[str, any]) -> Model:\n",
    "    \"\"\"Build a deep learning model based on type with enhanced architecture.\"\"\"\n",
    "    if model_type == 'transformer':\n",
    "        inputs = tf.keras.Input(shape=input_shape)\n",
    "        x = MultiHeadAttention(num_heads=params['num_heads'], key_dim=params['key_dim'],\n",
    "                               kernel_regularizer=l2(0.01))(inputs, inputs)\n",
    "        x = Dropout(0.4)(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(params['dense_units'], activation='relu')(x)\n",
    "        outputs = Dense(1)(x)\n",
    "        return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "    model = Sequential()\n",
    "    if model_type == 'lstm':\n",
    "        model.add(LSTM(params['units'], input_shape=input_shape, return_sequences=True, kernel_regularizer=l2(0.01)))\n",
    "        model.add(LSTM(params['units'] // 2, kernel_regularizer=l2(0.01)))\n",
    "    elif model_type == 'gru':\n",
    "        model.add(GRU(params['units'], input_shape=input_shape, return_sequences=True, kernel_regularizer=l2(0.01)))\n",
    "        model.add(GRU(params['units'] // 2, kernel_regularizer=l2(0.01)))\n",
    "    elif model_type == 'cnn_lstm':\n",
    "        model.add(Conv1D(params['filters'], kernel_size=params['kernel_size'], activation='relu',\n",
    "                         input_shape=input_shape, padding='same', kernel_regularizer=l2(0.01)))\n",
    "        model.add(tf.keras.layers.MaxPooling1D(2))\n",
    "        model.add(LSTM(params['units'], kernel_regularizer=l2(0.01)))\n",
    "    elif model_type == 'tcn':\n",
    "        model.add(Conv1D(params['filters'], kernel_size=params['kernel_size'], activation='relu',\n",
    "                         input_shape=input_shape, padding='causal', kernel_regularizer=l2(0.01)))\n",
    "        model.add(Conv1D(params['filters'] // 2, kernel_size=params['kernel_size'], activation='relu',\n",
    "                         padding='causal', kernel_regularizer=l2(0.01)))\n",
    "        model.add(Flatten())\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    return model\n",
    "\n",
    "def run_model(\n",
    "    train: pd.DataFrame,\n",
    "    val: pd.DataFrame,\n",
    "    test: pd.DataFrame,\n",
    "    forecast_index: pd.DatetimeIndex,\n",
    "    target: str,\n",
    "    feature_cols: List[str],\n",
    "    model_name: str,\n",
    "    params: Dict[str, any]\n",
    ") -> Optional[Dict[str, any]]:\n",
    "    \"\"\"Run a deep learning model with K-fold cross-validation and ensemble forecasting.\"\"\"\n",
    "    start_time = datetime.now()\n",
    "    try:\n",
    "        X_train_seq, y_train_seq, X_val_seq, y_val_seq, X_test_seq, y_test_seq, scaler = prepare_data_for_dl(\n",
    "            train, val, test, target, feature_cols\n",
    "        )\n",
    "\n",
    "        # K-fold cross-validation\n",
    "        tscv = TimeSeriesSplit(n_splits=CONFIG['k_folds'])\n",
    "        val_rmses = []\n",
    "        for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train_seq)):\n",
    "            X_fold_train, y_fold_train = X_train_seq[train_idx], y_train_seq[train_idx]\n",
    "            X_fold_val, y_fold_val = X_train_seq[val_idx], y_train_seq[val_idx]\n",
    "\n",
    "            model = build_model(params['type'], (CONFIG['timesteps'], X_train_seq.shape[2]), params)\n",
    "            model.compile(optimizer=Adam(learning_rate=CONFIG['learning_rate']), loss='mse')\n",
    "\n",
    "            callbacks = [\n",
    "                EarlyStopping(patience=CONFIG['early_stopping_patience'], restore_best_weights=True, monitor='val_loss'),\n",
    "                ReduceLROnPlateau(factor=CONFIG['reduce_lr_factor'], patience=CONFIG['reduce_lr_patience'],\n",
    "                                 monitor='val_loss', min_lr=1e-6)\n",
    "            ]\n",
    "\n",
    "            model.fit(\n",
    "                X_fold_train, y_fold_train,\n",
    "                validation_data=(X_fold_val, y_fold_val),\n",
    "                epochs=CONFIG['epochs'],\n",
    "                batch_size=CONFIG['batch_size'],\n",
    "                callbacks=callbacks,\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "            val_pred = scaler.inverse_transform(model.predict(X_fold_val, verbose=0)).flatten()\n",
    "            val_true = scaler.inverse_transform(y_fold_val.reshape(-1, 1)).flatten()\n",
    "            val_rmse = np.sqrt(mean_squared_error(val_true, val_pred))\n",
    "            val_rmses.append(val_rmse)\n",
    "            logger.info(f\"{model_name} ({target}) Fold {fold+1}: Val RMSE={val_rmse:.4f}\")\n",
    "\n",
    "        avg_val_rmse = np.mean(val_rmses)\n",
    "        logger.info(f\"{model_name} ({target}) Average Val RMSE across {CONFIG['k_folds']} folds: {avg_val_rmse:.4f}\")\n",
    "\n",
    "        # Train final model on full training data\n",
    "        model = build_model(params['type'], (CONFIG['timesteps'], X_train_seq.shape[2]), params)\n",
    "        model.compile(optimizer=Adam(learning_rate=CONFIG['learning_rate']), loss='mse')\n",
    "        model.fit(\n",
    "            X_train_seq, y_train_seq,\n",
    "            validation_data=(X_val_seq, y_val_seq),\n",
    "            epochs=CONFIG['epochs'],\n",
    "            batch_size=CONFIG['batch_size'],\n",
    "            callbacks=callbacks,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # Ensemble forecasting (average predictions over multiple runs)\n",
    "        n_ensemble = 3\n",
    "        forecasts_scaled = []\n",
    "        for _ in range(n_ensemble):\n",
    "            input_seq = X_test_seq[-1].copy()\n",
    "            forecast_scaled = []\n",
    "            for i in range(CONFIG['forecast_horizon']):\n",
    "                pred = model.predict(input_seq.reshape(1, CONFIG['timesteps'], X_train_seq.shape[2]), verbose=0)\n",
    "                forecast_scaled.append(pred[0, 0])\n",
    "                input_seq = np.roll(input_seq, -1, axis=0)\n",
    "                input_seq[-1, feature_cols.index(f'{target}_lag_1')] = y_test_seq[i] if i < len(y_test_seq) else pred[0, 0]\n",
    "            forecasts_scaled.append(forecast_scaled)\n",
    "        \n",
    "        forecast_scaled = np.mean(forecasts_scaled, axis=0)\n",
    "        forecast = scaler.inverse_transform(np.array(forecast_scaled).reshape(-1, 1)).flatten()\n",
    "        forecast = pd.Series(forecast, index=forecast_index)\n",
    "\n",
    "        train_pred = scaler.inverse_transform(model.predict(X_train_seq, verbose=0)).flatten()\n",
    "        val_pred = scaler.inverse_transform(model.predict(X_val_seq, verbose=0)).flatten()\n",
    "        train_metrics = calculate_metrics(scaler.inverse_transform(y_train_seq.reshape(-1, 1)).flatten(), train_pred)\n",
    "        val_metrics = calculate_metrics(scaler.inverse_transform(y_val_seq.reshape(-1, 1)).flatten(), val_pred)\n",
    "        test_metrics = calculate_metrics(scaler.inverse_transform(y_test_seq.reshape(-1, 1)).flatten(),\n",
    "                                        forecast[:len(y_test_seq)])\n",
    "\n",
    "        plot_forecast(\n",
    "            train[target][-48:], val[target], test[target][:len(forecast)],\n",
    "            forecast, forecast_index, f'{model_name} Forecast for {target}',\n",
    "            target, f'{target}_{model_name.lower().replace(\"-\", \"_\")}_forecast.png'\n",
    "        )\n",
    "\n",
    "        model.save(os.path.join(CONFIG['img_dir'], f'{model_name.lower().replace(\"-\", \"_\")}_{target}.h5'))\n",
    "\n",
    "        elapsed_time = (datetime.now() - start_time).total_seconds()\n",
    "        logger.info(f\"{model_name} ({target}): Train RMSE={train_metrics[0]:.4f}, \"\n",
    "                    f\"Val RMSE={val_metrics[0]:.4f}, Test RMSE={test_metrics[0]:.4f}, Time={elapsed_time:.2f}s\")\n",
    "\n",
    "        return {\n",
    "            'forecast': forecast,\n",
    "            'residuals': scaler.inverse_transform(y_train_seq.reshape(-1, 1)).flatten() - train_pred,\n",
    "            'train_rmse': train_metrics[0],\n",
    "            'val_rmse': val_metrics[0],\n",
    "            'test_rmse': test_metrics[0],\n",
    "            'test_mae': test_metrics[1],\n",
    "            'test_mape': test_metrics[2],\n",
    "            'test_smape': test_metrics[3],\n",
    "            'test_norm_mape': test_metrics[4],\n",
    "            'test_dir_acc': test_metrics[5]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in {model_name} ({target}): {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"Main program to run deep learning models for time series forecasting.\"\"\"\n",
    "    tf.keras.utils.set_random_seed(42)\n",
    "    try:\n",
    "        data = pd.read_csv('data/data.csv')\n",
    "        validate_data(data)\n",
    "\n",
    "        data['time'] = pd.to_datetime(data['time'])\n",
    "        data.set_index('time', inplace=True)\n",
    "\n",
    "        features_cpi = create_features(data, 'cpi', trend_features=False)\n",
    "        train_cpi = features_cpi[:-36]\n",
    "        val_cpi = features_cpi[-36:-12]\n",
    "        test_cpi = features_cpi[-12:]\n",
    "\n",
    "        forecast_index = pd.date_range(start=test_cpi.index[0], periods=CONFIG['forecast_horizon'], freq='MS')\n",
    "        feature_cols_cpi = [col for col in train_cpi.columns if col != 'cpi']\n",
    "\n",
    "        results, forecasts_cpi, metrics_cpi = [], {}, {}\n",
    "        for model_name, params in CONFIG['model_params'].items():\n",
    "            result = run_model(\n",
    "                train_cpi, val_cpi, test_cpi, forecast_index, 'cpi',\n",
    "                feature_cols_cpi, model_name, params\n",
    "            )\n",
    "            if result:\n",
    "                results.append({\n",
    "                    'Target': 'cpi',\n",
    "                    'Model': model_name,\n",
    "                    'Train RMSE': result['train_rmse'],\n",
    "                    'Val RMSE': result['val_rmse'],\n",
    "                    'Test RMSE': result['test_rmse'],\n",
    "                    'Test MAE': result['test_mae'],\n",
    "                    'Test MAPE': result['test_mape'],\n",
    "                    'Test sMAPE': result['test_smape'],\n",
    "                    'Test NormMAPE': result['test_norm_mape'],\n",
    "                    'Test DirAcc': result['test_dir_acc']\n",
    "                })\n",
    "                forecasts_cpi[model_name] = result['forecast']\n",
    "                metrics_cpi[model_name] = {'Test RMSE': result['test_rmse']}\n",
    "\n",
    "        if forecasts_cpi:\n",
    "            # Ensemble forecast by averaging\n",
    "            valid_forecasts = [f for f in forecasts_cpi.values() if f is not None]\n",
    "            if valid_forecasts:\n",
    "                ensemble_forecast = pd.concat(valid_forecasts, axis=1).mean(axis=1)\n",
    "                ensemble_forecast = pd.Series(ensemble_forecast, index=forecast_index)\n",
    "                forecasts_cpi['Ensemble'] = ensemble_forecast\n",
    "                metrics_cpi['Ensemble'] = {\n",
    "                    'Test RMSE': np.sqrt(mean_squared_error(test_cpi['cpi'][:len(ensemble_forecast)],\n",
    "                                                          ensemble_forecast[:len(test_cpi['cpi'])]))\n",
    "                }\n",
    "\n",
    "                plot_forecast(\n",
    "                    train_cpi['cpi'][-48:], val_cpi['cpi'], test_cpi['cpi'],\n",
    "                    ensemble_forecast, forecast_index, 'Ensemble Forecast for CPI',\n",
    "                    'CPI', 'cpi_ensemble_forecast.png'\n",
    "                )\n",
    "\n",
    "            plot_comparison_forecasts(\n",
    "                train_cpi['cpi'][-36:], val_cpi['cpi'], test_cpi['cpi'],\n",
    "                forecasts_cpi, forecast_index, 'DL Model Comparison for CPI',\n",
    "                'CPI', 'cpi_dl_model_comparison.png', metrics_cpi\n",
    "            )\n",
    "\n",
    "        results_df = pd.DataFrame(results)\n",
    "        print(\"Model Results:\")\n",
    "        print(results_df)\n",
    "        results_df.to_csv(CONFIG['results_file'], index=False)\n",
    "        logger.info(f\"Results saved to {CONFIG['results_file']}\")\n",
    "\n",
    "        if forecasts_cpi:\n",
    "            combined_forecast = pd.DataFrame({'Date': forecast_index})\n",
    "            for model_name, forecast in forecasts_cpi.items():\n",
    "                combined_forecast[f'{model_name}_cpi'] = forecast\n",
    "            combined_forecast.to_csv(os.path.join(CONFIG['img_dir'], 'combined_forecast_cpi.csv'), index=False)\n",
    "            logger.info(f\"Combined forecast saved to {CONFIG['img_dir']}/combined_forecast_cpi.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Main program error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
