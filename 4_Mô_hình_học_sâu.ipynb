{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f15b48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LSTM model for cpi_yoy...\n",
      "Training data: (260, 12, 63), Test data: (65, 12, 63)\n",
      "Training LSTM model...\n",
      "Epoch 1/100\n",
      "7/7 [==============================] - 4s 140ms/step - loss: 2.5496 - val_loss: 0.0383 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1.2966 - val_loss: 0.0388 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 1.0982 - val_loss: 0.0215 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 1.0238 - val_loss: 0.0207 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.7929 - val_loss: 0.0328 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.9686 - val_loss: 0.0180 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.8547 - val_loss: 0.0155 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.6453 - val_loss: 0.0096 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.5732 - val_loss: 0.0076 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.9149 - val_loss: 0.0089 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.6932 - val_loss: 0.0138 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.7332 - val_loss: 0.0226 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.6053 - val_loss: 0.0578 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.6660 - val_loss: 0.0716 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.6783 - val_loss: 0.0667 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.5404 - val_loss: 0.1037 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.5577 - val_loss: 0.1467 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.5036 - val_loss: 0.0998 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 0s 35ms/step - loss: 0.4981 - val_loss: 0.0685 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.4636 - val_loss: 0.0645 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.4645 - val_loss: 0.0689 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.4827 - val_loss: 0.0985 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.4284 - val_loss: 0.1161 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.4267 - val_loss: 0.1276 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3880 - val_loss: 0.1318 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.4325 - val_loss: 0.1196 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.5042 - val_loss: 0.1197 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.3948 - val_loss: 0.1060 - lr: 5.0000e-04\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.4533 - val_loss: 0.0967 - lr: 5.0000e-04\n",
      "9/9 [==============================] - 1s 6ms/step\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "Training - RMSE: 7.9077, MAE: 5.6517, MAPE: 5.23%, R²: -0.7570\n",
      "Testing - RMSE: 3.4181, MAE: 2.7573, MAPE: 2.68%, R²: -6.5927\n",
      "Generating 24 period forecast...\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "\n",
      "Future cpi_yoy forecasts:\n",
      "2025-01-01    113.654999\n",
      "2025-02-01    114.412773\n",
      "2025-03-01    115.104546\n",
      "2025-04-01    114.555603\n",
      "2025-05-01    114.184265\n",
      "2025-06-01    113.717079\n",
      "2025-07-01    113.084297\n",
      "2025-08-01    112.553818\n",
      "2025-09-01    112.189323\n",
      "2025-10-01    111.886482\n",
      "2025-11-01    111.989037\n",
      "2025-12-01    111.833817\n",
      "2026-01-01    111.833817\n",
      "2026-02-01    111.833817\n",
      "2026-03-01    111.833817\n",
      "2026-04-01    111.833817\n",
      "2026-05-01    111.833817\n",
      "2026-06-01    111.833817\n",
      "2026-07-01    111.833817\n",
      "2026-08-01    111.833817\n",
      "2026-09-01    111.833817\n",
      "2026-10-01    111.833817\n",
      "2026-11-01    111.833817\n",
      "2026-12-01    111.833817\n",
      "Freq: MS, dtype: float32\n",
      "Running LSTM model for cpi_mom...\n",
      "Training data: (260, 12, 63), Test data: (65, 12, 63)\n",
      "Training LSTM model...\n",
      "Epoch 1/100\n",
      "7/7 [==============================] - 4s 126ms/step - loss: 1.9685 - val_loss: 0.1009 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 0s 41ms/step - loss: 1.2187 - val_loss: 0.0581 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 1.0688 - val_loss: 0.0377 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.9901 - val_loss: 0.0231 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 1.0086 - val_loss: 0.0116 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.7663 - val_loss: 0.0087 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.8492 - val_loss: 0.0089 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.5413 - val_loss: 0.0102 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.6824 - val_loss: 0.0119 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.6040 - val_loss: 0.0160 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.8082 - val_loss: 0.0160 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.6139 - val_loss: 0.0137 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.6207 - val_loss: 0.0125 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.6599 - val_loss: 0.0240 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.5054 - val_loss: 0.0345 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.4398 - val_loss: 0.0359 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.6109 - val_loss: 0.0312 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.4779 - val_loss: 0.0263 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.4511 - val_loss: 0.0269 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.6105 - val_loss: 0.0326 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.5574 - val_loss: 0.0393 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.5728 - val_loss: 0.0384 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.4926 - val_loss: 0.0455 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.5187 - val_loss: 0.0568 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3717 - val_loss: 0.0557 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3962 - val_loss: 0.0400 - lr: 5.0000e-04\n",
      "9/9 [==============================] - 0s 5ms/step\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "Training - RMSE: 1.9885, MAE: 1.2773, MAPE: 1.27%, R²: -5.0879\n",
      "Testing - RMSE: 0.7024, MAE: 0.5576, MAPE: 0.56%, R²: -1.1166\n",
      "Generating 24 period forecast...\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "\n",
      "Future cpi_mom forecasts:\n",
      "2025-01-01    100.823608\n",
      "2025-02-01    100.801300\n",
      "2025-03-01    100.775093\n",
      "2025-04-01    100.736115\n",
      "2025-05-01    100.703918\n",
      "2025-06-01    100.664597\n",
      "2025-07-01    100.618858\n",
      "2025-08-01    100.573883\n",
      "2025-09-01    100.534706\n",
      "2025-10-01    100.519157\n",
      "2025-11-01    100.463966\n",
      "2025-12-01    100.451538\n",
      "2026-01-01    100.451538\n",
      "2026-02-01    100.451538\n",
      "2026-03-01    100.451538\n",
      "2026-04-01    100.451538\n",
      "2026-05-01    100.451538\n",
      "2026-06-01    100.451538\n",
      "2026-07-01    100.451538\n",
      "2026-08-01    100.451538\n",
      "2026-09-01    100.451538\n",
      "2026-10-01    100.451538\n",
      "2026-11-01    100.451538\n",
      "2026-12-01    100.451538\n",
      "Freq: MS, dtype: float32\n",
      "\n",
      "Metrics saved to plots/lstm_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load and prepare the time series data\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert time column to datetime\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    \n",
    "    # Set time as index\n",
    "    df.set_index('time', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"Create additional features for modeling\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Create lag features\n",
    "    for i in range(1, 13):\n",
    "        if 'cpi_mom' in df_copy.columns:\n",
    "            df_copy[f'cpi_mom_lag_{i}'] = df_copy['cpi_mom'].shift(i)\n",
    "        if 'cpi_yoy' in df_copy.columns:\n",
    "            df_copy[f'cpi_yoy_lag_{i}'] = df_copy['cpi_yoy'].shift(i)\n",
    "    \n",
    "    # Create rolling window features\n",
    "    for window in [3, 6, 12]:\n",
    "        if 'cpi_mom' in df_copy.columns:\n",
    "            df_copy[f'cpi_mom_rolling_mean_{window}'] = df_copy['cpi_mom'].rolling(window=window).mean()\n",
    "            df_copy[f'cpi_mom_rolling_std_{window}'] = df_copy['cpi_mom'].rolling(window=window).std()\n",
    "        if 'cpi_yoy' in df_copy.columns:\n",
    "            df_copy[f'cpi_yoy_rolling_mean_{window}'] = df_copy['cpi_yoy'].rolling(window=window).mean()\n",
    "            df_copy[f'cpi_yoy_rolling_std_{window}'] = df_copy['cpi_yoy'].rolling(window=window).std()\n",
    "    \n",
    "    # Create economic indicator lag features\n",
    "    for i in range(1, 4):\n",
    "        if 'oil_price' in df_copy.columns:\n",
    "            df_copy[f'oil_price_lag_{i}'] = df_copy['oil_price'].shift(i)\n",
    "        if 'gold_price' in df_copy.columns:\n",
    "            df_copy[f'gold_price_lag_{i}'] = df_copy['gold_price'].shift(i)\n",
    "        if 'interest_rate' in df_copy.columns:\n",
    "            df_copy[f'interest_rate_lag_{i}'] = df_copy['interest_rate'].shift(i)\n",
    "    \n",
    "    # Add month and year as cyclical features\n",
    "    if 'month' in df_copy.columns:\n",
    "        df_copy['month_sin'] = np.sin(2 * np.pi * df_copy['month']/12)\n",
    "        df_copy['month_cos'] = np.cos(2 * np.pi * df_copy['month']/12)\n",
    "    \n",
    "    # Create interaction features\n",
    "    if all(col in df_copy.columns for col in ['oil_price', 'gold_price']):\n",
    "        df_copy['oil_gold_ratio'] = df_copy['oil_price'] / df_copy['gold_price']\n",
    "    \n",
    "    # Drop rows with NaN values (due to lag features)\n",
    "    df_clean = df_copy.dropna()\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def calculate_metrics(actual, predicted):\n",
    "    \"\"\"Calculate evaluation metrics\"\"\"\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    r2 = r2_score(actual, predicted)\n",
    "    mape = np.mean(np.abs((actual - predicted) / actual)) * 100\n",
    "    \n",
    "    return rmse, mae, mape, r2\n",
    "\n",
    "def create_sequences(X, y, time_steps=12):\n",
    "    \"\"\"\n",
    "    Create sequences for LSTM input\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : np.array\n",
    "        Feature array\n",
    "    y : np.array\n",
    "        Target array\n",
    "    time_steps : int\n",
    "        Number of time steps to use for each sequence\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        X_seq, y_seq arrays with shape [samples, time_steps, features] and [samples]\n",
    "    \"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        X_seq.append(X[i:i + time_steps])\n",
    "        y_seq.append(y[i + time_steps])\n",
    "    \n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "def build_lstm_model(input_shape, lstm_units=50, dropout_rate=0.2, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Build an LSTM model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_shape : tuple\n",
    "        Shape of input data (time_steps, features)\n",
    "    lstm_units : int\n",
    "        Number of LSTM units\n",
    "    dropout_rate : float\n",
    "        Dropout rate for regularization\n",
    "    learning_rate : float\n",
    "        Learning rate for optimizer\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tf.keras.Model\n",
    "        Compiled LSTM model\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        LSTM(lstm_units, activation='relu', return_sequences=True, input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        LSTM(lstm_units // 2, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='mse'\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def run_lstm(df, target_col, test_size=0.2, time_steps=12, epochs=100, batch_size=32):\n",
    "    \"\"\"\n",
    "    Train and evaluate an LSTM model for time series forecasting\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with engineered features\n",
    "    target_col : str\n",
    "        Name of the target column\n",
    "    test_size : float\n",
    "        Proportion of data to use for testing\n",
    "    time_steps : int\n",
    "        Number of time steps to use for each sequence\n",
    "    epochs : int\n",
    "        Number of training epochs\n",
    "    batch_size : int\n",
    "        Batch size for training\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        Model, predictions, metrics, history\n",
    "    \"\"\"\n",
    "    print(f\"Running LSTM model for {target_col}...\")\n",
    "    \n",
    "    # Define features and target\n",
    "    X = df.drop([col for col in ['cpi_mom', 'cpi_yoy', target_col] if col in df.columns], axis=1).values\n",
    "    y = df[target_col].values\n",
    "    \n",
    "    # Scale features and target\n",
    "    X_scaler = StandardScaler()\n",
    "    X_scaled = X_scaler.fit_transform(X)\n",
    "    \n",
    "    y_scaler = MinMaxScaler()\n",
    "    y_scaled = y_scaler.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Create sequences\n",
    "    X_seq, y_seq = create_sequences(X_scaled, y_scaled, time_steps)\n",
    "    \n",
    "    # Split data into training and testing sets (time-based split)\n",
    "    split_idx = int(len(X_seq) * (1 - test_size))\n",
    "    X_train, X_test = X_seq[:split_idx], X_seq[split_idx:]\n",
    "    y_train, y_test = y_seq[:split_idx], y_seq[split_idx:]\n",
    "    \n",
    "    print(f\"Training data: {X_train.shape}, Test data: {X_test.shape}\")\n",
    "    \n",
    "    # Build model\n",
    "    model = build_lstm_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        lstm_units=64,\n",
    "        dropout_rate=0.2,\n",
    "        learning_rate=0.001\n",
    "    )\n",
    "    \n",
    "    # Define callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=0.0001),\n",
    "        ModelCheckpoint(f'models/lstm_{target_col}.h5', monitor='val_loss', save_best_only=True)\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training LSTM model...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train_scaled = model.predict(X_train)\n",
    "    y_pred_test_scaled = model.predict(X_test)\n",
    "    \n",
    "    # Inverse transform predictions\n",
    "    y_pred_train = y_scaler.inverse_transform(y_pred_train_scaled).flatten()\n",
    "    y_pred_test = y_scaler.inverse_transform(y_pred_test_scaled).flatten()\n",
    "    \n",
    "    # Inverse transform actual values\n",
    "    y_train_actual = y_scaler.inverse_transform(y_train.reshape(-1, 1)).flatten()\n",
    "    y_test_actual = y_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_rmse, train_mae, train_mape, train_r2 = calculate_metrics(y_train_actual, y_pred_train)\n",
    "    test_rmse, test_mae, test_mape, test_r2 = calculate_metrics(y_test_actual, y_pred_test)\n",
    "    \n",
    "    print(f\"Training - RMSE: {train_rmse:.4f}, MAE: {train_mae:.4f}, MAPE: {train_mape:.2f}%, R²: {train_r2:.4f}\")\n",
    "    print(f\"Testing - RMSE: {test_rmse:.4f}, MAE: {test_mae:.4f}, MAPE: {test_mape:.2f}%, R²: {test_r2:.4f}\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'LSTM Training History for {target_col}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/lstm_training_history_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot actual vs predicted\n",
    "    # Get the original dates for the test set\n",
    "    test_dates = df.index[time_steps+split_idx:time_steps+len(X_seq)]\n",
    "    \n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(test_dates, y_test_actual, label='Actual', color='blue')\n",
    "    plt.plot(test_dates, y_pred_test, label='Predicted', color='red', linestyle='--')\n",
    "    plt.title(f'LSTM: Actual vs Predicted {target_col}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(target_col)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/lstm_predictions_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create a DataFrame with predictions\n",
    "    predictions = pd.DataFrame({\n",
    "        'Actual': y_test_actual,\n",
    "        'Predicted': y_pred_test,\n",
    "        'Error': y_test_actual - y_pred_test\n",
    "    }, index=test_dates)\n",
    "    \n",
    "    # Plot error distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(predictions['Error'], kde=True)\n",
    "    plt.title(f'Error Distribution for {target_col}')\n",
    "    plt.xlabel('Error')\n",
    "    plt.savefig(f'plots/lstm_error_distribution_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Return results\n",
    "    metrics = {\n",
    "        'train_rmse': train_rmse,\n",
    "        'train_mae': train_mae,\n",
    "        'train_mape': train_mape,\n",
    "        'train_r2': train_r2,\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_mae': test_mae,\n",
    "        'test_mape': test_mape,\n",
    "        'test_r2': test_r2\n",
    "    }\n",
    "    \n",
    "    return model, predictions, metrics, history, X_scaler, y_scaler\n",
    "\n",
    "def forecast_future_lstm(model, df, target_col, X_scaler, y_scaler, time_steps=12, forecast_horizon=24):\n",
    "    \"\"\"\n",
    "    Generate future forecasts using the trained LSTM model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : tf.keras.Model\n",
    "        Trained LSTM model\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with features\n",
    "    target_col : str\n",
    "        Name of the target column\n",
    "    X_scaler : StandardScaler\n",
    "        Scaler used for features\n",
    "    y_scaler : MinMaxScaler\n",
    "        Scaler used for target\n",
    "    time_steps : int\n",
    "        Number of time steps used for training\n",
    "    forecast_horizon : int\n",
    "        Number of periods to forecast\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        Forecasted values\n",
    "    \"\"\"\n",
    "    print(f\"Generating {forecast_horizon} period forecast...\")\n",
    "    \n",
    "    # Get the last time_steps data points\n",
    "    X_last = df.drop([col for col in ['cpi_mom', 'cpi_yoy', target_col] if col in df.columns], axis=1).values[-time_steps:]\n",
    "    X_last_scaled = X_scaler.transform(X_last)\n",
    "    \n",
    "    # Reshape for LSTM input [1, time_steps, features]\n",
    "    X_last_scaled = X_last_scaled.reshape(1, time_steps, X_last_scaled.shape[1])\n",
    "    \n",
    "    # Get the last date in the dataframe\n",
    "    last_date = df.index[-1]\n",
    "    \n",
    "    # Create a list to store forecasts\n",
    "    forecasts = []\n",
    "    \n",
    "    # Generate forecasts recursively\n",
    "    for i in range(forecast_horizon):\n",
    "        # Make prediction\n",
    "        forecast_scaled = model.predict(X_last_scaled)\n",
    "        \n",
    "        # Inverse transform prediction\n",
    "        forecast = y_scaler.inverse_transform(forecast_scaled)[0][0]\n",
    "        \n",
    "        # Store forecast\n",
    "        forecasts.append(forecast)\n",
    "        \n",
    "        # For a more accurate implementation, we would need to update all features\n",
    "        # based on the new forecast, but this is a simplified version\n",
    "        # Here we just shift the input sequence and append the new prediction\n",
    "        X_last_scaled = np.roll(X_last_scaled, -1, axis=1)\n",
    "        X_last_scaled[0, -1, :] = X_last_scaled[0, -2, :]  # Simple copy of the last known features\n",
    "    \n",
    "    # Create a Series with the forecasts\n",
    "    future_dates = pd.date_range(start=last_date + pd.DateOffset(months=1), periods=forecast_horizon, freq='MS')\n",
    "    forecast_series = pd.Series(forecasts, index=future_dates)\n",
    "    \n",
    "    # Plot historical data with forecasts\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df[target_col].index, df[target_col], label='Historical Data')\n",
    "    plt.plot(forecast_series.index, forecast_series, label='Forecast', color='red', linestyle='--')\n",
    "    plt.title(f'LSTM: {target_col} Forecast')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(target_col)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/lstm_future_forecast_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return forecast_series\n",
    "\n",
    "def main():\n",
    "    # Create plots and models directories if they don't exist\n",
    "    import os\n",
    "    for directory in ['plots', 'models']:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    \n",
    "    # Load data\n",
    "    file_path = 'data/analyzed_time_series.csv'\n",
    "    df = load_data(file_path)\n",
    "    \n",
    "    # Engineer features\n",
    "    df_engineered = engineer_features(df)\n",
    "    \n",
    "    # Run LSTM for CPI Year-over-Year\n",
    "    target_col = 'cpi_yoy'\n",
    "    model_yoy, predictions_yoy, metrics_yoy, history_yoy, X_scaler_yoy, y_scaler_yoy = run_lstm(\n",
    "        df_engineered, target_col, test_size=0.2, time_steps=12, epochs=100, batch_size=32\n",
    "    )\n",
    "    \n",
    "    # Generate future forecasts for YoY\n",
    "    forecast_yoy = forecast_future_lstm(\n",
    "        model_yoy, df_engineered, target_col, X_scaler_yoy, y_scaler_yoy, time_steps=12, forecast_horizon=24\n",
    "    )\n",
    "    print(f\"\\nFuture {target_col} forecasts:\")\n",
    "    print(forecast_yoy)\n",
    "    \n",
    "    # Run LSTM for CPI Month-over-Month\n",
    "    target_col = 'cpi_mom'\n",
    "    model_mom, predictions_mom, metrics_mom, history_mom, X_scaler_mom, y_scaler_mom = run_lstm(\n",
    "        df_engineered, target_col, test_size=0.2, time_steps=12, epochs=100, batch_size=32\n",
    "    )\n",
    "    \n",
    "    # Generate future forecasts for MoM\n",
    "    forecast_mom = forecast_future_lstm(\n",
    "        model_mom, df_engineered, target_col, X_scaler_mom, y_scaler_mom, time_steps=12, forecast_horizon=24\n",
    "    )\n",
    "    print(f\"\\nFuture {target_col} forecasts:\")\n",
    "    print(forecast_mom)\n",
    "    \n",
    "    # Save metrics to CSV\n",
    "    metrics_data = [\n",
    "        {\n",
    "            'Model': 'LSTM', 'Target': 'CPI MoM',\n",
    "            'Train_RMSE': metrics_mom['train_rmse'], 'Train_MAE': metrics_mom['train_mae'], \n",
    "            'Train_MAPE': metrics_mom['train_mape'], 'Train_R2': metrics_mom['train_r2'],\n",
    "            'Test_RMSE': metrics_mom['test_rmse'], 'Test_MAE': metrics_mom['test_mae'], \n",
    "            'Test_MAPE': metrics_mom['test_mape'], 'Test_R2': metrics_mom['test_r2']\n",
    "        },\n",
    "        {\n",
    "            'Model': 'LSTM', 'Target': 'CPI YoY',\n",
    "            'Train_RMSE': metrics_yoy['train_rmse'], 'Train_MAE': metrics_yoy['train_mae'], \n",
    "            'Train_MAPE': metrics_yoy['train_mape'], 'Train_R2': metrics_yoy['train_r2'],\n",
    "            'Test_RMSE': metrics_yoy['test_rmse'], 'Test_MAE': metrics_yoy['test_mae'], \n",
    "            'Test_MAPE': metrics_yoy['test_mape'], 'Test_R2': metrics_yoy['test_r2']\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "    metrics_df.to_csv('plots/lstm_metrics.csv', index=False)\n",
    "    print(\"\\nMetrics saved to plots/lstm_metrics.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3ed45d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GRU model for cpi_yoy...\n",
      "Training data: (260, 12, 63), Test data: (65, 12, 63)\n",
      "Training GRU model...\n",
      "Epoch 1/100\n",
      "7/7 [==============================] - 7s 187ms/step - loss: 2.0722 - val_loss: 0.1292 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 1.1733 - val_loss: 0.1093 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.7590 - val_loss: 0.0488 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.9447 - val_loss: 0.1044 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.7300 - val_loss: 0.1564 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.5882 - val_loss: 0.1138 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.6877 - val_loss: 0.0803 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.5609 - val_loss: 0.0780 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.5853 - val_loss: 0.0987 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.6651 - val_loss: 0.0826 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.5633 - val_loss: 0.0818 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.6219 - val_loss: 0.0693 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.5871 - val_loss: 0.0627 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.4902 - val_loss: 0.0674 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.4443 - val_loss: 0.0762 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.5063 - val_loss: 0.0862 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.4374 - val_loss: 0.0916 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.4780 - val_loss: 0.0974 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.4719 - val_loss: 0.1119 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.4446 - val_loss: 0.1240 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.3937 - val_loss: 0.0954 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.5084 - val_loss: 0.0724 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.4523 - val_loss: 0.0776 - lr: 5.0000e-04\n",
      "9/9 [==============================] - 1s 7ms/step\n",
      "3/3 [==============================] - 0s 7ms/step\n",
      "Training - RMSE: 9.9214, MAE: 8.2575, MAPE: 7.63%, R²: -1.7658\n",
      "Testing - RMSE: 8.7249, MAE: 7.3740, MAPE: 7.18%, R²: -48.4703\n",
      "Generating 24 period forecast...\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "\n",
      "Future cpi_yoy forecasts:\n",
      "2025-01-01    104.716835\n",
      "2025-02-01    105.649406\n",
      "2025-03-01    106.437927\n",
      "2025-04-01    106.994568\n",
      "2025-05-01    107.284180\n",
      "2025-06-01    107.444801\n",
      "2025-07-01    107.556892\n",
      "2025-08-01    107.559547\n",
      "2025-09-01    107.475830\n",
      "2025-10-01    107.467003\n",
      "2025-11-01    107.379791\n",
      "2025-12-01    107.463058\n",
      "2026-01-01    107.463058\n",
      "2026-02-01    107.463058\n",
      "2026-03-01    107.463058\n",
      "2026-04-01    107.463058\n",
      "2026-05-01    107.463058\n",
      "2026-06-01    107.463058\n",
      "2026-07-01    107.463058\n",
      "2026-08-01    107.463058\n",
      "2026-09-01    107.463058\n",
      "2026-10-01    107.463058\n",
      "2026-11-01    107.463058\n",
      "2026-12-01    107.463058\n",
      "Freq: MS, dtype: float32\n",
      "Running GRU model for cpi_mom...\n",
      "Training data: (260, 12, 63), Test data: (65, 12, 63)\n",
      "Training GRU model...\n",
      "Epoch 1/100\n",
      "7/7 [==============================] - 6s 188ms/step - loss: 1.1803 - val_loss: 0.2470 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 0s 36ms/step - loss: 0.9561 - val_loss: 0.1598 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.7926 - val_loss: 0.1691 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 0.6703 - val_loss: 0.0936 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.6486 - val_loss: 0.0373 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 0s 42ms/step - loss: 0.5997 - val_loss: 0.0228 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 0s 35ms/step - loss: 0.5194 - val_loss: 0.0271 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 0.4762 - val_loss: 0.0419 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 0s 39ms/step - loss: 0.4683 - val_loss: 0.0156 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.5339 - val_loss: 0.0173 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.5139 - val_loss: 0.0215 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.3969 - val_loss: 0.0153 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.4134 - val_loss: 0.0128 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.3885 - val_loss: 0.0192 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.4054 - val_loss: 0.0195 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.4218 - val_loss: 0.0194 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.3843 - val_loss: 0.0132 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.3874 - val_loss: 0.0140 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.4374 - val_loss: 0.0156 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.3434 - val_loss: 0.0168 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3274 - val_loss: 0.0147 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.4161 - val_loss: 0.0176 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.3258 - val_loss: 0.0140 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.3493 - val_loss: 0.0143 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.3406 - val_loss: 0.0147 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.3518 - val_loss: 0.0143 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3261 - val_loss: 0.0158 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.3124 - val_loss: 0.0177 - lr: 5.0000e-04\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.3374 - val_loss: 0.0159 - lr: 5.0000e-04\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.2797 - val_loss: 0.0154 - lr: 5.0000e-04\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.2816 - val_loss: 0.0252 - lr: 5.0000e-04\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.3010 - val_loss: 0.0388 - lr: 5.0000e-04\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.3390 - val_loss: 0.0322 - lr: 5.0000e-04\n",
      "9/9 [==============================] - 1s 8ms/step\n",
      "3/3 [==============================] - 0s 7ms/step\n",
      "Training - RMSE: 0.8058, MAE: 0.6437, MAPE: 0.64%, R²: 0.0003\n",
      "Testing - RMSE: 1.1454, MAE: 0.9808, MAPE: 0.98%, R²: -4.6290\n",
      "Generating 24 period forecast...\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "\n",
      "Future cpi_mom forecasts:\n",
      "2025-01-01    99.682961\n",
      "2025-02-01    99.686012\n",
      "2025-03-01    99.684937\n",
      "2025-04-01    99.692955\n",
      "2025-05-01    99.685005\n",
      "2025-06-01    99.644508\n",
      "2025-07-01    99.659233\n",
      "2025-08-01    99.653625\n",
      "2025-09-01    99.684120\n",
      "2025-10-01    99.703865\n",
      "2025-11-01    99.688293\n",
      "2025-12-01    99.709511\n",
      "2026-01-01    99.709511\n",
      "2026-02-01    99.709511\n",
      "2026-03-01    99.709511\n",
      "2026-04-01    99.709511\n",
      "2026-05-01    99.709511\n",
      "2026-06-01    99.709511\n",
      "2026-07-01    99.709511\n",
      "2026-08-01    99.709511\n",
      "2026-09-01    99.709511\n",
      "2026-10-01    99.709511\n",
      "2026-11-01    99.709511\n",
      "2026-12-01    99.709511\n",
      "Freq: MS, dtype: float32\n",
      "\n",
      "Comparison with LSTM saved to plots/rnn_comparison_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load and prepare the time series data\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert time column to datetime\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    \n",
    "    # Set time as index\n",
    "    df.set_index('time', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"Create additional features for modeling\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Create lag features\n",
    "    for i in range(1, 13):\n",
    "        if 'cpi_mom' in df_copy.columns:\n",
    "            df_copy[f'cpi_mom_lag_{i}'] = df_copy['cpi_mom'].shift(i)\n",
    "        if 'cpi_yoy' in df_copy.columns:\n",
    "            df_copy[f'cpi_yoy_lag_{i}'] = df_copy['cpi_yoy'].shift(i)\n",
    "    \n",
    "    # Create rolling window features\n",
    "    for window in [3, 6, 12]:\n",
    "        if 'cpi_mom' in df_copy.columns:\n",
    "            df_copy[f'cpi_mom_rolling_mean_{window}'] = df_copy['cpi_mom'].rolling(window=window).mean()\n",
    "            df_copy[f'cpi_mom_rolling_std_{window}'] = df_copy['cpi_mom'].rolling(window=window).std()\n",
    "        if 'cpi_yoy' in df_copy.columns:\n",
    "            df_copy[f'cpi_yoy_rolling_mean_{window}'] = df_copy['cpi_yoy'].rolling(window=window).mean()\n",
    "            df_copy[f'cpi_yoy_rolling_std_{window}'] = df_copy['cpi_yoy'].rolling(window=window).std()\n",
    "    \n",
    "    # Create economic indicator lag features\n",
    "    for i in range(1, 4):\n",
    "        if 'oil_price' in df_copy.columns:\n",
    "            df_copy[f'oil_price_lag_{i}'] = df_copy['oil_price'].shift(i)\n",
    "        if 'gold_price' in df_copy.columns:\n",
    "            df_copy[f'gold_price_lag_{i}'] = df_copy['gold_price'].shift(i)\n",
    "        if 'interest_rate' in df_copy.columns:\n",
    "            df_copy[f'interest_rate_lag_{i}'] = df_copy['interest_rate'].shift(i)\n",
    "    \n",
    "    # Add month and year as cyclical features\n",
    "    if 'month' in df_copy.columns:\n",
    "        df_copy['month_sin'] = np.sin(2 * np.pi * df_copy['month']/12)\n",
    "        df_copy['month_cos'] = np.cos(2 * np.pi * df_copy['month']/12)\n",
    "    \n",
    "    # Create interaction features\n",
    "    if all(col in df_copy.columns for col in ['oil_price', 'gold_price']):\n",
    "        df_copy['oil_gold_ratio'] = df_copy['oil_price'] / df_copy['gold_price']\n",
    "    \n",
    "    # Drop rows with NaN values (due to lag features)\n",
    "    df_clean = df_copy.dropna()\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def calculate_metrics(actual, predicted):\n",
    "    \"\"\"Calculate evaluation metrics\"\"\"\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    r2 = r2_score(actual, predicted)\n",
    "    mape = np.mean(np.abs((actual - predicted) / actual)) * 100\n",
    "    \n",
    "    return rmse, mae, mape, r2\n",
    "\n",
    "def create_sequences(X, y, time_steps=12):\n",
    "    \"\"\"\n",
    "    Create sequences for GRU input\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : np.array\n",
    "        Feature array\n",
    "    y : np.array\n",
    "        Target array\n",
    "    time_steps : int\n",
    "        Number of time steps to use for each sequence\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        X_seq, y_seq arrays with shape [samples, time_steps, features] and [samples]\n",
    "    \"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        X_seq.append(X[i:i + time_steps])\n",
    "        y_seq.append(y[i + time_steps])\n",
    "    \n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "def build_gru_model(input_shape, gru_units=50, dropout_rate=0.2, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Build a GRU model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_shape : tuple\n",
    "        Shape of input data (time_steps, features)\n",
    "    gru_units : int\n",
    "        Number of GRU units\n",
    "    dropout_rate : float\n",
    "        Dropout rate for regularization\n",
    "    learning_rate : float\n",
    "        Learning rate for optimizer\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tf.keras.Model\n",
    "        Compiled GRU model\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        GRU(gru_units, activation='tanh', return_sequences=True, input_shape=input_shape, \n",
    "            recurrent_dropout=0.0, reset_after=True),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        GRU(gru_units // 2, activation='tanh', recurrent_dropout=0.0, reset_after=True),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='mse'\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def run_gru(df, target_col, test_size=0.2, time_steps=12, epochs=100, batch_size=32):\n",
    "    \"\"\"\n",
    "    Train and evaluate a GRU model for time series forecasting\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with engineered features\n",
    "    target_col : str\n",
    "        Name of the target column\n",
    "    test_size : float\n",
    "        Proportion of data to use for testing\n",
    "    time_steps : int\n",
    "        Number of time steps to use for each sequence\n",
    "    epochs : int\n",
    "        Number of training epochs\n",
    "    batch_size : int\n",
    "        Batch size for training\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        Model, predictions, metrics, history\n",
    "    \"\"\"\n",
    "    print(f\"Running GRU model for {target_col}...\")\n",
    "    \n",
    "    # Define features and target\n",
    "    X = df.drop([col for col in ['cpi_mom', 'cpi_yoy', target_col] if col in df.columns], axis=1).values\n",
    "    y = df[target_col].values\n",
    "    \n",
    "    # Scale features and target\n",
    "    X_scaler = StandardScaler()\n",
    "    X_scaled = X_scaler.fit_transform(X)\n",
    "    \n",
    "    y_scaler = MinMaxScaler()\n",
    "    y_scaled = y_scaler.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Create sequences\n",
    "    X_seq, y_seq = create_sequences(X_scaled, y_scaled, time_steps)\n",
    "    \n",
    "    # Split data into training and testing sets (time-based split)\n",
    "    split_idx = int(len(X_seq) * (1 - test_size))\n",
    "    X_train, X_test = X_seq[:split_idx], X_seq[split_idx:]\n",
    "    y_train, y_test = y_seq[:split_idx], y_seq[split_idx:]\n",
    "    \n",
    "    print(f\"Training data: {X_train.shape}, Test data: {X_test.shape}\")\n",
    "    \n",
    "    # Build model\n",
    "    model = build_gru_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        gru_units=64,\n",
    "        dropout_rate=0.2,\n",
    "        learning_rate=0.001\n",
    "    )\n",
    "    \n",
    "    # Define callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=0.0001),\n",
    "        ModelCheckpoint(f'models/gru_{target_col}.h5', monitor='val_loss', save_best_only=True)\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training GRU model...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train_scaled = model.predict(X_train)\n",
    "    y_pred_test_scaled = model.predict(X_test)\n",
    "    \n",
    "    # Inverse transform predictions\n",
    "    y_pred_train = y_scaler.inverse_transform(y_pred_train_scaled).flatten()\n",
    "    y_pred_test = y_scaler.inverse_transform(y_pred_test_scaled).flatten()\n",
    "    \n",
    "    # Inverse transform actual values\n",
    "    y_train_actual = y_scaler.inverse_transform(y_train.reshape(-1, 1)).flatten()\n",
    "    y_test_actual = y_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_rmse, train_mae, train_mape, train_r2 = calculate_metrics(y_train_actual, y_pred_train)\n",
    "    test_rmse, test_mae, test_mape, test_r2 = calculate_metrics(y_test_actual, y_pred_test)\n",
    "    \n",
    "    print(f\"Training - RMSE: {train_rmse:.4f}, MAE: {train_mae:.4f}, MAPE: {train_mape:.2f}%, R²: {train_r2:.4f}\")\n",
    "    print(f\"Testing - RMSE: {test_rmse:.4f}, MAE: {test_mae:.4f}, MAPE: {test_mape:.2f}%, R²: {test_r2:.4f}\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'GRU Training History for {target_col}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/gru_training_history_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot actual vs predicted\n",
    "    # Get the original dates for the test set\n",
    "    test_dates = df.index[time_steps+split_idx:time_steps+len(X_seq)]\n",
    "    \n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(test_dates, y_test_actual, label='Actual', color='blue')\n",
    "    plt.plot(test_dates, y_pred_test, label='Predicted', color='red', linestyle='--')\n",
    "    plt.title(f'GRU: Actual vs Predicted {target_col}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(target_col)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/gru_predictions_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create a DataFrame with predictions\n",
    "    predictions = pd.DataFrame({\n",
    "        'Actual': y_test_actual,\n",
    "        'Predicted': y_pred_test,\n",
    "        'Error': y_test_actual - y_pred_test\n",
    "    }, index=test_dates)\n",
    "    \n",
    "    # Plot error distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(predictions['Error'], kde=True)\n",
    "    plt.title(f'Error Distribution for {target_col}')\n",
    "    plt.xlabel('Error')\n",
    "    plt.savefig(f'plots/gru_error_distribution_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Return results\n",
    "    metrics = {\n",
    "        'train_rmse': train_rmse,\n",
    "        'train_mae': train_mae,\n",
    "        'train_mape': train_mape,\n",
    "        'train_r2': train_r2,\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_mae': test_mae,\n",
    "        'test_mape': test_mape,\n",
    "        'test_r2': test_r2\n",
    "    }\n",
    "    \n",
    "    return model, predictions, metrics, history, X_scaler, y_scaler\n",
    "\n",
    "def forecast_future_gru(model, df, target_col, X_scaler, y_scaler, time_steps=12, forecast_horizon=24):\n",
    "    \"\"\"\n",
    "    Generate future forecasts using the trained GRU model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : tf.keras.Model\n",
    "        Trained GRU model\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with features\n",
    "    target_col : str\n",
    "        Name of the target column\n",
    "    X_scaler : StandardScaler\n",
    "        Scaler used for features\n",
    "    y_scaler : MinMaxScaler\n",
    "        Scaler used for target\n",
    "    time_steps : int\n",
    "        Number of time steps used for training\n",
    "    forecast_horizon : int\n",
    "        Number of periods to forecast\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        Forecasted values\n",
    "    \"\"\"\n",
    "    print(f\"Generating {forecast_horizon} period forecast...\")\n",
    "    \n",
    "    # Get the last time_steps data points\n",
    "    X_last = df.drop([col for col in ['cpi_mom', 'cpi_yoy', target_col] if col in df.columns], axis=1).values[-time_steps:]\n",
    "    X_last_scaled = X_scaler.transform(X_last)\n",
    "    \n",
    "    # Reshape for GRU input [1, time_steps, features]\n",
    "    X_last_scaled = X_last_scaled.reshape(1, time_steps, X_last_scaled.shape[1])\n",
    "    \n",
    "    # Get the last date in the dataframe\n",
    "    last_date = df.index[-1]\n",
    "    \n",
    "    # Create a list to store forecasts\n",
    "    forecasts = []\n",
    "    \n",
    "    # Generate forecasts recursively\n",
    "    for i in range(forecast_horizon):\n",
    "        # Make prediction\n",
    "        forecast_scaled = model.predict(X_last_scaled)\n",
    "        \n",
    "        # Inverse transform prediction\n",
    "        forecast = y_scaler.inverse_transform(forecast_scaled)[0][0]\n",
    "        \n",
    "        # Store forecast\n",
    "        forecasts.append(forecast)\n",
    "        \n",
    "        # For a more accurate implementation, we would need to update all features\n",
    "        # based on the new forecast, but this is a simplified version\n",
    "        # Here we just shift the input sequence and append the new prediction\n",
    "        X_last_scaled = np.roll(X_last_scaled, -1, axis=1)\n",
    "        X_last_scaled[0, -1, :] = X_last_scaled[0, -2, :]  # Simple copy of the last known features\n",
    "    \n",
    "    # Create a Series with the forecasts\n",
    "    future_dates = pd.date_range(start=last_date + pd.DateOffset(months=1), periods=forecast_horizon, freq='MS')\n",
    "    forecast_series = pd.Series(forecasts, index=future_dates)\n",
    "    \n",
    "    # Plot historical data with forecasts\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df[target_col].index, df[target_col], label='Historical Data')\n",
    "    plt.plot(forecast_series.index, forecast_series, label='Forecast', color='red', linestyle='--')\n",
    "    plt.title(f'GRU: {target_col} Forecast')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(target_col)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/gru_future_forecast_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return forecast_series\n",
    "\n",
    "def main():\n",
    "    # Create plots and models directories if they don't exist\n",
    "    import os\n",
    "    for directory in ['plots', 'models']:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    \n",
    "    # Load data\n",
    "    file_path = 'data/analyzed_time_series.csv'\n",
    "    df = load_data(file_path)\n",
    "    \n",
    "    # Engineer features\n",
    "    df_engineered = engineer_features(df)\n",
    "    \n",
    "    # Run GRU for CPI Year-over-Year\n",
    "    target_col = 'cpi_yoy'\n",
    "    model_yoy, predictions_yoy, metrics_yoy, history_yoy, X_scaler_yoy, y_scaler_yoy = run_gru(\n",
    "        df_engineered, target_col, test_size=0.2, time_steps=12, epochs=100, batch_size=32\n",
    "    )\n",
    "    \n",
    "    # Generate future forecasts for YoY\n",
    "    forecast_yoy = forecast_future_gru(\n",
    "        model_yoy, df_engineered, target_col, X_scaler_yoy, y_scaler_yoy, time_steps=12, forecast_horizon=24\n",
    "    )\n",
    "    print(f\"\\nFuture {target_col} forecasts:\")\n",
    "    print(forecast_yoy)\n",
    "    \n",
    "    # Run GRU for CPI Month-over-Month\n",
    "    target_col = 'cpi_mom'\n",
    "    model_mom, predictions_mom, metrics_mom, history_mom, X_scaler_mom, y_scaler_mom = run_gru(\n",
    "        df_engineered, target_col, test_size=0.2, time_steps=12, epochs=100, batch_size=32\n",
    "    )\n",
    "    \n",
    "    # Generate future forecasts for MoM\n",
    "    forecast_mom = forecast_future_gru(\n",
    "        model_mom, df_engineered, target_col, X_scaler_mom, y_scaler_mom, time_steps=12, forecast_horizon=24\n",
    "    )\n",
    "    print(f\"\\nFuture {target_col} forecasts:\")\n",
    "    print(forecast_mom)\n",
    "    \n",
    "    # Compare with LSTM (if available)\n",
    "    try:\n",
    "        lstm_metrics = pd.read_csv('plots/lstm_metrics.csv')\n",
    "        gru_metrics = pd.DataFrame([\n",
    "            {\n",
    "                'Model': 'GRU', 'Target': 'CPI MoM',\n",
    "                'Train_RMSE': metrics_mom['train_rmse'], 'Train_MAE': metrics_mom['train_mae'], \n",
    "                'Train_MAPE': metrics_mom['train_mape'], 'Train_R2': metrics_mom['train_r2'],\n",
    "                'Test_RMSE': metrics_mom['test_rmse'], 'Test_MAE': metrics_mom['test_mae'], \n",
    "                'Test_MAPE': metrics_mom['test_mape'], 'Test_R2': metrics_mom['test_r2']\n",
    "            },\n",
    "            {\n",
    "                'Model': 'GRU', 'Target': 'CPI YoY',\n",
    "                'Train_RMSE': metrics_yoy['train_rmse'], 'Train_MAE': metrics_yoy['train_mae'], \n",
    "                'Train_MAPE': metrics_yoy['train_mape'], 'Train_R2': metrics_yoy['train_r2'],\n",
    "                'Test_RMSE': metrics_yoy['test_rmse'], 'Test_MAE': metrics_yoy['test_mae'], \n",
    "                'Test_MAPE': metrics_yoy['test_mape'], 'Test_R2': metrics_yoy['test_r2']\n",
    "            }\n",
    "        ])\n",
    "        \n",
    "        # Combine metrics\n",
    "        combined_metrics = pd.concat([lstm_metrics, gru_metrics])\n",
    "        combined_metrics.to_csv('plots/rnn_comparison_metrics.csv', index=False)\n",
    "        print(\"\\nComparison with LSTM saved to plots/rnn_comparison_metrics.csv\")\n",
    "        \n",
    "        # Create comparison plots\n",
    "        for target, target_name in [('CPI MoM', 'cpi_mom'), ('CPI YoY', 'cpi_yoy')]:\n",
    "            lstm_row = lstm_metrics[lstm_metrics['Target'] == target].iloc[0]\n",
    "            gru_row = gru_metrics[gru_metrics['Target'] == target].iloc[0]\n",
    "            \n",
    "            # Compare test metrics\n",
    "            metrics = ['Test_RMSE', 'Test_MAE', 'Test_MAPE']\n",
    "            values = [\n",
    "                [lstm_row[metric], gru_row[metric]] for metric in metrics\n",
    "            ]\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            x = np.arange(len(metrics))\n",
    "            width = 0.35\n",
    "            \n",
    "            plt.bar(x - width/2, [v[0] for v in values], width, label='LSTM')\n",
    "            plt.bar(x + width/2, [v[1] for v in values], width, label='GRU')\n",
    "            \n",
    "            plt.xlabel('Metric')\n",
    "            plt.ylabel('Value')\n",
    "            plt.title(f'LSTM vs GRU Performance Comparison for {target}')\n",
    "            plt.xticks(x, metrics)\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'plots/lstm_vs_gru_{target_name}.png')\n",
    "            plt.close()\n",
    "    except:\n",
    "        # Save metrics to CSV\n",
    "        metrics_data = [\n",
    "            {\n",
    "                'Model': 'GRU', 'Target': 'CPI MoM',\n",
    "                'Train_RMSE': metrics_mom['train_rmse'], 'Train_MAE': metrics_mom['train_mae'], \n",
    "                'Train_MAPE': metrics_mom['train_mape'], 'Train_R2': metrics_mom['train_r2'],\n",
    "                'Test_RMSE': metrics_mom['test_rmse'], 'Test_MAE': metrics_mom['test_mae'], \n",
    "                'Test_MAPE': metrics_mom['test_mape'], 'Test_R2': metrics_mom['test_r2']\n",
    "            },\n",
    "            {\n",
    "                'Model': 'GRU', 'Target': 'CPI YoY',\n",
    "                'Train_RMSE': metrics_yoy['train_rmse'], 'Train_MAE': metrics_yoy['train_mae'], \n",
    "                'Train_MAPE': metrics_yoy['train_mape'], 'Train_R2': metrics_yoy['train_r2'],\n",
    "                'Test_RMSE': metrics_yoy['test_rmse'], 'Test_MAE': metrics_yoy['test_mae'], \n",
    "                'Test_MAPE': metrics_yoy['test_mape'], 'Test_R2': metrics_yoy['test_r2']\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        metrics_df = pd.DataFrame(metrics_data)\n",
    "        metrics_df.to_csv('plots/gru_metrics.csv', index=False)\n",
    "        print(\"\\nMetrics saved to plots/gru_metrics.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a051831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Transformer model for cpi_yoy...\n",
      "Training data: (260, 12, 63), Test data: (65, 12, 63)\n",
      "Training Transformer model...\n",
      "Epoch 1/100\n",
      "7/7 [==============================] - 4s 168ms/step - loss: 0.2250 - val_loss: 0.0019 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 0.0541 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0235 - val_loss: 0.0237 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0206 - val_loss: 0.0030 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0182 - val_loss: 0.0025 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0117 - val_loss: 0.0025 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0086 - val_loss: 0.0016 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0079 - val_loss: 0.0020 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0075 - val_loss: 0.0024 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0065 - val_loss: 0.0029 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0058 - val_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0064 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0062 - val_loss: 0.0019 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.0052 - val_loss: 0.0028 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 0s 37ms/step - loss: 0.0064 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 0.0057 - val_loss: 0.0018 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 0s 37ms/step - loss: 0.0050 - val_loss: 0.0037 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.0071 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.0055 - val_loss: 0.0018 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.0042 - val_loss: 0.0020 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.0049 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 0s 37ms/step - loss: 0.0053 - val_loss: 0.0020 - lr: 5.0000e-04\n",
      "9/9 [==============================] - 0s 8ms/step\n",
      "3/3 [==============================] - 0s 7ms/step\n",
      "Training - RMSE: 2.6557, MAE: 2.0813, MAPE: 1.94%, R²: 0.8018\n",
      "Testing - RMSE: 1.8976, MAE: 1.5027, MAPE: 1.46%, R²: -1.3401\n",
      "Generating 24 period forecast...\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "\n",
      "Future cpi_yoy forecasts:\n",
      "2025-01-01    103.279755\n",
      "2025-02-01    103.890457\n",
      "2025-03-01    103.712593\n",
      "2025-04-01    104.517929\n",
      "2025-05-01    104.370888\n",
      "2025-06-01    103.729355\n",
      "2025-07-01    103.118645\n",
      "2025-08-01    102.262581\n",
      "2025-09-01    102.084023\n",
      "2025-10-01    101.919800\n",
      "2025-11-01    101.800255\n",
      "2025-12-01    101.455254\n",
      "2026-01-01    101.455254\n",
      "2026-02-01    101.455254\n",
      "2026-03-01    101.455254\n",
      "2026-04-01    101.455254\n",
      "2026-05-01    101.455254\n",
      "2026-06-01    101.455254\n",
      "2026-07-01    101.455254\n",
      "2026-08-01    101.455254\n",
      "2026-09-01    101.455254\n",
      "2026-10-01    101.455254\n",
      "2026-11-01    101.455254\n",
      "2026-12-01    101.455254\n",
      "Freq: MS, dtype: float32\n",
      "Running Transformer model for cpi_mom...\n",
      "Training data: (260, 12, 63), Test data: (65, 12, 63)\n",
      "Training Transformer model...\n",
      "Epoch 1/100\n",
      "7/7 [==============================] - 4s 122ms/step - loss: 0.1657 - val_loss: 0.0167 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0508 - val_loss: 0.0773 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0485 - val_loss: 0.1192 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0388 - val_loss: 0.0262 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 0.0325 - val_loss: 0.0131 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0314 - val_loss: 0.0247 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0251 - val_loss: 0.0136 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0263 - val_loss: 0.0209 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0254 - val_loss: 0.0196 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0254 - val_loss: 0.0257 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0203 - val_loss: 0.0289 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0223 - val_loss: 0.0244 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0207 - val_loss: 0.0165 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0185 - val_loss: 0.0157 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0199 - val_loss: 0.0198 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0170 - val_loss: 0.0181 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0175 - val_loss: 0.0153 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0142 - val_loss: 0.0155 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 0.0143 - val_loss: 0.0125 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.0148 - val_loss: 0.0131 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.0137 - val_loss: 0.0132 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0144 - val_loss: 0.0154 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 0s 55ms/step - loss: 0.0131 - val_loss: 0.0105 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.0131 - val_loss: 0.0146 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.0136 - val_loss: 0.0143 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.0096 - val_loss: 0.0185 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0103 - val_loss: 0.0116 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.0126 - val_loss: 0.0135 - lr: 5.0000e-04\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0106 - val_loss: 0.0090 - lr: 5.0000e-04\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0117 - val_loss: 0.0094 - lr: 5.0000e-04\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.0092 - val_loss: 0.0079 - lr: 5.0000e-04\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.0089 - val_loss: 0.0082 - lr: 5.0000e-04\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0101 - val_loss: 0.0096 - lr: 5.0000e-04\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0098 - val_loss: 0.0106 - lr: 5.0000e-04\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0093 - val_loss: 0.0133 - lr: 5.0000e-04\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0068 - val_loss: 0.0132 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.0089 - val_loss: 0.0126 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.0067 - val_loss: 0.0128 - lr: 5.0000e-04\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0071 - val_loss: 0.0142 - lr: 5.0000e-04\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0081 - val_loss: 0.0126 - lr: 5.0000e-04\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0076 - val_loss: 0.0142 - lr: 5.0000e-04\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0071 - val_loss: 0.0178 - lr: 2.5000e-04\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0083 - val_loss: 0.0135 - lr: 2.5000e-04\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0058 - val_loss: 0.0122 - lr: 2.5000e-04\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0062 - val_loss: 0.0109 - lr: 2.5000e-04\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0077 - val_loss: 0.0131 - lr: 2.5000e-04\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0063 - val_loss: 0.0132 - lr: 2.5000e-04\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0052 - val_loss: 0.0131 - lr: 2.5000e-04\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0053 - val_loss: 0.0109 - lr: 2.5000e-04\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0055 - val_loss: 0.0096 - lr: 2.5000e-04\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0066 - val_loss: 0.0100 - lr: 2.5000e-04\n",
      "9/9 [==============================] - 0s 7ms/step\n",
      "3/3 [==============================] - 0s 6ms/step\n",
      "Training - RMSE: 0.4212, MAE: 0.3305, MAPE: 0.33%, R²: 0.7268\n",
      "Testing - RMSE: 1.1633, MAE: 1.0263, MAPE: 1.02%, R²: -4.8061\n",
      "Generating 24 period forecast...\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\n",
      "Future cpi_mom forecasts:\n",
      "2025-01-01    98.383919\n",
      "2025-02-01    98.460487\n",
      "2025-03-01    98.466354\n",
      "2025-04-01    98.492317\n",
      "2025-05-01    98.596970\n",
      "2025-06-01    98.635864\n",
      "2025-07-01    98.831596\n",
      "2025-08-01    99.058205\n",
      "2025-09-01    98.993195\n",
      "2025-10-01    99.073708\n",
      "2025-11-01    99.126129\n",
      "2025-12-01    99.139412\n",
      "2026-01-01    99.139412\n",
      "2026-02-01    99.139412\n",
      "2026-03-01    99.139412\n",
      "2026-04-01    99.139412\n",
      "2026-05-01    99.139412\n",
      "2026-06-01    99.139412\n",
      "2026-07-01    99.139412\n",
      "2026-08-01    99.139412\n",
      "2026-09-01    99.139412\n",
      "2026-10-01    99.139412\n",
      "2026-11-01    99.139412\n",
      "2026-12-01    99.139412\n",
      "Freq: MS, dtype: float32\n",
      "Could not compare with other models: [Errno 2] No such file or directory: 'plots/gru_metrics.csv'\n",
      "\n",
      "Metrics saved to plots/transformer_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, Embedding, Add, Lambda\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load and prepare the time series data\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert time column to datetime\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    \n",
    "    # Set time as index\n",
    "    df.set_index('time', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"Create additional features for modeling\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Create lag features\n",
    "    for i in range(1, 13):\n",
    "        if 'cpi_mom' in df_copy.columns:\n",
    "            df_copy[f'cpi_mom_lag_{i}'] = df_copy['cpi_mom'].shift(i)\n",
    "        if 'cpi_yoy' in df_copy.columns:\n",
    "            df_copy[f'cpi_yoy_lag_{i}'] = df_copy['cpi_yoy'].shift(i)\n",
    "    \n",
    "    # Create rolling window features\n",
    "    for window in [3, 6, 12]:\n",
    "        if 'cpi_mom' in df_copy.columns:\n",
    "            df_copy[f'cpi_mom_rolling_mean_{window}'] = df_copy['cpi_mom'].rolling(window=window).mean()\n",
    "            df_copy[f'cpi_mom_rolling_std_{window}'] = df_copy['cpi_mom'].rolling(window=window).std()\n",
    "        if 'cpi_yoy' in df_copy.columns:\n",
    "            df_copy[f'cpi_yoy_rolling_mean_{window}'] = df_copy['cpi_yoy'].rolling(window=window).mean()\n",
    "            df_copy[f'cpi_yoy_rolling_std_{window}'] = df_copy['cpi_yoy'].rolling(window=window).std()\n",
    "    \n",
    "    # Create economic indicator lag features\n",
    "    for i in range(1, 4):\n",
    "        if 'oil_price' in df_copy.columns:\n",
    "            df_copy[f'oil_price_lag_{i}'] = df_copy['oil_price'].shift(i)\n",
    "        if 'gold_price' in df_copy.columns:\n",
    "            df_copy[f'gold_price_lag_{i}'] = df_copy['gold_price'].shift(i)\n",
    "        if 'interest_rate' in df_copy.columns:\n",
    "            df_copy[f'interest_rate_lag_{i}'] = df_copy['interest_rate'].shift(i)\n",
    "    \n",
    "    # Add month and year as cyclical features\n",
    "    if 'month' in df_copy.columns:\n",
    "        df_copy['month_sin'] = np.sin(2 * np.pi * df_copy['month']/12)\n",
    "        df_copy['month_cos'] = np.cos(2 * np.pi * df_copy['month']/12)\n",
    "    \n",
    "    # Create interaction features\n",
    "    if all(col in df_copy.columns for col in ['oil_price', 'gold_price']):\n",
    "        df_copy['oil_gold_ratio'] = df_copy['oil_price'] / df_copy['gold_price']\n",
    "    \n",
    "    # Drop rows with NaN values (due to lag features)\n",
    "    df_clean = df_copy.dropna()\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def calculate_metrics(actual, predicted):\n",
    "    \"\"\"Calculate evaluation metrics\"\"\"\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    r2 = r2_score(actual, predicted)\n",
    "    mape = np.mean(np.abs((actual - predicted) / actual)) * 100\n",
    "    \n",
    "    return rmse, mae, mape, r2\n",
    "\n",
    "def create_sequences(X, y, time_steps=12):\n",
    "    \"\"\"\n",
    "    Create sequences for Transformer input\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : np.array\n",
    "        Feature array\n",
    "    y : np.array\n",
    "        Target array\n",
    "    time_steps : int\n",
    "        Number of time steps to use for each sequence\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        X_seq, y_seq arrays with shape [samples, time_steps, features] and [samples]\n",
    "    \"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        X_seq.append(X[i:i + time_steps])\n",
    "        y_seq.append(y[i + time_steps])\n",
    "    \n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "def positional_encoding(seq_len, d_model):\n",
    "    \"\"\"\n",
    "    Create positional encoding for transformer\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    seq_len : int\n",
    "        Sequence length\n",
    "    d_model : int\n",
    "        Dimension of the model\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tf.Tensor\n",
    "        Positional encoding tensor\n",
    "    \"\"\"\n",
    "    positions = np.arange(seq_len)[:, np.newaxis]\n",
    "    angles = np.arange(d_model)[np.newaxis, :] / np.power(10000, (2 * (np.arange(d_model)[np.newaxis, :] // 2)) / d_model)\n",
    "    \n",
    "    # Apply sin to even indices\n",
    "    sines = np.sin(positions * angles[:, 0::2])\n",
    "    \n",
    "    # Apply cos to odd indices\n",
    "    cosines = np.cos(positions * angles[:, 1::2])\n",
    "    \n",
    "    # Combine sin and cos\n",
    "    pos_encoding = np.zeros((seq_len, d_model))\n",
    "    pos_encoding[:, 0::2] = sines\n",
    "    pos_encoding[:, 1::2] = cosines\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.1):\n",
    "    \"\"\"\n",
    "    Create a transformer encoder block\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    inputs : tf.Tensor\n",
    "        Input tensor\n",
    "    head_size : int\n",
    "        Size of each attention head\n",
    "    num_heads : int\n",
    "        Number of attention heads\n",
    "    ff_dim : int\n",
    "        Hidden layer size in feed forward network\n",
    "    dropout : float\n",
    "        Dropout rate\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tf.Tensor\n",
    "        Output tensor\n",
    "    \"\"\"\n",
    "    # Multi-head attention\n",
    "    attention_output = MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(inputs, inputs)\n",
    "    \n",
    "    # Add & Norm\n",
    "    attention_output = LayerNormalization(epsilon=1e-6)(inputs + attention_output)\n",
    "    \n",
    "    # Feed Forward\n",
    "    ff_output = Dense(ff_dim, activation='relu')(attention_output)\n",
    "    ff_output = Dense(inputs.shape[-1])(ff_output)\n",
    "    ff_output = Dropout(dropout)(ff_output)\n",
    "    \n",
    "    # Add & Norm\n",
    "    return LayerNormalization(epsilon=1e-6)(attention_output + ff_output)\n",
    "\n",
    "def build_transformer_model(input_shape, head_size=64, num_heads=4, ff_dim=256, num_transformer_blocks=4, mlp_units=[128, 64], dropout=0.1, mlp_dropout=0.1):\n",
    "    \"\"\"\n",
    "    Build a transformer model for time series forecasting\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_shape : tuple\n",
    "        Shape of input data (time_steps, features)\n",
    "    head_size : int\n",
    "        Size of each attention head\n",
    "    num_heads : int\n",
    "        Number of attention heads\n",
    "    ff_dim : int\n",
    "        Hidden layer size in feed forward network\n",
    "    num_transformer_blocks : int\n",
    "        Number of transformer blocks\n",
    "    mlp_units : list\n",
    "        Number of units in MLP layers\n",
    "    dropout : float\n",
    "        Dropout rate in transformer blocks\n",
    "    mlp_dropout : float\n",
    "        Dropout rate in MLP layers\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tf.keras.Model\n",
    "        Compiled transformer model\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Add positional encoding\n",
    "    pos_encoding = positional_encoding(input_shape[0], input_shape[1])\n",
    "    x = inputs + pos_encoding\n",
    "    \n",
    "    # Transformer blocks\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "    \n",
    "    # Global average pooling\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # MLP layers\n",
    "    for dim in mlp_units:\n",
    "        x = Dense(dim, activation='relu')(x)\n",
    "        x = Dropout(mlp_dropout)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = Dense(1)(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mse'\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def run_transformer(df, target_col, test_size=0.2, time_steps=12, epochs=100, batch_size=32):\n",
    "    \"\"\"\n",
    "    Train and evaluate a Transformer model for time series forecasting\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with engineered features\n",
    "    target_col : str\n",
    "        Name of the target column\n",
    "    test_size : float\n",
    "        Proportion of data to use for testing\n",
    "    time_steps : int\n",
    "        Number of time steps to use for each sequence\n",
    "    epochs : int\n",
    "        Number of training epochs\n",
    "    batch_size : int\n",
    "        Batch size for training\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        Model, predictions, metrics, history\n",
    "    \"\"\"\n",
    "    print(f\"Running Transformer model for {target_col}...\")\n",
    "    \n",
    "    # Define features and target\n",
    "    X = df.drop([col for col in ['cpi_mom', 'cpi_yoy', target_col] if col in df.columns], axis=1).values\n",
    "    y = df[target_col].values\n",
    "    \n",
    "    # Scale features and target\n",
    "    X_scaler = StandardScaler()\n",
    "    X_scaled = X_scaler.fit_transform(X)\n",
    "    \n",
    "    y_scaler = MinMaxScaler()\n",
    "    y_scaled = y_scaler.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Create sequences\n",
    "    X_seq, y_seq = create_sequences(X_scaled, y_scaled, time_steps)\n",
    "    \n",
    "    # Split data into training and testing sets (time-based split)\n",
    "    split_idx = int(len(X_seq) * (1 - test_size))\n",
    "    X_train, X_test = X_seq[:split_idx], X_seq[split_idx:]\n",
    "    y_train, y_test = y_seq[:split_idx], y_seq[split_idx:]\n",
    "    \n",
    "    print(f\"Training data: {X_train.shape}, Test data: {X_test.shape}\")\n",
    "    \n",
    "    # Build model\n",
    "    model = build_transformer_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        head_size=64,\n",
    "        num_heads=4,\n",
    "        ff_dim=256,\n",
    "        num_transformer_blocks=2,\n",
    "        mlp_units=[128, 64],\n",
    "        dropout=0.1,\n",
    "        mlp_dropout=0.1\n",
    "    )\n",
    "    \n",
    "    # Define callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=0.0001),\n",
    "        ModelCheckpoint(f'models/transformer_{target_col}.h5', monitor='val_loss', save_best_only=True)\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training Transformer model...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train_scaled = model.predict(X_train)\n",
    "    y_pred_test_scaled = model.predict(X_test)\n",
    "    \n",
    "    # Inverse transform predictions\n",
    "    y_pred_train = y_scaler.inverse_transform(y_pred_train_scaled).flatten()\n",
    "    y_pred_test = y_scaler.inverse_transform(y_pred_test_scaled).flatten()\n",
    "    \n",
    "    # Inverse transform actual values\n",
    "    y_train_actual = y_scaler.inverse_transform(y_train.reshape(-1, 1)).flatten()\n",
    "    y_test_actual = y_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_rmse, train_mae, train_mape, train_r2 = calculate_metrics(y_train_actual, y_pred_train)\n",
    "    test_rmse, test_mae, test_mape, test_r2 = calculate_metrics(y_test_actual, y_pred_test)\n",
    "    \n",
    "    print(f\"Training - RMSE: {train_rmse:.4f}, MAE: {train_mae:.4f}, MAPE: {train_mape:.2f}%, R²: {train_r2:.4f}\")\n",
    "    print(f\"Testing - RMSE: {test_rmse:.4f}, MAE: {test_mae:.4f}, MAPE: {test_mape:.2f}%, R²: {test_r2:.4f}\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'Transformer Training History for {target_col}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/transformer_training_history_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot actual vs predicted\n",
    "    # Get the original dates for the test set\n",
    "    test_dates = df.index[time_steps+split_idx:time_steps+len(X_seq)]\n",
    "    \n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(test_dates, y_test_actual, label='Actual', color='blue')\n",
    "    plt.plot(test_dates, y_pred_test, label='Predicted', color='red', linestyle='--')\n",
    "    plt.title(f'Transformer: Actual vs Predicted {target_col}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(target_col)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/transformer_predictions_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create a DataFrame with predictions\n",
    "    predictions = pd.DataFrame({\n",
    "        'Actual': y_test_actual,\n",
    "        'Predicted': y_pred_test,\n",
    "        'Error': y_test_actual - y_pred_test\n",
    "    }, index=test_dates)\n",
    "    \n",
    "    # Plot error distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(predictions['Error'], kde=True)\n",
    "    plt.title(f'Error Distribution for {target_col}')\n",
    "    plt.xlabel('Error')\n",
    "    plt.savefig(f'plots/transformer_error_distribution_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Return results\n",
    "    metrics = {\n",
    "        'train_rmse': train_rmse,\n",
    "        'train_mae': train_mae,\n",
    "        'train_mape': train_mape,\n",
    "        'train_r2': train_r2,\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_mae': test_mae,\n",
    "        'test_mape': test_mape,\n",
    "        'test_r2': test_r2\n",
    "    }\n",
    "    \n",
    "    return model, predictions, metrics, history, X_scaler, y_scaler\n",
    "\n",
    "def forecast_future_transformer(model, df, target_col, X_scaler, y_scaler, time_steps=12, forecast_horizon=24):\n",
    "    \"\"\"\n",
    "    Generate future forecasts using the trained Transformer model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : tf.keras.Model\n",
    "        Trained Transformer model\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with features\n",
    "    target_col : str\n",
    "        Name of the target column\n",
    "    X_scaler : StandardScaler\n",
    "        Scaler used for features\n",
    "    y_scaler : MinMaxScaler\n",
    "        Scaler used for target\n",
    "    time_steps : int\n",
    "        Number of time steps used for training\n",
    "    forecast_horizon : int\n",
    "        Number of periods to forecast\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        Forecasted values\n",
    "    \"\"\"\n",
    "    print(f\"Generating {forecast_horizon} period forecast...\")\n",
    "    \n",
    "    # Get the last time_steps data points\n",
    "    X_last = df.drop([col for col in ['cpi_mom', 'cpi_yoy', target_col] if col in df.columns], axis=1).values[-time_steps:]\n",
    "    X_last_scaled = X_scaler.transform(X_last)\n",
    "    \n",
    "    # Reshape for Transformer input [1, time_steps, features]\n",
    "    X_last_scaled = X_last_scaled.reshape(1, time_steps, X_last_scaled.shape[1])\n",
    "    \n",
    "    # Get the last date in the dataframe\n",
    "    last_date = df.index[-1]\n",
    "    \n",
    "    # Create a list to store forecasts\n",
    "    forecasts = []\n",
    "    \n",
    "    # Generate forecasts recursively\n",
    "    for i in range(forecast_horizon):\n",
    "        # Make prediction\n",
    "        forecast_scaled = model.predict(X_last_scaled)\n",
    "        \n",
    "        # Inverse transform prediction\n",
    "        forecast = y_scaler.inverse_transform(forecast_scaled)[0][0]\n",
    "        \n",
    "        # Store forecast\n",
    "        forecasts.append(forecast)\n",
    "        \n",
    "        # For a more accurate implementation, we would need to update all features\n",
    "        # based on the new forecast, but this is a simplified version\n",
    "        # Here we just shift the input sequence and append the new prediction\n",
    "        X_last_scaled = np.roll(X_last_scaled, -1, axis=1)\n",
    "        X_last_scaled[0, -1, :] = X_last_scaled[0, -2, :]  # Simple copy of the last known features\n",
    "    \n",
    "    # Create a Series with the forecasts\n",
    "    future_dates = pd.date_range(start=last_date + pd.DateOffset(months=1), periods=forecast_horizon, freq='MS')\n",
    "    forecast_series = pd.Series(forecasts, index=future_dates)\n",
    "    \n",
    "    # Plot historical data with forecasts\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df[target_col].index, df[target_col], label='Historical Data')\n",
    "    plt.plot(forecast_series.index, forecast_series, label='Forecast', color='red', linestyle='--')\n",
    "    plt.title(f'Transformer: {target_col} Forecast')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(target_col)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/transformer_future_forecast_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return forecast_series\n",
    "\n",
    "def main():\n",
    "    # Create plots and models directories if they don't exist\n",
    "    import os\n",
    "    for directory in ['plots', 'models']:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    \n",
    "    # Load data\n",
    "    file_path = 'data/analyzed_time_series.csv'\n",
    "    df = load_data(file_path)\n",
    "    \n",
    "    # Engineer features\n",
    "    df_engineered = engineer_features(df)\n",
    "    \n",
    "    # Run Transformer for CPI Year-over-Year\n",
    "    target_col = 'cpi_yoy'\n",
    "    model_yoy, predictions_yoy, metrics_yoy, history_yoy, X_scaler_yoy, y_scaler_yoy = run_transformer(\n",
    "        df_engineered, target_col, test_size=0.2, time_steps=12, epochs=100, batch_size=32\n",
    "    )\n",
    "    \n",
    "    # Generate future forecasts for YoY\n",
    "    forecast_yoy = forecast_future_transformer(\n",
    "        model_yoy, df_engineered, target_col, X_scaler_yoy, y_scaler_yoy, time_steps=12, forecast_horizon=24\n",
    "    )\n",
    "    print(f\"\\nFuture {target_col} forecasts:\")\n",
    "    print(forecast_yoy)\n",
    "    \n",
    "    # Run Transformer for CPI Month-over-Month\n",
    "    target_col = 'cpi_mom'\n",
    "    model_mom, predictions_mom, metrics_mom, history_mom, X_scaler_mom, y_scaler_mom = run_transformer(\n",
    "        df_engineered, target_col, test_size=0.2, time_steps=12, epochs=100, batch_size=32\n",
    "    )\n",
    "    \n",
    "    # Generate future forecasts for MoM\n",
    "    forecast_mom = forecast_future_transformer(\n",
    "        model_mom, df_engineered, target_col, X_scaler_mom, y_scaler_mom, time_steps=12, forecast_horizon=24\n",
    "    )\n",
    "    print(f\"\\nFuture {target_col} forecasts:\")\n",
    "    print(forecast_mom)\n",
    "    \n",
    "    # Compare with other deep learning models (if available)\n",
    "    try:\n",
    "        # Try to load metrics from other models\n",
    "        lstm_metrics = pd.read_csv('plots/lstm_metrics.csv')\n",
    "        gru_metrics = pd.read_csv('plots/gru_metrics.csv')\n",
    "        tcn_metrics = pd.read_csv('plots/tcn_metrics.csv')\n",
    "        \n",
    "        # Create Transformer metrics dataframe\n",
    "        transformer_metrics = pd.DataFrame([\n",
    "            {\n",
    "                'Model': 'Transformer', 'Target': 'CPI MoM',\n",
    "                'Train_RMSE': metrics_mom['train_rmse'], 'Train_MAE': metrics_mom['train_mae'], \n",
    "                'Train_MAPE': metrics_mom['train_mape'], 'Train_R2': metrics_mom['train_r2'],\n",
    "                'Test_RMSE': metrics_mom['test_rmse'], 'Test_MAE': metrics_mom['test_mae'], \n",
    "                'Test_MAPE': metrics_mom['test_mape'], 'Test_R2': metrics_mom['test_r2']\n",
    "            },\n",
    "            {\n",
    "                'Model': 'Transformer', 'Target': 'CPI YoY',\n",
    "                'Train_RMSE': metrics_yoy['train_rmse'], 'Train_MAE': metrics_yoy['train_mae'], \n",
    "                'Train_MAPE': metrics_yoy['train_mape'], 'Train_R2': metrics_yoy['train_r2'],\n",
    "                'Test_RMSE': metrics_yoy['test_rmse'], 'Test_MAE': metrics_yoy['test_mae'], \n",
    "                'Test_MAPE': metrics_yoy['test_mape'], 'Test_R2': metrics_yoy['test_r2']\n",
    "            }\n",
    "        ])\n",
    "        \n",
    "        # Combine metrics\n",
    "        combined_metrics = pd.concat([lstm_metrics, gru_metrics, tcn_metrics, transformer_metrics])\n",
    "        combined_metrics.to_csv('plots/all_models_comparison_metrics.csv', index=False)\n",
    "        print(\"\\nComparison with other deep learning models saved to plots/all_models_comparison_metrics.csv\")\n",
    "        \n",
    "        # Create comparison plots\n",
    "        for target, target_name in [('CPI MoM', 'cpi_mom'), ('CPI YoY', 'cpi_yoy')]:\n",
    "            # Filter metrics for the target\n",
    "            target_metrics = combined_metrics[combined_metrics['Target'] == target]\n",
    "            \n",
    "            # Compare test metrics\n",
    "            metrics_to_compare = ['Test_RMSE', 'Test_MAE', 'Test_MAPE']\n",
    "            \n",
    "            plt.figure(figsize=(14, 7))\n",
    "            \n",
    "            # Create grouped bar chart\n",
    "            x = np.arange(len(metrics_to_compare))\n",
    "            width = 0.2\n",
    "            \n",
    "            # Plot bars for each model\n",
    "            models = ['LSTM', 'GRU', 'TCN', 'Transformer']\n",
    "            for i, model_name in enumerate(models):\n",
    "                model_data = target_metrics[target_metrics['Model'] == model_name]\n",
    "                if not model_data.empty:\n",
    "                    values = [model_data[metric].values[0] for metric in metrics_to_compare]\n",
    "                    plt.bar(x + (i-1.5)*width, values, width, label=model_name)\n",
    "            \n",
    "            plt.xlabel('Metric')\n",
    "            plt.ylabel('Value')\n",
    "            plt.title(f'Deep Learning Models Comparison for {target}')\n",
    "            plt.xticks(x, metrics_to_compare)\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'plots/all_models_comparison_{target_name}.png')\n",
    "            plt.close()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Could not compare with other models: {e}\")\n",
    "        # Save metrics to CSV\n",
    "        metrics_data = [\n",
    "            {\n",
    "                'Model': 'Transformer', 'Target': 'CPI MoM',\n",
    "                'Train_RMSE': metrics_mom['train_rmse'], 'Train_MAE': metrics_mom['train_mae'], \n",
    "                'Train_MAPE': metrics_mom['train_mape'], 'Train_R2': metrics_mom['train_r2'],\n",
    "                'Test_RMSE': metrics_mom['test_rmse'], 'Test_MAE': metrics_mom['test_mae'], \n",
    "                'Test_MAPE': metrics_mom['test_mape'], 'Test_R2': metrics_mom['test_r2']\n",
    "            },\n",
    "            {\n",
    "                'Model': 'Transformer', 'Target': 'CPI YoY',\n",
    "                'Train_RMSE': metrics_yoy['train_rmse'], 'Train_MAE': metrics_yoy['train_mae'], \n",
    "                'Train_MAPE': metrics_yoy['train_mape'], 'Train_R2': metrics_yoy['train_r2'],\n",
    "                'Test_RMSE': metrics_yoy['test_rmse'], 'Test_MAE': metrics_yoy['test_mae'], \n",
    "                'Test_MAPE': metrics_yoy['test_mape'], 'Test_R2': metrics_yoy['test_r2']\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        metrics_df = pd.DataFrame(metrics_data)\n",
    "        metrics_df.to_csv('plots/transformer_metrics.csv', index=False)\n",
    "        print(\"\\nMetrics saved to plots/transformer_metrics.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ee5f8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running TCN model for cpi_yoy...\n",
      "Training data: (260, 12, 63), Test data: (65, 12, 63)\n",
      "Training TCN model...\n",
      "Epoch 1/100\n",
      "7/7 [==============================] - 7s 277ms/step - loss: 0.1371 - val_loss: 0.1458 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 1s 109ms/step - loss: 0.0617 - val_loss: 0.2297 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 0.0490 - val_loss: 0.1238 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 1s 112ms/step - loss: 0.0439 - val_loss: 0.0951 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0306 - val_loss: 0.1075 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0217 - val_loss: 0.1214 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 0.0231 - val_loss: 0.0570 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0221 - val_loss: 0.1008 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 1s 125ms/step - loss: 0.0170 - val_loss: 0.0547 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 1s 103ms/step - loss: 0.0161 - val_loss: 0.0917 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 1s 117ms/step - loss: 0.0148 - val_loss: 0.0442 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0139 - val_loss: 0.0680 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 1s 82ms/step - loss: 0.0118 - val_loss: 0.0488 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 1s 103ms/step - loss: 0.0112 - val_loss: 0.0245 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0137 - val_loss: 0.0447 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0145 - val_loss: 0.0285 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0137 - val_loss: 0.0350 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 1s 115ms/step - loss: 0.0127 - val_loss: 0.0122 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0122 - val_loss: 0.0326 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0101 - val_loss: 0.0224 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 1s 80ms/step - loss: 0.0097 - val_loss: 0.0187 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0091 - val_loss: 0.0199 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.0143 - val_loss: 0.0265 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 0.0102 - val_loss: 0.0141 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 1s 74ms/step - loss: 0.0092 - val_loss: 0.0135 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 1s 77ms/step - loss: 0.0082 - val_loss: 0.0199 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 0.0099 - val_loss: 0.0192 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 0.0068 - val_loss: 0.0044 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 0s 73ms/step - loss: 0.0096 - val_loss: 0.0191 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 1s 80ms/step - loss: 0.0087 - val_loss: 0.0064 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 1s 77ms/step - loss: 0.0085 - val_loss: 0.0108 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0077 - val_loss: 0.0072 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 1s 82ms/step - loss: 0.0059 - val_loss: 0.0083 - lr: 0.0010\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0062 - val_loss: 0.0090 - lr: 0.0010\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 1s 76ms/step - loss: 0.0070 - val_loss: 0.0058 - lr: 0.0010\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 0s 74ms/step - loss: 0.0047 - val_loss: 0.0104 - lr: 0.0010\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 0.0066 - val_loss: 0.0053 - lr: 0.0010\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0058 - val_loss: 0.0084 - lr: 0.0010\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.0080 - val_loss: 0.0094 - lr: 5.0000e-04\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.0057 - val_loss: 0.0068 - lr: 5.0000e-04\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 1s 122ms/step - loss: 0.0036 - val_loss: 0.0030 - lr: 5.0000e-04\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 1s 75ms/step - loss: 0.0052 - val_loss: 0.0033 - lr: 5.0000e-04\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.0042 - val_loss: 0.0052 - lr: 5.0000e-04\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 1s 76ms/step - loss: 0.0051 - val_loss: 0.0053 - lr: 5.0000e-04\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 1s 81ms/step - loss: 0.0052 - val_loss: 0.0030 - lr: 5.0000e-04\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 1s 120ms/step - loss: 0.0059 - val_loss: 0.0025 - lr: 5.0000e-04\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 1s 76ms/step - loss: 0.0053 - val_loss: 0.0039 - lr: 5.0000e-04\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 0.0045 - val_loss: 0.0032 - lr: 5.0000e-04\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 0.0048 - val_loss: 0.0025 - lr: 5.0000e-04\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 0.0038 - val_loss: 0.0027 - lr: 5.0000e-04\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 1s 114ms/step - loss: 0.0064 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0035 - val_loss: 0.0032 - lr: 5.0000e-04\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.0048 - val_loss: 0.0027 - lr: 5.0000e-04\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.0044 - val_loss: 0.0025 - lr: 5.0000e-04\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.0043 - val_loss: 0.0031 - lr: 5.0000e-04\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.0041 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 1s 116ms/step - loss: 0.0051 - val_loss: 0.0016 - lr: 5.0000e-04\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 1s 138ms/step - loss: 0.0050 - val_loss: 0.0012 - lr: 5.0000e-04\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.0072 - val_loss: 0.0015 - lr: 5.0000e-04\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0037 - val_loss: 0.0015 - lr: 5.0000e-04\n",
      "Epoch 61/100\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.0046 - val_loss: 0.0017 - lr: 5.0000e-04\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0036 - val_loss: 0.0017 - lr: 5.0000e-04\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0048 - val_loss: 0.0014 - lr: 5.0000e-04\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 0.0046 - val_loss: 0.0019 - lr: 5.0000e-04\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - 0s 59ms/step - loss: 0.0050 - val_loss: 0.0012 - lr: 5.0000e-04\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0033 - val_loss: 0.0013 - lr: 5.0000e-04\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 0.0039 - val_loss: 0.0012 - lr: 5.0000e-04\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0030 - val_loss: 9.4238e-04 - lr: 5.0000e-04\n",
      "Epoch 69/100\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.0041 - val_loss: 0.0015 - lr: 5.0000e-04\n",
      "Epoch 70/100\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0043 - val_loss: 0.0018 - lr: 5.0000e-04\n",
      "Epoch 71/100\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0032 - val_loss: 0.0011 - lr: 5.0000e-04\n",
      "Epoch 72/100\n",
      "7/7 [==============================] - 0s 40ms/step - loss: 0.0033 - val_loss: 0.0011 - lr: 5.0000e-04\n",
      "Epoch 73/100\n",
      "7/7 [==============================] - 0s 67ms/step - loss: 0.0033 - val_loss: 0.0011 - lr: 5.0000e-04\n",
      "Epoch 74/100\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0037 - val_loss: 0.0012 - lr: 5.0000e-04\n",
      "Epoch 75/100\n",
      "7/7 [==============================] - 1s 142ms/step - loss: 0.0047 - val_loss: 9.7453e-04 - lr: 5.0000e-04\n",
      "Epoch 76/100\n",
      "7/7 [==============================] - 1s 108ms/step - loss: 0.0054 - val_loss: 0.0013 - lr: 5.0000e-04\n",
      "Epoch 77/100\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 0.0036 - val_loss: 0.0011 - lr: 5.0000e-04\n",
      "Epoch 78/100\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0033 - val_loss: 9.7231e-04 - lr: 5.0000e-04\n",
      "Epoch 79/100\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 0.0034 - val_loss: 0.0011 - lr: 2.5000e-04\n",
      "Epoch 80/100\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 0.0038 - val_loss: 0.0012 - lr: 2.5000e-04\n",
      "Epoch 81/100\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0027 - val_loss: 9.9834e-04 - lr: 2.5000e-04\n",
      "Epoch 82/100\n",
      "7/7 [==============================] - 0s 55ms/step - loss: 0.0052 - val_loss: 0.0011 - lr: 2.5000e-04\n",
      "Epoch 83/100\n",
      "7/7 [==============================] - 0s 54ms/step - loss: 0.0027 - val_loss: 9.7443e-04 - lr: 2.5000e-04\n",
      "Epoch 84/100\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.0030 - val_loss: 9.2726e-04 - lr: 2.5000e-04\n",
      "Epoch 85/100\n",
      "7/7 [==============================] - 0s 53ms/step - loss: 0.0040 - val_loss: 0.0010 - lr: 2.5000e-04\n",
      "Epoch 86/100\n",
      "7/7 [==============================] - 0s 41ms/step - loss: 0.0029 - val_loss: 0.0010 - lr: 2.5000e-04\n",
      "Epoch 87/100\n",
      "7/7 [==============================] - 1s 77ms/step - loss: 0.0026 - val_loss: 8.1713e-04 - lr: 2.5000e-04\n",
      "Epoch 88/100\n",
      "7/7 [==============================] - 0s 57ms/step - loss: 0.0033 - val_loss: 7.9197e-04 - lr: 2.5000e-04\n",
      "Epoch 89/100\n",
      "7/7 [==============================] - 0s 60ms/step - loss: 0.0033 - val_loss: 8.1013e-04 - lr: 2.5000e-04\n",
      "Epoch 90/100\n",
      "7/7 [==============================] - 0s 41ms/step - loss: 0.0042 - val_loss: 8.4336e-04 - lr: 2.5000e-04\n",
      "Epoch 91/100\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0028 - val_loss: 8.3923e-04 - lr: 2.5000e-04\n",
      "Epoch 92/100\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 0.0037 - val_loss: 7.6673e-04 - lr: 2.5000e-04\n",
      "Epoch 93/100\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 0.0031 - val_loss: 9.3607e-04 - lr: 2.5000e-04\n",
      "Epoch 94/100\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0047 - val_loss: 0.0014 - lr: 2.5000e-04\n",
      "Epoch 95/100\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 0.0030 - val_loss: 8.5413e-04 - lr: 2.5000e-04\n",
      "Epoch 96/100\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0026 - val_loss: 7.7267e-04 - lr: 2.5000e-04\n",
      "Epoch 97/100\n",
      "7/7 [==============================] - 1s 81ms/step - loss: 0.0032 - val_loss: 7.9355e-04 - lr: 2.5000e-04\n",
      "Epoch 98/100\n",
      "7/7 [==============================] - 0s 70ms/step - loss: 0.0034 - val_loss: 8.1492e-04 - lr: 1.2500e-04\n",
      "Epoch 99/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.0031 - val_loss: 9.0632e-04 - lr: 1.2500e-04\n",
      "Epoch 100/100\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0031 - val_loss: 9.0169e-04 - lr: 1.2500e-04\n",
      "9/9 [==============================] - 1s 5ms/step\n",
      "3/3 [==============================] - 0s 7ms/step\n",
      "Training - RMSE: 1.5776, MAE: 1.2876, MAPE: 1.19%, R²: 0.9301\n",
      "Testing - RMSE: 2.0399, MAE: 1.5143, MAPE: 1.48%, R²: -1.7041\n",
      "Generating 24 period forecast...\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "\n",
      "Future cpi_yoy forecasts:\n",
      "2025-01-01    103.722816\n",
      "2025-02-01    103.799942\n",
      "2025-03-01    103.634880\n",
      "2025-04-01    104.306717\n",
      "2025-05-01    104.087822\n",
      "2025-06-01    104.369019\n",
      "2025-07-01    104.612839\n",
      "2025-08-01    104.995270\n",
      "2025-09-01    105.574272\n",
      "2025-10-01    105.442848\n",
      "2025-11-01    105.425926\n",
      "2025-12-01    105.462852\n",
      "2026-01-01    105.462852\n",
      "2026-02-01    105.462852\n",
      "2026-03-01    105.462852\n",
      "2026-04-01    105.462852\n",
      "2026-05-01    105.462852\n",
      "2026-06-01    105.462852\n",
      "2026-07-01    105.462852\n",
      "2026-08-01    105.462852\n",
      "2026-09-01    105.462852\n",
      "2026-10-01    105.462852\n",
      "2026-11-01    105.462852\n",
      "2026-12-01    105.462852\n",
      "Freq: MS, dtype: float32\n",
      "Running TCN model for cpi_mom...\n",
      "Training data: (260, 12, 63), Test data: (65, 12, 63)\n",
      "Training TCN model...\n",
      "Epoch 1/100\n",
      "7/7 [==============================] - 6s 187ms/step - loss: 0.1172 - val_loss: 0.0919 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 1s 120ms/step - loss: 0.0572 - val_loss: 0.0311 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0548 - val_loss: 0.0763 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.0391 - val_loss: 0.0791 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 1s 80ms/step - loss: 0.0292 - val_loss: 0.0324 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 1s 76ms/step - loss: 0.0293 - val_loss: 0.0319 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.0219 - val_loss: 0.0221 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0229 - val_loss: 0.0217 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 0.0244 - val_loss: 0.0197 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0187 - val_loss: 0.0188 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 1s 80ms/step - loss: 0.0192 - val_loss: 0.0203 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 1s 80ms/step - loss: 0.0231 - val_loss: 0.0202 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 1s 76ms/step - loss: 0.0160 - val_loss: 0.0193 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 0.0197 - val_loss: 0.0199 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 0s 71ms/step - loss: 0.0158 - val_loss: 0.0198 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 1s 77ms/step - loss: 0.0167 - val_loss: 0.0217 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 1s 77ms/step - loss: 0.0171 - val_loss: 0.0271 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 1s 116ms/step - loss: 0.0189 - val_loss: 0.0177 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.0124 - val_loss: 0.0165 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.0163 - val_loss: 0.0205 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.0144 - val_loss: 0.0239 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 0s 57ms/step - loss: 0.0159 - val_loss: 0.0233 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 0.0160 - val_loss: 0.0213 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 0s 69ms/step - loss: 0.0129 - val_loss: 0.0218 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 0.0117 - val_loss: 0.0225 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0130 - val_loss: 0.0271 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0125 - val_loss: 0.0273 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 0.0111 - val_loss: 0.0305 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 1s 81ms/step - loss: 0.0153 - val_loss: 0.0299 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 0s 67ms/step - loss: 0.0121 - val_loss: 0.0328 - lr: 5.0000e-04\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 0s 59ms/step - loss: 0.0141 - val_loss: 0.0304 - lr: 5.0000e-04\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 0s 67ms/step - loss: 0.0104 - val_loss: 0.0301 - lr: 5.0000e-04\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0101 - val_loss: 0.0373 - lr: 5.0000e-04\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.0109 - val_loss: 0.0327 - lr: 5.0000e-04\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 0.0113 - val_loss: 0.0285 - lr: 5.0000e-04\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 1s 81ms/step - loss: 0.0116 - val_loss: 0.0293 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.0113 - val_loss: 0.0307 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0111 - val_loss: 0.0296 - lr: 5.0000e-04\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.0098 - val_loss: 0.0265 - lr: 5.0000e-04\n",
      "9/9 [==============================] - 1s 6ms/step\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "Training - RMSE: 0.9902, MAE: 0.8634, MAPE: 0.86%, R²: -0.5097\n",
      "Testing - RMSE: 0.6962, MAE: 0.5173, MAPE: 0.52%, R²: -1.0794\n",
      "Generating 24 period forecast...\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "\n",
      "Future cpi_mom forecasts:\n",
      "2025-01-01    99.726822\n",
      "2025-02-01    99.596558\n",
      "2025-03-01    99.207687\n",
      "2025-04-01    99.230293\n",
      "2025-05-01    99.164696\n",
      "2025-06-01    99.132019\n",
      "2025-07-01    99.220718\n",
      "2025-08-01    99.019638\n",
      "2025-09-01    98.942871\n",
      "2025-10-01    98.774887\n",
      "2025-11-01    98.671791\n",
      "2025-12-01    98.674171\n",
      "2026-01-01    98.674171\n",
      "2026-02-01    98.674171\n",
      "2026-03-01    98.674171\n",
      "2026-04-01    98.674171\n",
      "2026-05-01    98.674171\n",
      "2026-06-01    98.674171\n",
      "2026-07-01    98.674171\n",
      "2026-08-01    98.674171\n",
      "2026-09-01    98.674171\n",
      "2026-10-01    98.674171\n",
      "2026-11-01    98.674171\n",
      "2026-12-01    98.674171\n",
      "Freq: MS, dtype: float32\n",
      "Could not compare with other models: [Errno 2] No such file or directory: 'plots/gru_metrics.csv'\n",
      "\n",
      "Metrics saved to plots/tcn_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import Add, Dropout, Lambda, Reshape\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load and prepare the time series data\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert time column to datetime\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    \n",
    "    # Set time as index\n",
    "    df.set_index('time', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"Create additional features for modeling\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Create lag features\n",
    "    for i in range(1, 13):\n",
    "        if 'cpi_mom' in df_copy.columns:\n",
    "            df_copy[f'cpi_mom_lag_{i}'] = df_copy['cpi_mom'].shift(i)\n",
    "        if 'cpi_yoy' in df_copy.columns:\n",
    "            df_copy[f'cpi_yoy_lag_{i}'] = df_copy['cpi_yoy'].shift(i)\n",
    "    \n",
    "    # Create rolling window features\n",
    "    for window in [3, 6, 12]:\n",
    "        if 'cpi_mom' in df_copy.columns:\n",
    "            df_copy[f'cpi_mom_rolling_mean_{window}'] = df_copy['cpi_mom'].rolling(window=window).mean()\n",
    "            df_copy[f'cpi_mom_rolling_std_{window}'] = df_copy['cpi_mom'].rolling(window=window).std()\n",
    "        if 'cpi_yoy' in df_copy.columns:\n",
    "            df_copy[f'cpi_yoy_rolling_mean_{window}'] = df_copy['cpi_yoy'].rolling(window=window).mean()\n",
    "            df_copy[f'cpi_yoy_rolling_std_{window}'] = df_copy['cpi_yoy'].rolling(window=window).std()\n",
    "    \n",
    "    # Create economic indicator lag features\n",
    "    for i in range(1, 4):\n",
    "        if 'oil_price' in df_copy.columns:\n",
    "            df_copy[f'oil_price_lag_{i}'] = df_copy['oil_price'].shift(i)\n",
    "        if 'gold_price' in df_copy.columns:\n",
    "            df_copy[f'gold_price_lag_{i}'] = df_copy['gold_price'].shift(i)\n",
    "        if 'interest_rate' in df_copy.columns:\n",
    "            df_copy[f'interest_rate_lag_{i}'] = df_copy['interest_rate'].shift(i)\n",
    "    \n",
    "    # Add month and year as cyclical features\n",
    "    if 'month' in df_copy.columns:\n",
    "        df_copy['month_sin'] = np.sin(2 * np.pi * df_copy['month']/12)\n",
    "        df_copy['month_cos'] = np.cos(2 * np.pi * df_copy['month']/12)\n",
    "    \n",
    "    # Create interaction features\n",
    "    if all(col in df_copy.columns for col in ['oil_price', 'gold_price']):\n",
    "        df_copy['oil_gold_ratio'] = df_copy['oil_price'] / df_copy['gold_price']\n",
    "    \n",
    "    # Drop rows with NaN values (due to lag features)\n",
    "    df_clean = df_copy.dropna()\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def calculate_metrics(actual, predicted):\n",
    "    \"\"\"Calculate evaluation metrics\"\"\"\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    r2 = r2_score(actual, predicted)\n",
    "    mape = np.mean(np.abs((actual - predicted) / actual)) * 100\n",
    "    \n",
    "    return rmse, mae, mape, r2\n",
    "\n",
    "def create_sequences(X, y, time_steps=12):\n",
    "    \"\"\"\n",
    "    Create sequences for TCN input\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : np.array\n",
    "        Feature array\n",
    "    y : np.array\n",
    "        Target array\n",
    "    time_steps : int\n",
    "        Number of time steps to use for each sequence\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        X_seq, y_seq arrays with shape [samples, time_steps, features] and [samples]\n",
    "    \"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        X_seq.append(X[i:i + time_steps])\n",
    "        y_seq.append(y[i + time_steps])\n",
    "    \n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "def residual_block(x, dilation_rate, nb_filters, kernel_size, dropout_rate=0.2):\n",
    "    \"\"\"\n",
    "    Defines the residual block for the TCN\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : tensor\n",
    "        Input tensor\n",
    "    dilation_rate : int\n",
    "        Dilation rate for the dilated convolution\n",
    "    nb_filters : int\n",
    "        Number of filters in the convolution\n",
    "    kernel_size : int\n",
    "        Size of the kernel\n",
    "    dropout_rate : float\n",
    "        Dropout rate\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tensor\n",
    "        Output tensor\n",
    "    \"\"\"\n",
    "    prev_x = x\n",
    "    \n",
    "    # First dilated convolution\n",
    "    x = Conv1D(filters=nb_filters, kernel_size=kernel_size, \n",
    "               dilation_rate=dilation_rate, padding='causal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Second dilated convolution\n",
    "    x = Conv1D(filters=nb_filters, kernel_size=kernel_size,\n",
    "               dilation_rate=dilation_rate, padding='causal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # If the input shape is different from the output shape, use a 1x1 conv to match dimensions\n",
    "    if prev_x.shape[-1] != nb_filters:\n",
    "        prev_x = Conv1D(nb_filters, kernel_size=1)(prev_x)\n",
    "    \n",
    "    # Add skip connection\n",
    "    res = Add()([prev_x, x])\n",
    "    \n",
    "    return res\n",
    "\n",
    "def build_tcn_model(input_shape, nb_filters=64, kernel_size=3, nb_stacks=1, dilations=None, \n",
    "                   dropout_rate=0.2, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Build a TCN model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_shape : tuple\n",
    "        Shape of input data (time_steps, features)\n",
    "    nb_filters : int\n",
    "        Number of filters in the convolution\n",
    "    kernel_size : int\n",
    "        Size of the kernel\n",
    "    nb_stacks : int\n",
    "        Number of stacks of residual blocks\n",
    "    dilations : list\n",
    "        List of dilations for each residual block\n",
    "    dropout_rate : float\n",
    "        Dropout rate\n",
    "    learning_rate : float\n",
    "        Learning rate for optimizer\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tf.keras.Model\n",
    "        Compiled TCN model\n",
    "    \"\"\"\n",
    "    if dilations is None:\n",
    "        dilations = [1, 2, 4, 8, 16, 32]\n",
    "    \n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "    \n",
    "    # Create TCN architecture\n",
    "    for stack in range(nb_stacks):\n",
    "        for dilation_rate in dilations:\n",
    "            x = residual_block(x, dilation_rate, nb_filters, kernel_size, dropout_rate)\n",
    "    \n",
    "    # Apply a final convolution to get the output\n",
    "    x = Conv1D(filters=nb_filters, kernel_size=1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # Global average pooling to get a single vector\n",
    "    x = Lambda(lambda z: tf.reduce_mean(z, axis=1))(x)\n",
    "    \n",
    "    # Output layer\n",
    "    output_layer = Dense(1)(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='mse'\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def run_tcn(df, target_col, test_size=0.2, time_steps=12, epochs=100, batch_size=32):\n",
    "    \"\"\"\n",
    "    Train and evaluate a TCN model for time series forecasting\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with engineered features\n",
    "    target_col : str\n",
    "        Name of the target column\n",
    "    test_size : float\n",
    "        Proportion of data to use for testing\n",
    "    time_steps : int\n",
    "        Number of time steps to use for each sequence\n",
    "    epochs : int\n",
    "        Number of training epochs\n",
    "    batch_size : int\n",
    "        Batch size for training\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        Model, predictions, metrics, history\n",
    "    \"\"\"\n",
    "    print(f\"Running TCN model for {target_col}...\")\n",
    "    \n",
    "    # Define features and target\n",
    "    X = df.drop([col for col in ['cpi_mom', 'cpi_yoy', target_col] if col in df.columns], axis=1).values\n",
    "    y = df[target_col].values\n",
    "    \n",
    "    # Scale features and target\n",
    "    X_scaler = StandardScaler()\n",
    "    X_scaled = X_scaler.fit_transform(X)\n",
    "    \n",
    "    y_scaler = MinMaxScaler()\n",
    "    y_scaled = y_scaler.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Create sequences\n",
    "    X_seq, y_seq = create_sequences(X_scaled, y_scaled, time_steps)\n",
    "    \n",
    "    # Split data into training and testing sets (time-based split)\n",
    "    split_idx = int(len(X_seq) * (1 - test_size))\n",
    "    X_train, X_test = X_seq[:split_idx], X_seq[split_idx:]\n",
    "    y_train, y_test = y_seq[:split_idx], y_seq[split_idx:]\n",
    "    \n",
    "    print(f\"Training data: {X_train.shape}, Test data: {X_test.shape}\")\n",
    "    \n",
    "    # Build model\n",
    "    model = build_tcn_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        nb_filters=64,\n",
    "        kernel_size=3,\n",
    "        nb_stacks=1,\n",
    "        dilations=[1, 2, 4, 8, 16],\n",
    "        dropout_rate=0.2,\n",
    "        learning_rate=0.001\n",
    "    )\n",
    "    \n",
    "    # Define callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=0.0001),\n",
    "        ModelCheckpoint(f'models/tcn_{target_col}.h5', monitor='val_loss', save_best_only=True)\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training TCN model...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train_scaled = model.predict(X_train)\n",
    "    y_pred_test_scaled = model.predict(X_test)\n",
    "    \n",
    "    # Inverse transform predictions\n",
    "    y_pred_train = y_scaler.inverse_transform(y_pred_train_scaled).flatten()\n",
    "    y_pred_test = y_scaler.inverse_transform(y_pred_test_scaled).flatten()\n",
    "    \n",
    "    # Inverse transform actual values\n",
    "    y_train_actual = y_scaler.inverse_transform(y_train.reshape(-1, 1)).flatten()\n",
    "    y_test_actual = y_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_rmse, train_mae, train_mape, train_r2 = calculate_metrics(y_train_actual, y_pred_train)\n",
    "    test_rmse, test_mae, test_mape, test_r2 = calculate_metrics(y_test_actual, y_pred_test)\n",
    "    \n",
    "    print(f\"Training - RMSE: {train_rmse:.4f}, MAE: {train_mae:.4f}, MAPE: {train_mape:.2f}%, R²: {train_r2:.4f}\")\n",
    "    print(f\"Testing - RMSE: {test_rmse:.4f}, MAE: {test_mae:.4f}, MAPE: {test_mape:.2f}%, R²: {test_r2:.4f}\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'TCN Training History for {target_col}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/tcn_training_history_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot actual vs predicted\n",
    "    # Get the original dates for the test set\n",
    "    test_dates = df.index[time_steps+split_idx:time_steps+len(X_seq)]\n",
    "    \n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(test_dates, y_test_actual, label='Actual', color='blue')\n",
    "    plt.plot(test_dates, y_pred_test, label='Predicted', color='red', linestyle='--')\n",
    "    plt.title(f'TCN: Actual vs Predicted {target_col}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(target_col)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/tcn_predictions_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create a DataFrame with predictions\n",
    "    predictions = pd.DataFrame({\n",
    "        'Actual': y_test_actual,\n",
    "        'Predicted': y_pred_test,\n",
    "        'Error': y_test_actual - y_pred_test\n",
    "    }, index=test_dates)\n",
    "    \n",
    "    # Plot error distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(predictions['Error'], kde=True)\n",
    "    plt.title(f'Error Distribution for {target_col}')\n",
    "    plt.xlabel('Error')\n",
    "    plt.savefig(f'plots/tcn_error_distribution_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Return results\n",
    "    metrics = {\n",
    "        'train_rmse': train_rmse,\n",
    "        'train_mae': train_mae,\n",
    "        'train_mape': train_mape,\n",
    "        'train_r2': train_r2,\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_mae': test_mae,\n",
    "        'test_mape': test_mape,\n",
    "        'test_r2': test_r2\n",
    "    }\n",
    "    \n",
    "    return model, predictions, metrics, history, X_scaler, y_scaler\n",
    "\n",
    "def forecast_future_tcn(model, df, target_col, X_scaler, y_scaler, time_steps=12, forecast_horizon=24):\n",
    "    \"\"\"\n",
    "    Generate future forecasts using the trained TCN model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : tf.keras.Model\n",
    "        Trained TCN model\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with features\n",
    "    target_col : str\n",
    "        Name of the target column\n",
    "    X_scaler : StandardScaler\n",
    "        Scaler used for features\n",
    "    y_scaler : MinMaxScaler\n",
    "        Scaler used for target\n",
    "    time_steps : int\n",
    "        Number of time steps used for training\n",
    "    forecast_horizon : int\n",
    "        Number of periods to forecast\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        Forecasted values\n",
    "    \"\"\"\n",
    "    print(f\"Generating {forecast_horizon} period forecast...\")\n",
    "    \n",
    "    # Get the last time_steps data points\n",
    "    X_last = df.drop([col for col in ['cpi_mom', 'cpi_yoy', target_col] if col in df.columns], axis=1).values[-time_steps:]\n",
    "    X_last_scaled = X_scaler.transform(X_last)\n",
    "    \n",
    "    # Reshape for TCN input [1, time_steps, features]\n",
    "    X_last_scaled = X_last_scaled.reshape(1, time_steps, X_last_scaled.shape[1])\n",
    "    \n",
    "    # Get the last date in the dataframe\n",
    "    last_date = df.index[-1]\n",
    "    \n",
    "    # Create a list to store forecasts\n",
    "    forecasts = []\n",
    "    \n",
    "    # Generate forecasts recursively\n",
    "    for i in range(forecast_horizon):\n",
    "        # Make prediction\n",
    "        forecast_scaled = model.predict(X_last_scaled)\n",
    "        \n",
    "        # Inverse transform prediction\n",
    "        forecast = y_scaler.inverse_transform(forecast_scaled)[0][0]\n",
    "        \n",
    "        # Store forecast\n",
    "        forecasts.append(forecast)\n",
    "        \n",
    "        # For a more accurate implementation, we would need to update all features\n",
    "        # based on the new forecast, but this is a simplified version\n",
    "        # Here we just shift the input sequence and append the new prediction\n",
    "        X_last_scaled = np.roll(X_last_scaled, -1, axis=1)\n",
    "        X_last_scaled[0, -1, :] = X_last_scaled[0, -2, :]  # Simple copy of the last known features\n",
    "    \n",
    "    # Create a Series with the forecasts\n",
    "    future_dates = pd.date_range(start=last_date + pd.DateOffset(months=1), periods=forecast_horizon, freq='MS')\n",
    "    forecast_series = pd.Series(forecasts, index=future_dates)\n",
    "    \n",
    "    # Plot historical data with forecasts\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df[target_col].index, df[target_col], label='Historical Data')\n",
    "    plt.plot(forecast_series.index, forecast_series, label='Forecast', color='red', linestyle='--')\n",
    "    plt.title(f'TCN: {target_col} Forecast')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(target_col)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/tcn_future_forecast_{target_col}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return forecast_series\n",
    "\n",
    "def main():\n",
    "    # Create plots and models directories if they don't exist\n",
    "    import os\n",
    "    for directory in ['plots', 'models']:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    \n",
    "    # Load data\n",
    "    file_path = 'data/analyzed_time_series.csv'\n",
    "    df = load_data(file_path)\n",
    "    \n",
    "    # Engineer features\n",
    "    df_engineered = engineer_features(df)\n",
    "    \n",
    "    # Run TCN for CPI Year-over-Year\n",
    "    target_col = 'cpi_yoy'\n",
    "    model_yoy, predictions_yoy, metrics_yoy, history_yoy, X_scaler_yoy, y_scaler_yoy = run_tcn(\n",
    "        df_engineered, target_col, test_size=0.2, time_steps=12, epochs=100, batch_size=32\n",
    "    )\n",
    "    \n",
    "    # Generate future forecasts for YoY\n",
    "    forecast_yoy = forecast_future_tcn(\n",
    "        model_yoy, df_engineered, target_col, X_scaler_yoy, y_scaler_yoy, time_steps=12, forecast_horizon=24\n",
    "    )\n",
    "    print(f\"\\nFuture {target_col} forecasts:\")\n",
    "    print(forecast_yoy)\n",
    "    \n",
    "    # Run TCN for CPI Month-over-Month\n",
    "    target_col = 'cpi_mom'\n",
    "    model_mom, predictions_mom, metrics_mom, history_mom, X_scaler_mom, y_scaler_mom = run_tcn(\n",
    "        df_engineered, target_col, test_size=0.2, time_steps=12, epochs=100, batch_size=32\n",
    "    )\n",
    "    \n",
    "    # Generate future forecasts for MoM\n",
    "    forecast_mom = forecast_future_tcn(\n",
    "        model_mom, df_engineered, target_col, X_scaler_mom, y_scaler_mom, time_steps=12, forecast_horizon=24\n",
    "    )\n",
    "    print(f\"\\nFuture {target_col} forecasts:\")\n",
    "    print(forecast_mom)\n",
    "    \n",
    "    # Compare with other deep learning models (if available)\n",
    "    try:\n",
    "        # Try to load metrics from other models\n",
    "        lstm_metrics = pd.read_csv('plots/lstm_metrics.csv')\n",
    "        gru_metrics = pd.read_csv('plots/gru_metrics.csv')\n",
    "        \n",
    "        # Create TCN metrics dataframe\n",
    "        tcn_metrics = pd.DataFrame([\n",
    "            {\n",
    "                'Model': 'TCN', 'Target': 'CPI MoM',\n",
    "                'Train_RMSE': metrics_mom['train_rmse'], 'Train_MAE': metrics_mom['train_mae'], \n",
    "                'Train_MAPE': metrics_mom['train_mape'], 'Train_R2': metrics_mom['train_r2'],\n",
    "                'Test_RMSE': metrics_mom['test_rmse'], 'Test_MAE': metrics_mom['test_mae'], \n",
    "                'Test_MAPE': metrics_mom['test_mape'], 'Test_R2': metrics_mom['test_r2']\n",
    "            },\n",
    "            {\n",
    "                'Model': 'TCN', 'Target': 'CPI YoY',\n",
    "                'Train_RMSE': metrics_yoy['train_rmse'], 'Train_MAE': metrics_yoy['train_mae'], \n",
    "                'Train_MAPE': metrics_yoy['train_mape'], 'Train_R2': metrics_yoy['train_r2'],\n",
    "                'Test_RMSE': metrics_yoy['test_rmse'], 'Test_MAE': metrics_yoy['test_mae'], \n",
    "                'Test_MAPE': metrics_yoy['test_mape'], 'Test_R2': metrics_yoy['test_r2']\n",
    "            }\n",
    "        ])\n",
    "        \n",
    "        # Combine metrics\n",
    "        combined_metrics = pd.concat([lstm_metrics, gru_metrics, tcn_metrics])\n",
    "        combined_metrics.to_csv('plots/deep_learning_comparison_metrics.csv', index=False)\n",
    "        print(\"\\nComparison with other deep learning models saved to plots/deep_learning_comparison_metrics.csv\")\n",
    "        \n",
    "        # Create comparison plots\n",
    "        for target, target_name in [('CPI MoM', 'cpi_mom'), ('CPI YoY', 'cpi_yoy')]:\n",
    "            # Filter metrics for the target\n",
    "            target_metrics = combined_metrics[combined_metrics['Target'] == target]\n",
    "            \n",
    "            # Compare test metrics\n",
    "            metrics_to_compare = ['Test_RMSE', 'Test_MAE', 'Test_MAPE']\n",
    "            \n",
    "            plt.figure(figsize=(12, 6))\n",
    "            \n",
    "            # Create grouped bar chart\n",
    "            x = np.arange(len(metrics_to_compare))\n",
    "            width = 0.25\n",
    "            \n",
    "            # Plot bars for each model\n",
    "            for i, model_name in enumerate(['LSTM', 'GRU', 'TCN']):\n",
    "                model_data = target_metrics[target_metrics['Model'] == model_name]\n",
    "                if not model_data.empty:\n",
    "                    values = [model_data[metric].values[0] for metric in metrics_to_compare]\n",
    "                    plt.bar(x + (i-1)*width, values, width, label=model_name)\n",
    "            \n",
    "            plt.xlabel('Metric')\n",
    "            plt.ylabel('Value')\n",
    "            plt.title(f'Deep Learning Models Comparison for {target}')\n",
    "            plt.xticks(x, metrics_to_compare)\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'plots/deep_learning_comparison_{target_name}.png')\n",
    "            plt.close()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Could not compare with other models: {e}\")\n",
    "        # Save metrics to CSV\n",
    "        metrics_data = [\n",
    "            {\n",
    "                'Model': 'TCN', 'Target': 'CPI MoM',\n",
    "                'Train_RMSE': metrics_mom['train_rmse'], 'Train_MAE': metrics_mom['train_mae'], \n",
    "                'Train_MAPE': metrics_mom['train_mape'], 'Train_R2': metrics_mom['train_r2'],\n",
    "                'Test_RMSE': metrics_mom['test_rmse'], 'Test_MAE': metrics_mom['test_mae'], \n",
    "                'Test_MAPE': metrics_mom['test_mape'], 'Test_R2': metrics_mom['test_r2']\n",
    "            },\n",
    "            {\n",
    "                'Model': 'TCN', 'Target': 'CPI YoY',\n",
    "                'Train_RMSE': metrics_yoy['train_rmse'], 'Train_MAE': metrics_yoy['train_mae'], \n",
    "                'Train_MAPE': metrics_yoy['train_mape'], 'Train_R2': metrics_yoy['train_r2'],\n",
    "                'Test_RMSE': metrics_yoy['test_rmse'], 'Test_MAE': metrics_yoy['test_mae'], \n",
    "                'Test_MAPE': metrics_yoy['test_mape'], 'Test_R2': metrics_yoy['test_r2']\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        metrics_df = pd.DataFrame(metrics_data)\n",
    "        metrics_df.to_csv('plots/tcn_metrics.csv', index=False)\n",
    "        print(\"\\nMetrics saved to plots/tcn_metrics.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
