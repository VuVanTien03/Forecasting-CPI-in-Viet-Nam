{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f5f3ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done 5 out of 5 | elapsed:  9.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Target                  Model      RMSE       MAE      MAPE     sMAPE  \\\n",
      "0    cpi                  ARIMA  0.305111  0.209585  0.208702  0.208942   \n",
      "1    cpi  Exponential Smoothing  0.267943  0.228681  0.228223  0.228172   \n",
      "2    cpi                 SARIMA  0.179295  0.159355  0.158901  0.158919   \n",
      "3    cpi                SARIMAX  0.208344  0.173125  0.172700  0.172629   \n",
      "\n",
      "   NormMAPE     DirAcc      MASE  \n",
      "0  0.002082  63.636364  0.929611  \n",
      "1  0.002277  36.363636  1.014313  \n",
      "2  0.001585  27.272727  0.706817  \n",
      "3  0.001723  27.272727  0.767893  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "import optuna\n",
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from joblib import Parallel, delayed, wrap_non_picklable_objects\n",
    "import time\n",
    "\n",
    "# Disable tqdm monitor to avoid warnings\n",
    "tqdm.monitor_interval = 0\n",
    "\n",
    "# Setup output directory and logging\n",
    "img_dir = 'classical_model_results'\n",
    "os.makedirs(img_dir, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=f'{img_dir}/classical_models_log.txt',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'forecast_horizon': 12,\n",
    "    'seasonal_periods': 12,\n",
    "    'min_data_length': 24,\n",
    "    'img_dir': img_dir,\n",
    "    'results_file': f'{img_dir}/classical_model_results.csv',\n",
    "    'n_jobs': min(4, os.cpu_count()),\n",
    "    'outlier_threshold': 3,\n",
    "    'max_diff': 2,\n",
    "    'lag_features': list(range(1, 7)),\n",
    "    'rolling_windows': [3, 6, 12],\n",
    "    'correlation_threshold': 0.2,\n",
    "    'optuna_trials': 100,\n",
    "}\n",
    "\n",
    "def detect_outliers(series, method='iqr', threshold=CONFIG['outlier_threshold']):\n",
    "    \"\"\"Detect and replace outliers in a series using IQR or Z-score.\"\"\"\n",
    "    if method == 'zscore':\n",
    "        z_scores = np.abs((series - series.mean()) / series.std())\n",
    "        outliers = z_scores > threshold\n",
    "    elif method == 'iqr':\n",
    "        Q1 = series.quantile(0.25)\n",
    "        Q3 = series.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = (series < (Q1 - 1.5 * IQR)) | (series > (Q3 + 1.5 * IQR))\n",
    "    series_clean = series.copy()\n",
    "    series_clean[outliers] = series.interpolate(method='linear').bfill().ffill()\n",
    "    logger.info(f\"Detected {outliers.sum()} outliers in series using {method}\")\n",
    "    return series_clean\n",
    "\n",
    "def validate_input_data(df, required_columns):\n",
    "    \"\"\"Validate input DataFrame for required columns, duplicates, and data types.\"\"\"\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        raise ValueError(f\"Missing required columns: {set(required_columns) - set(df.columns)}\")\n",
    "    if df.index.duplicated().any():\n",
    "        raise ValueError(\"Index contains duplicates!\")\n",
    "    if not df.index.is_monotonic_increasing:\n",
    "        df = df.sort_index()\n",
    "        logger.warning(\"Index was not monotonically increasing, sorted!\")\n",
    "    if df[required_columns].isnull().sum().any():\n",
    "        logger.warning(f\"Missing values in data: {df[required_columns].isnull().sum().to_dict()}\")\n",
    "        df[required_columns] = df[required_columns].interpolate(method='linear').bfill().ffill()\n",
    "    if df[required_columns].replace([np.inf, -np.inf], np.nan).isnull().sum().any():\n",
    "        raise ValueError(\"Data contains infinite values!\")\n",
    "    if not all(df[required_columns].dtypes.apply(lambda x: np.issubdtype(x, np.number))):\n",
    "        raise ValueError(\"Some columns are not numeric!\")\n",
    "    return df\n",
    "\n",
    "def check_stationarity(series, name):\n",
    "    \"\"\"Check stationarity using ADF and KPSS tests, return differencing order.\"\"\"\n",
    "    max_diff = CONFIG['max_diff']\n",
    "    series_clean = series.dropna().replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if len(series_clean) < 2:\n",
    "        logger.error(f\"{name}: Data too short after cleaning!\")\n",
    "        return 0\n",
    "    d = 0\n",
    "    while d <= max_diff:\n",
    "        adf_result = adfuller(series_clean)\n",
    "        kpss_result = kpss(series_clean, regression='c', nlags=\"auto\")\n",
    "        logger.info(f\"{name} (d={d}): ADF p-value={adf_result[1]:.4f}, KPSS p-value={kpss_result[1]:.4f}\")\n",
    "        if adf_result[1] < 0.05 and kpss_result[1] > 0.1:\n",
    "            logger.info(f\"{name} stationary at differencing order d={d}\")\n",
    "            return d\n",
    "        if d == max_diff:\n",
    "            logger.warning(f\"{name} not stationary after {max_diff} differencing. Using d={d}.\")\n",
    "            return d\n",
    "        series_clean = series_clean.diff().dropna()\n",
    "        if len(series_clean) < 2:\n",
    "            logger.warning(f\"{name}: Data too short after differencing {d+1}!\")\n",
    "            return d\n",
    "        d += 1\n",
    "    return d\n",
    "\n",
    "def create_features(data, target, lags=CONFIG['lag_features'], rolling_windows=CONFIG['rolling_windows']):\n",
    "    \"\"\"Create lagged, rolling, and seasonal features.\"\"\"\n",
    "    logger.info(f\"Creating features for {target}, data shape: {data.shape}\")\n",
    "    if len(data) < CONFIG['min_data_length']:\n",
    "        raise ValueError(f\"Data too short: {len(data)} rows\")\n",
    "    if not isinstance(data.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"DataFrame index must be DatetimeIndex\")\n",
    "    \n",
    "    df = data.copy()\n",
    "    exog_vars = ['oil_price', 'gold_price']\n",
    "    required_cols = [target] + exog_vars\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        raise ValueError(f\"Missing columns: {required_cols}\")\n",
    "    \n",
    "    # Handle missing values with interpolation\n",
    "    df[required_cols] = df[required_cols].interpolate(method='linear').bfill().ffill()\n",
    "    \n",
    "    # Create lag and rolling features\n",
    "    for col in required_cols:\n",
    "        for lag in lags:\n",
    "            df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n",
    "        for window in rolling_windows:\n",
    "            df[f'{col}_roll_mean_{window}'] = df[col].rolling(window=window, min_periods=1).mean()\n",
    "            df[f'{col}_roll_std_{window}'] = df[col].rolling(window=window, min_periods=1).std()\n",
    "    \n",
    "    # Add seasonal features\n",
    "    df['month'] = df.index.month\n",
    "    df = pd.get_dummies(df, columns=['month'], prefix='month', dtype=float)  # Ensure numeric dummies\n",
    "    period = CONFIG['seasonal_periods']\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df.index.month / period)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df.index.month / period)\n",
    "    df['quarter'] = df.index.quarter.astype(float)  # Ensure quarter is numeric\n",
    "    \n",
    "    # Drop columns with all NaN and fill remaining NaN\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    df = df.interpolate(method='linear').bfill().ffill()\n",
    "    if df.isnull().any().any():\n",
    "        raise ValueError(f\"Data still contains NaN: {df.isnull().sum().to_dict()}\")\n",
    "    \n",
    "    # Select only numeric columns for feature selection, excluding target\n",
    "    feature_cols = [col for col in df.columns if col != target and df[col].dtype in [np.float64, np.float32, np.int64, np.int32]]\n",
    "    logger.info(f\"Feature columns before RFE: {feature_cols}\")\n",
    "    \n",
    "    # Log non-numeric columns for debugging\n",
    "    non_numeric_cols = [col for col in df.columns if col not in feature_cols and col != target]\n",
    "    if non_numeric_cols:\n",
    "        logger.warning(f\"Non-numeric columns excluded from features: {non_numeric_cols}\")\n",
    "    \n",
    "    X = df[feature_cols].dropna()\n",
    "    y = df[target].loc[X.index]\n",
    "    \n",
    "    # Ensure X has enough features for RFE\n",
    "    n_features = min(10, len(feature_cols))\n",
    "    if n_features == 0:\n",
    "        raise ValueError(\"No valid numeric features available for RFE\")\n",
    "    \n",
    "    rfe = RFE(estimator=LinearRegression(), n_features_to_select=n_features)\n",
    "    rfe.fit(X, y)\n",
    "    selected_features = X.columns[rfe.support_].tolist()\n",
    "    logger.info(f\"Selected features: {selected_features}\")\n",
    "    \n",
    "    # Return DataFrame with target and selected features\n",
    "    return df[[target] + selected_features]\n",
    "\n",
    "def calculate_metrics(actual, predicted, naive_forecast=None):\n",
    "    \"\"\"Calculate evaluation metrics for forecasts.\"\"\"\n",
    "    actual = np.array(actual, dtype=float)\n",
    "    predicted = np.array(predicted, dtype=float)\n",
    "    valid_mask = ~np.isnan(actual) & ~np.isnan(predicted) & ~np.isinf(actual) & ~np.isinf(predicted)\n",
    "    actual = actual[valid_mask]\n",
    "    predicted = predicted[valid_mask]\n",
    "    if len(actual) == 0:\n",
    "        logger.warning(\"No valid data for metrics calculation!\")\n",
    "        return np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "    rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    mape = mean_absolute_percentage_error(actual, predicted) * 100 if np.all(np.abs(actual) > 1e-8) else np.nan\n",
    "    smape = 100 * np.mean(2 * np.abs(predicted - actual) / (np.abs(actual) + np.abs(predicted) + 1e-8))\n",
    "    norm_mape = mape / np.mean(np.abs(actual)) if not np.isnan(mape) else np.nan\n",
    "    directional_acc = np.mean((np.diff(actual) * np.diff(predicted)) > 0) * 100 if len(actual) > 1 else np.nan\n",
    "    mase = np.mean(np.abs(actual - predicted)) / np.mean(np.abs(actual[1:] - naive_forecast[:-1])) if naive_forecast is not None else np.nan\n",
    "    return rmse, mae, mape, smape, norm_mape, directional_acc, mase\n",
    "\n",
    "def plot_decomposition(series, period, filename):\n",
    "    \"\"\"Plot seasonal decomposition of a series.\"\"\"\n",
    "    try:\n",
    "        decomposition = seasonal_decompose(series, period=period, model='additive')\n",
    "        fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(12, 8))\n",
    "        ax1.plot(series.index, series, label='Original'); ax1.legend(loc='upper left')\n",
    "        ax2.plot(series.index, decomposition.trend, label='Trend'); ax2.legend(loc='upper left')\n",
    "        ax3.plot(series.index, decomposition.seasonal, label='Seasonal'); ax3.legend(loc='upper left')\n",
    "        ax4.plot(series.index, decomposition.resid, label='Residual'); ax4.legend(loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(CONFIG['img_dir'], filename), dpi=300)\n",
    "        logger.info(f\"Saved decomposition plot: {filename}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving decomposition plot: {str(e)}\")\n",
    "    finally:\n",
    "        plt.close()\n",
    "\n",
    "def plot_forecast(historical, test, forecast, forecast_index, title, ylabel, filename, confidence_intervals=None):\n",
    "    \"\"\"Plot historical data, actual test data, and forecast.\"\"\"\n",
    "    try:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(historical.index, historical, label='Historical', color='blue')\n",
    "        plt.plot(test.index, test, label='Actual (Test)', color='green')\n",
    "        plt.plot(forecast_index, forecast, label='Forecast', color='orange', linestyle='--', linewidth=2)\n",
    "        if confidence_intervals:\n",
    "            plt.fill_between(forecast_index, confidence_intervals[0], confidence_intervals[1], color='orange', alpha=0.2, label='95% CI')\n",
    "        plt.title(title); plt.xlabel('Time'); plt.ylabel(ylabel); plt.legend(); plt.grid(True)\n",
    "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "        plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n",
    "        plt.xticks(rotation=45); plt.tight_layout()\n",
    "        plt.savefig(os.path.join(CONFIG['img_dir'], filename), dpi=300)\n",
    "        logger.info(f\"Saved plot: {filename}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving plot: {str(e)}\")\n",
    "    finally:\n",
    "        plt.close()\n",
    "\n",
    "def plot_comparison_forecasts(historical, test, forecasts, forecast_index, title, ylabel, filename, metrics=None):\n",
    "    \"\"\"Plot comparison of forecasts from multiple models.\"\"\"\n",
    "    try:\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        plt.plot(historical.index, historical, label='Historical', color='blue')\n",
    "        plt.plot(test.index, test, label='Actual (Test)', color='green')\n",
    "        colors = sns.color_palette(\"husl\", len(forecasts))\n",
    "        for (model_name, forecast), color in zip(forecasts.items(), colors):\n",
    "            rmse = metrics.get(model_name, {}).get('RMSE', np.nan) if metrics else np.nan\n",
    "            if forecast is None or pd.isna(rmse):\n",
    "                continue\n",
    "            plt.plot(forecast_index, forecast, label=f'Forecast {model_name} (RMSE: {rmse:.4f})', linestyle='--', color=color)\n",
    "        plt.title(title); plt.xlabel('Time'); plt.ylabel(ylabel); plt.legend(); plt.grid(True)\n",
    "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "        plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n",
    "        plt.xticks(rotation=45); plt.tight_layout()\n",
    "        plt.savefig(os.path.join(CONFIG['img_dir'], filename), dpi=300)\n",
    "        logger.info(f\"Saved comparison plot: {filename}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving comparison plot: {str(e)}\")\n",
    "    finally:\n",
    "        plt.close()\n",
    "\n",
    "def plot_metrics_bar(metrics_df, filename):\n",
    "    \"\"\"Plot bar comparison of metrics across models.\"\"\"\n",
    "    try:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        metrics = ['RMSE', 'MAE', 'MAPE', 'sMAPE', 'NormMAPE', 'DirAcc', 'MASE']\n",
    "        for i, metric in enumerate(metrics, 1):\n",
    "            plt.subplot(3, 3, i)\n",
    "            sns.barplot(x='Model', y=metric, data=metrics_df)\n",
    "            plt.title(f'{metric} Comparison')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "        plt.savefig(os.path.join(CONFIG['img_dir'], filename), dpi=300)\n",
    "        logger.info(f\"Saved metrics bar plot: {filename}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving metrics bar plot: {str(e)}\")\n",
    "    finally:\n",
    "        plt.close()\n",
    "\n",
    "def plot_residual_acf(residuals, title, filename):\n",
    "    \"\"\"Plot ACF of residuals.\"\"\"\n",
    "    if residuals is None or len(residuals) < 2:\n",
    "        logger.warning(f\"Skipping ACF: Insufficient residual data - {title}\")\n",
    "        return\n",
    "    try:\n",
    "        plt.figure(figsize=(5, 3))\n",
    "        max_lags = min(20, len(residuals) - 1)\n",
    "        if max_lags < 1:\n",
    "            logger.warning(f\"Skipping ACF: Too few residuals - {title}\")\n",
    "            return\n",
    "        plot_acf(residuals, lags=max_lags, title=title)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(CONFIG['img_dir'], filename), dpi=300)\n",
    "        logger.info(f\"Saved ACF plot: {filename}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving ACF plot: {str(e)}\")\n",
    "    finally:\n",
    "        plt.close()\n",
    "\n",
    "def run_exponential_smoothing(train, test, forecast_index, seasonal_periods=CONFIG['seasonal_periods']):\n",
    "    \"\"\"Run Exponential Smoothing model.\"\"\"\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        model = ExponentialSmoothing(train, trend='add', seasonal='add', seasonal_periods=seasonal_periods).fit(optimized=True)\n",
    "        forecast = model.forecast(CONFIG['forecast_horizon'])\n",
    "        residuals = train - model.fittedvalues\n",
    "        forecast = pd.Series(forecast.values, index=forecast_index)\n",
    "        resid_std = np.std(residuals)\n",
    "        ci_lower = forecast - 1.96 * resid_std\n",
    "        ci_upper = forecast + 1.96 * resid_std\n",
    "        naive_forecast = train.shift(1).reindex(test.index).fillna(train.iloc[-1])\n",
    "        rmse, mae, mape, smape, norm_mape, dir_acc, mase = calculate_metrics(test, forecast, naive_forecast)\n",
    "        logger.info(f\"Exponential Smoothing: RMSE={rmse:.4f}, Time={time.time() - start_time:.2f}s\")\n",
    "        return forecast, residuals, rmse, mae, mape, smape, norm_mape, dir_acc, mase, (ci_lower, ci_upper)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error Exponential Smoothing: {str(e)}\")\n",
    "        return None, None, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, None\n",
    "\n",
    "def run_arima(train, test, forecast_index):\n",
    "    \"\"\"Run ARIMA model with Optuna tuning using statsmodels.\"\"\"\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        d = check_stationarity(train, \"ARIMA\")\n",
    "        def objective(trial):\n",
    "            p = trial.suggest_int('p', 0, 3)\n",
    "            q = trial.suggest_int('q', 0, 3)\n",
    "            try:\n",
    "                model = SARIMAX(train, order=(p, d, q), trend='c')\n",
    "                results = model.fit(disp=False, maxiter=100)\n",
    "                forecast = results.forecast(steps=CONFIG['forecast_horizon'])\n",
    "                rmse = np.sqrt(mean_squared_error(test, forecast))\n",
    "                return rmse\n",
    "            except:\n",
    "                return float('inf')  # Return high value for failed fits\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(objective, n_trials=CONFIG['optuna_trials'])\n",
    "        best_params = study.best_params\n",
    "        model = SARIMAX(train, order=(best_params['p'], d, best_params['q']), trend='c')\n",
    "        results = model.fit(disp=False, maxiter=100)\n",
    "        forecast = results.forecast(steps=CONFIG['forecast_horizon'])\n",
    "        residuals = train - results.fittedvalues\n",
    "        forecast = pd.Series(forecast, index=forecast_index)\n",
    "        naive_forecast = train.shift(1).reindex(test.index).fillna(train.iloc[-1])\n",
    "        rmse, mae, mape, smape, norm_mape, dir_acc, mase = calculate_metrics(test, forecast, naive_forecast)\n",
    "        logger.info(f\"ARIMA (order=({best_params['p']}, {d}, {best_params['q']})): RMSE={rmse:.4f}, Time={time.time() - start_time:.2f}s\")\n",
    "        return forecast, residuals, rmse, mae, mape, smape, norm_mape, dir_acc, mase, None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error ARIMA: {str(e)}\")\n",
    "        return None, None, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, None\n",
    "\n",
    "def run_sarima(train, test, forecast_index, seasonal_periods=CONFIG['seasonal_periods']):\n",
    "    \"\"\"Run SARIMA model with Optuna tuning using statsmodels.\"\"\"\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        d = check_stationarity(train, \"SARIMA\")\n",
    "        def objective(trial):\n",
    "            p = trial.suggest_int('p', 0, 3)\n",
    "            q = trial.suggest_int('q', 0, 3)\n",
    "            P = trial.suggest_int('P', 0, 1)\n",
    "            D = trial.suggest_int('D', 0, 1)\n",
    "            Q = trial.suggest_int('Q', 0, 1)\n",
    "            try:\n",
    "                model = SARIMAX(train, order=(p, d, q), seasonal_order=(P, D, Q, seasonal_periods), trend='c')\n",
    "                results = model.fit(disp=False, maxiter=100)\n",
    "                forecast = results.forecast(steps=CONFIG['forecast_horizon'])\n",
    "                rmse = np.sqrt(mean_squared_error(test, forecast))\n",
    "                return rmse\n",
    "            except:\n",
    "                return float('inf')\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(objective, n_trials=CONFIG['optuna_trials'])\n",
    "        best_params = study.best_params\n",
    "        model = SARIMAX(train, order=(best_params['p'], d, best_params['q']),\n",
    "                        seasonal_order=(best_params['P'], best_params['D'], best_params['Q'], seasonal_periods),\n",
    "                        trend='c')\n",
    "        results = model.fit(disp=False, maxiter=100)\n",
    "        forecast = results.forecast(steps=CONFIG['forecast_horizon'])\n",
    "        residuals = train - results.fittedvalues\n",
    "        forecast = pd.Series(forecast, index=forecast_index)\n",
    "        naive_forecast = train.shift(1).reindex(test.index).fillna(train.iloc[-1])\n",
    "        rmse, mae, mape, smape, norm_mape, dir_acc, mase = calculate_metrics(test, forecast, naive_forecast)\n",
    "        logger.info(f\"SARIMA (order=({best_params['p']}, {d}, {best_params['q']}), seasonal_order=({best_params['P']}, {best_params['D']}, {best_params['Q']}, {seasonal_periods})): RMSE={rmse:.4f}, Time={time.time() - start_time:.2f}s\")\n",
    "        return forecast, residuals, rmse, mae, mape, smape, norm_mape, dir_acc, mase, None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error SARIMA: {str(e)}\")\n",
    "        return None, None, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, None\n",
    "\n",
    "def run_sarimax(train, test, forecast_index, exog_train, exog_test):\n",
    "    \"\"\"Run SARIMAX model with Optuna tuning using statsmodels.\"\"\"\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        exog_train = exog_train.interpolate(method='linear').bfill().ffill()\n",
    "        exog_test = exog_test.interpolate(method='linear').bfill().ffill()\n",
    "        d = check_stationarity(train, \"SARIMAX\")\n",
    "        def objective(trial):\n",
    "            p = trial.suggest_int('p', 0, 2)\n",
    "            q = trial.suggest_int('q', 0, 2)\n",
    "            P = trial.suggest_int('P', 0, 1)\n",
    "            D = trial.suggest_int('D', 0, 1)\n",
    "            Q = trial.suggest_int('Q', 0, 1)\n",
    "            try:\n",
    "                model = SARIMAX(train, exog=exog_train, order=(p, d, q),\n",
    "                                seasonal_order=(P, D, Q, CONFIG['seasonal_periods']), trend='c')\n",
    "                results = model.fit(disp=False, maxiter=100)\n",
    "                forecast = results.forecast(steps=CONFIG['forecast_horizon'], exog=exog_test)\n",
    "                rmse = np.sqrt(mean_squared_error(test, forecast))\n",
    "                return rmse\n",
    "            except:\n",
    "                return float('inf')\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(objective, n_trials=CONFIG['optuna_trials'])\n",
    "        best_params = study.best_params\n",
    "        model = SARIMAX(train, exog=exog_train, order=(best_params['p'], d, best_params['q']),\n",
    "                        seasonal_order=(best_params['P'], best_params['D'], best_params['Q'], CONFIG['seasonal_periods']),\n",
    "                        trend='c')\n",
    "        results = model.fit(disp=False, maxiter=100)\n",
    "        forecast = results.forecast(steps=CONFIG['forecast_horizon'], exog=exog_test)\n",
    "        residuals = train - results.fittedvalues\n",
    "        forecast = pd.Series(forecast, index=forecast_index)\n",
    "        naive_forecast = train.shift(1).reindex(test.index).fillna(train.iloc[-1])\n",
    "        rmse, mae, mape, smape, norm_mape, dir_acc, mase = calculate_metrics(test, forecast, naive_forecast)\n",
    "        logger.info(f\"SARIMAX (order=({best_params['p']}, {d}, {best_params['q']}), seasonal_order=({best_params['P']}, {best_params['D']}, {best_params['Q']}, {CONFIG['seasonal_periods']})): RMSE={rmse:.4f}, Time={time.time() - start_time:.2f}s\")\n",
    "        return forecast, residuals, rmse, mae, mape, smape, norm_mape, dir_acc, mase, None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error SARIMAX: {str(e)}\")\n",
    "        return None, None, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, None\n",
    "\n",
    "def run_prophet(train, test, forecast_index, exog_train=None, exog_test=None):\n",
    "    \"\"\"Run Prophet model with Optuna tuning.\"\"\"\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        df_train = pd.DataFrame({'ds': train.index, 'y': train.values})\n",
    "        if exog_train is not None:\n",
    "            for col in exog_train.columns:\n",
    "                df_train[col] = exog_train[col].values\n",
    "        def objective(trial):\n",
    "            model = Prophet(\n",
    "                yearly_seasonality=True,\n",
    "                weekly_seasonality=False,\n",
    "                daily_seasonality=False,\n",
    "                changepoint_prior_scale=trial.suggest_float('changepoint_prior_scale', 0.01, 0.5, log=True),\n",
    "                seasonality_prior_scale=trial.suggest_float('seasonality_prior_scale', 0.1, 10.0, log=True)\n",
    "            )\n",
    "            if exog_train is not None:\n",
    "                for col in exog_train.columns:\n",
    "                    model.add_regressor(col)\n",
    "            model.fit(df_train)\n",
    "            future = pd.DataFrame({'ds': forecast_index})\n",
    "            if exog_test is not None:\n",
    "                for col in exog_test.columns:\n",
    "                    future[col] = exog_test[col].values\n",
    "            forecast = model.predict(future)\n",
    "            rmse = np.sqrt(mean_squared_error(test, forecast['yhat']))\n",
    "            return rmse\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(objective, n_trials=CONFIG['optuna_trials'])\n",
    "        best_params = study.best_params\n",
    "        model = Prophet(\n",
    "            yearly_seasonality=True,\n",
    "            weekly_seasonality=False,\n",
    "            daily_seasonality=False,\n",
    "            changepoint_prior_scale=best_params['changepoint_prior_scale'],\n",
    "            seasonality_prior_scale=best_params['seasonality_prior_scale']\n",
    "        )\n",
    "        if exog_train is not None:\n",
    "            for col in exog_train.columns:\n",
    "                model.add_regressor(col)\n",
    "        model.fit(df_train)\n",
    "        future = pd.DataFrame({'ds': forecast_index})\n",
    "        if exog_test is not None:\n",
    "            for col in exog_test.columns:\n",
    "                future[col] = exog_test[col].values\n",
    "        forecast = model.predict(future)\n",
    "        forecast_series = pd.Series(forecast['yhat'].values, index=forecast_index)\n",
    "        residuals = train - model.predict(df_train)['yhat']\n",
    "        ci_lower = pd.Series(forecast['yhat_lower'].values, index=forecast_index)\n",
    "        ci_upper = pd.Series(forecast['yhat_upper'].values, index=forecast_index)\n",
    "        naive_forecast = train.shift(1).reindex(test.index).fillna(train.iloc[-1])\n",
    "        rmse, mae, mape, smape, norm_mape, dir_acc, mase = calculate_metrics(test, forecast_series, naive_forecast)\n",
    "        logger.info(f\"Prophet: RMSE={rmse:.4f}, Time={time.time() - start_time:.2f}s\")\n",
    "        return forecast_series, residuals, rmse, mae, mape, smape, norm_mape, dir_acc, mase, (ci_lower, ci_upper)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error Prophet: {str(e)}\")\n",
    "        return None, None, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, None\n",
    "\n",
    "def generate_report(results_df, forecasts_cpi, img_dir):\n",
    "    \"\"\"Generate HTML report of model performance.\"\"\"\n",
    "    try:\n",
    "        report = results_df.style.highlight_min(subset=['RMSE', 'MAE', 'MAPE', 'sMAPE', 'NormMAPE', 'MASE'], color='lightgreen')\\\n",
    "                                .highlight_max(subset=['DirAcc'], color='lightgreen')\\\n",
    "                                .set_caption(\"Model Performance Summary\")\n",
    "        with open(f'{img_dir}/report.html', 'w') as f:\n",
    "            f.write(report.to_html())\n",
    "        logger.info(\"Generated report at report.html\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating report: {str(e)}\")\n",
    "\n",
    "def run_model_for_target(target, train, test, forecast_index, model_name, model_func, params, retries=3):\n",
    "    \"\"\"Run a model for a target variable with retries.\"\"\"\n",
    "    logger.info(f\"Running {model_name} for {target}\")\n",
    "    start_time = time.time()\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            if model_name == 'SARIMAX' or model_name == 'Prophet':\n",
    "                exog_vars = ['oil_price', 'gold_price']\n",
    "                exog_train = train[exog_vars]\n",
    "                exog_test = test[exog_vars].reindex(forecast_index).interpolate(method='linear').bfill().ffill()\n",
    "                forecast, residuals, rmse, mae, mape, smape, norm_mape, dir_acc, mase, ci = model_func(\n",
    "                    train[target], test[target], forecast_index, exog_train, exog_test, **params\n",
    "                )\n",
    "            else:\n",
    "                forecast, residuals, rmse, mae, mape, smape, norm_mape, dir_acc, mase, ci = model_func(\n",
    "                    train[target], test[target], forecast_index, **params\n",
    "                )\n",
    "            if forecast is None or pd.isna(rmse):\n",
    "                logger.warning(f\"{model_name} for {target} failed to produce valid forecast or RMSE\")\n",
    "                continue\n",
    "            plot_forecast(train[target][-36:], test[target], forecast, forecast_index,\n",
    "                          f'{model_name} Forecast for {target}', target, f'{target}_{model_name}_forecast.png', ci)\n",
    "            if residuals is not None:\n",
    "                plot_residual_acf(residuals.dropna(), f'ACF of Residuals - {model_name} ({target})',\n",
    "                                  f'{target}_{model_name}_acf.png')\n",
    "            logger.info(f\"Completed {model_name} for {target} in {time.time() - start_time:.2f}s\")\n",
    "            return {\n",
    "                'Target': target,\n",
    "                'Model': model_name,\n",
    "                'RMSE': rmse,\n",
    "                'MAE': mae,\n",
    "                'MAPE': mape,\n",
    "                'sMAPE': smape,\n",
    "                'NormMAPE': norm_mape,\n",
    "                'DirAcc': dir_acc,\n",
    "                'MASE': mase,\n",
    "                'Forecast': forecast,\n",
    "                'Residuals': residuals,\n",
    "                'CI': ci\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Attempt {attempt+1} failed for {model_name}: {str(e)}\")\n",
    "            if attempt == retries - 1:\n",
    "                logger.error(f\"All retries failed for {model_name} on {target}\")\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run time series forecasting.\"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv('data/data.csv')\n",
    "        data['time'] = pd.to_datetime(data['date'])\n",
    "        data.set_index('time', inplace=True)\n",
    "        required_columns = ['cpi', 'oil_price', 'gold_price']\n",
    "        data = validate_input_data(data, required_columns)\n",
    "        \n",
    "        for col in required_columns:\n",
    "            data[col] = detect_outliers(data[col], method='iqr')\n",
    "        \n",
    "        data_features = create_features(data, 'cpi')\n",
    "        \n",
    "        train_size = len(data_features) - CONFIG['forecast_horizon']\n",
    "        train, test = data_features[:train_size], data_features[train_size:]\n",
    "        forecast_index = pd.date_range(start=test.index[0], periods=CONFIG['forecast_horizon'], freq='MS')\n",
    "        \n",
    "        plot_decomposition(data['cpi'], period=CONFIG['seasonal_periods'], filename='cpi_decomposition.png')\n",
    "        \n",
    "        models = {\n",
    "            'ARIMA': (run_arima, {}),\n",
    "            'Exponential Smoothing': (run_exponential_smoothing, {}),\n",
    "            'Prophet': (run_prophet, {}),\n",
    "            'SARIMA': (run_sarima, {}),\n",
    "            'SARIMAX': (run_sarimax, {})\n",
    "        }\n",
    "        \n",
    "        results = []\n",
    "        forecasts_cpi = {}\n",
    "        metrics_cpi = {}\n",
    "        logger.info(\"Running models for CPI\")\n",
    "        tasks = [delayed(wrap_non_picklable_objects(run_model_for_target))('cpi', train, test, forecast_index, model_name, model_func, params)\n",
    "                 for model_name, (model_func, params) in models.items()]\n",
    "        model_results = Parallel(n_jobs=CONFIG['n_jobs'], verbose=1)(tasks)\n",
    "        \n",
    "        for result in model_results:\n",
    "            if result is not None:\n",
    "                results.append({\n",
    "                    'Target': result['Target'],\n",
    "                    'Model': result['Model'],\n",
    "                    'RMSE': result['RMSE'],\n",
    "                    'MAE': result['MAE'],\n",
    "                    'MAPE': result['MAPE'],\n",
    "                    'sMAPE': result['sMAPE'],\n",
    "                    'NormMAPE': result['NormMAPE'],\n",
    "                    'DirAcc': result['DirAcc'],\n",
    "                    'MASE': result['MASE']\n",
    "                })\n",
    "                forecasts_cpi[result['Model']] = result['Forecast']\n",
    "                metrics_cpi[result['Model']] = {'RMSE': result['RMSE']}\n",
    "            else:\n",
    "                logger.warning(f\"Result for a CPI model is None, skipping!\")\n",
    "        \n",
    "        if forecasts_cpi:\n",
    "            weights = {model: 1/max(metrics_cpi[model]['RMSE'], 1e-8) for model in metrics_cpi}\n",
    "            total_weight = sum(weights.values())\n",
    "            weights = {model: w/total_weight for model, w in weights.items()}\n",
    "            plot_comparison_forecasts(train['cpi'][-36:], test['cpi'], forecasts_cpi, forecast_index,\n",
    "                                     'Comparison of Forecasts for CPI', 'CPI', 'cpi_model_comparison.png',\n",
    "                                     metrics=metrics_cpi)\n",
    "        \n",
    "        results_df = pd.DataFrame(results)\n",
    "        print(results_df)\n",
    "        results_df.to_csv(CONFIG['results_file'], index=False)\n",
    "        logger.info(f\"Results saved to {CONFIG['results_file']}\")\n",
    "        \n",
    "        if not results_df.empty:\n",
    "            plot_metrics_bar(results_df, 'cpi_metrics_comparison.png')\n",
    "            generate_report(results_df, forecasts_cpi, img_dir)\n",
    "        \n",
    "        if forecasts_cpi:\n",
    "            combined_forecast = pd.DataFrame({'Date': forecast_index})\n",
    "            for model_name, forecast in forecasts_cpi.items():\n",
    "                combined_forecast[f'{model_name}_cpi'] = forecast\n",
    "            combined_forecast.to_csv(f'{img_dir}/combined_forecast_cpi.csv', index=False)\n",
    "            logger.info(f\"Combined forecasts saved to {img_dir}/combined_forecast_cpi.csv\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Main program error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
